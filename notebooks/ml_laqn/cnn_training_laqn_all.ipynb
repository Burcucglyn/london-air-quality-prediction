{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f197f750",
      "metadata": {
        "id": "f197f750"
      },
      "source": [
        "# CNN model training for all LAQN stations\n",
        "\n",
        "## what this notebook does\n",
        "\n",
        "This notebook trains convolutional neural network (CNN) models to predict air pollution levels across ALL London Air Quality Network stations. I train CNN models for every station pollutant combination in the LAQN dataset.\n",
        "\n",
        "the goal is to compare network wide CNN performance against the random forest results from rf_training_laqn_all.ipynb.\n",
        "\n",
        "## why all stations instead of one?\n",
        "\n",
        "the single station approach (EN5_NO2) was useful for proof of concept. The dissertation needs to show how the models perform across the entire LAQN network. This means training separate models for each of the 141 station pollutant combinations.\n",
        "\n",
        "\n",
        "## structure of this notebook\n",
        "\n",
        "| section | what it does |\n",
        "|---------|-------------|\n",
        "| 1 | setup and imports |\n",
        "| 2 | load prepared data from ml_prep_all |\n",
        "| 3 | understand the data shapes |\n",
        "| 4 | identify all target columns |\n",
        "| 5 | build CNN model function |\n",
        "| 6 | set up callbacks |\n",
        "| 7 | train models for all targets with checkpoints |\n",
        "| 8 | load results from colab training |\n",
        "| 9 | investigate broken models |\n",
        "| 10 | baseline evaluation after exclusion |\n",
        "| 11 | results summary and save |\n",
        "| 12 | prediction visualisations |\n",
        "| 13 | residual analysis |\n",
        "| 14 | final summary |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40234ea0",
      "metadata": {
        "id": "40234ea0"
      },
      "source": [
        "## 1) Setup and Imports\n",
        "\n",
        "Importing everything needed for CNN training. tensorflow/keras handles the neural network numpy for arrays matplotlib and seaborn for plotting scikit learn for metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5baaa093",
      "metadata": {
        "id": "5baaa093"
      },
      "outputs": [],
      "source": [
        "#Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import warnings\n",
        "import time\n",
        "import gc\n",
        "from collections import Counter\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Scikit learn for metrics r2, MSE, MAE\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "#Tensorflow and keras for CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "print(f'Tensorflow version: {tf.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jk4emTxSKRBo",
      "metadata": {
        "id": "jk4emTxSKRBo"
      },
      "source": [
        "tensorflow version: 2.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74981b8f",
      "metadata": {
        "id": "74981b8f"
      },
      "outputs": [],
      "source": [
        "#paths\n",
        "base_dir = Path.cwd().parent.parent / 'data' / 'laqn'\n",
        "data_dir = base_dir / 'ml_prep_all'\n",
        "output_dir = Path.cwd().parent.parent / 'data' / 'ml' / 'LAQN_all' / 'cnn_model'\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f'loading data from: {data_dir}')\n",
        "print(f'saving outputs to: {output_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ca668bd",
      "metadata": {
        "id": "1ca668bd"
      },
      "source": [
        "### GPU availability\n",
        "\n",
        "checking if GPU is available. CNN training is faster on GPU but will still work on CPU.\n",
        "\n",
        "source: Use a GPU: Tensorflow Core (no date) TensorFlow. Available at: https://www.tensorflow.org/guide/gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc46095",
      "metadata": {
        "id": "2fc46095"
      },
      "outputs": [],
      "source": [
        "#Check gpu availability\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f'GPU available: {len(gpus)} device(s)')\n",
        "    for gpu in gpus:\n",
        "        print(f'  - {gpu.name}')\n",
        "else:\n",
        "    print('No GPU found, using CPU training will be slower but still works.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LORvoZpGKXG1",
      "metadata": {
        "id": "LORvoZpGKXG1"
      },
      "source": [
        "no GPU found, using CPU training will be slower but still works\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5137c237",
      "metadata": {
        "id": "5137c237"
      },
      "source": [
        "## 2) load prepared data\n",
        "\n",
        "The data was prepared in `ml_prep_laqn_all.ipynb`it created sequences where each sample has 12 hours of history to predict the next hour for all stations.\n",
        "\n",
        "### why 3D data for CNN?\n",
        "\n",
        "Random forest needs flat 2D data: (samples, features). CNN needs 3D data: (samples, timesteps, features). the 3D shape lets CNN learn patterns across time, not just treat each timestep as an independent feature.\n",
        "\n",
        "\n",
        "| Data Shape | Model | Structure |\n",
        "|------------|-------|----------|\n",
        "| 2D | random forest | each row is a flat list of numbers with no structure |\n",
        "| 3D | CNN | each sample is a grid where rows are hours and columns are features |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7be915f",
      "metadata": {
        "id": "d7be915f"
      },
      "outputs": [],
      "source": [
        "#Load the 3d sequences for cnn\n",
        "X_train = np.load(data_dir / 'X_train.npy')\n",
        "X_val = np.load(data_dir / 'X_val.npy')\n",
        "X_test = np.load(data_dir / 'X_test.npy')\n",
        "\n",
        "y_train = np.load(data_dir / 'y_train.npy')\n",
        "y_val = np.load(data_dir / 'y_val.npy')\n",
        "y_test = np.load(data_dir / 'y_test.npy')\n",
        "\n",
        "#Load feature_names and scaler\n",
        "feature_names = joblib.load(data_dir / 'feature_names.joblib')\n",
        "scaler = joblib.load(data_dir / 'scaler.joblib')\n",
        "\n",
        "print('Data loaded successfully.')\n",
        "print(f'\\nShapes:')\n",
        "print(f'X_train: {X_train.shape}')\n",
        "print(f'X_val: {X_val.shape}')\n",
        "print(f'X_test: {X_test.shape}')\n",
        "print(f'y_train: {y_train.shape}')\n",
        "print(f'y_val: {y_val.shape}')\n",
        "print(f'y_test: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UUxewWS8K1Rg",
      "metadata": {
        "id": "UUxewWS8K1Rg"
      },
      "source": [
        "    data loaded successfully\n",
        "\n",
        "    shapes:\n",
        "    X_train: (17107, 12, 145)\n",
        "    X_val: (3656, 12, 145)\n",
        "    X_test: (3657, 12, 145)\n",
        "    y_train: (17107, 145)\n",
        "    y_val: (3656, 145)\n",
        "    y_test: (3657, 145)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d096fba6",
      "metadata": {
        "id": "d096fba6"
      },
      "source": [
        "## 3) Understanding the Shapes\n",
        "\n",
        "X_train shape:\n",
        "\n",
        "| Dimension | What it represents |\n",
        "|-----------|--------------|\n",
        "| first | number of samples (individual training examples) |\n",
        "| second | timesteps (12 hours of history) |\n",
        "| third | features (all station pollutant columnS +temporal) |\n",
        "\n",
        "y_train shape is (samples, features). The model can predict all features for the next hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ecacd1",
      "metadata": {
        "id": "48ecacd1"
      },
      "outputs": [],
      "source": [
        "#Extract dimensions\n",
        "n_samples, timesteps, n_features = X_train.shape\n",
        "\n",
        "print(f'\\ndata dimensions:')\n",
        "print(f'  samples: {n_samples:,}')\n",
        "print(f'  timesteps: {timesteps}')\n",
        "print(f'  features: {n_features}')\n",
        "print(f'\\nfeature names ({len(feature_names)} total):')\n",
        "print(f'  first 10: {feature_names[:10]}')\n",
        "print(f'  last 10: {feature_names[-10:]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M1Ve61GULJrQ",
      "metadata": {
        "id": "M1Ve61GULJrQ"
      },
      "source": [
        "  data dimensions:\n",
        "  samples: 17,107\n",
        "  timesteps: 12\n",
        "  features: 145\n",
        "\n",
        "  feature names (145 total):\n",
        "    first 10: ['BG1_NO2', 'BG1_SO2', 'BG2_NO2', 'BG2_PM10', 'BQ7_NO2', 'BQ7_O3', 'BQ7_PM10', 'BQ7_PM25', 'BQ9_PM10', 'BQ9_PM25']\n",
        "    last 10: ['WAC_PM10', 'WM5_NO2', 'WM6_NO2', 'WM6_PM10', 'WMD_NO2', 'WMD_PM25', 'hour', 'day_of_week', 'month', 'is_weekend']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e1f1949",
      "metadata": {
        "id": "3e1f1949"
      },
      "source": [
        "## 4) Identify All Target Columns\n",
        "\n",
        "Need to identify which columns are pollutant predictions (targets) and which are temporal features. Temporal features like hour, day_of_week are inputs only, not things we want to predict.\n",
        "\n",
        "### Pollutant Naming Convention\n",
        "\n",
        "Each target column follows the pattern: `{SiteCode}_{PollutantCode}`\n",
        "\n",
        "Example:\n",
        "\n",
        "| Example | Meaning |\n",
        "|---------|--------|\n",
        "| BG1_NO2 | NO2 at site BG1 |\n",
        "| EN5_PM25 | PM2.5 at site EN5 |\n",
        "\n",
        "### The 6 regulatory pollutants:\n",
        "\n",
        "| Pollutant | Code | UK Annual Limit |\n",
        "|-----------|------|----------------|\n",
        "| Nitrogen Dioxide | NO2 | 40 ug/m3 |\n",
        "| PM2.5 Particulate | PM25 | 20 ug/m3 |\n",
        "| PM10 Particulate | PM10 | 40 ug/m3 |\n",
        "| Ozone | O3 | n/a |\n",
        "| Sulphur Dioxide | SO2 | n/a |\n",
        "| Carbon Monoxide | CO | n/a |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "target_identification",
      "metadata": {
        "id": "target_identification"
      },
      "outputs": [],
      "source": [
        "#Temporal vs pollutant columns\n",
        "temporal_cols = ['hour', 'day_of_week', 'month', 'is_weekend']\n",
        "\n",
        "#Get pollutant target columns everything except temporal\n",
        "target_names = [name for name in feature_names if name not in temporal_cols]\n",
        "target_indices = [i for i, name in enumerate(feature_names) if name not in temporal_cols]\n",
        "\n",
        "#Create target mapping dictionary\n",
        "target_mapping = {name: i for i, name in enumerate(feature_names) if name not in temporal_cols}\n",
        "\n",
        "print(f'total features: {len(feature_names)}')\n",
        "print(f'temporal features: {len(temporal_cols)}')\n",
        "print(f'pollutant targets: {len(target_names)}')\n",
        "\n",
        "#Count by pollutant type\n",
        "pollutant_codes = ['NO2', 'PM25', 'PM10', 'O3', 'SO2', 'CO']\n",
        "print(f'\\nbreakdown by pollutant:')\n",
        "for poll in pollutant_codes:\n",
        "    count = len([n for n in target_names if f'_{poll}' in n])\n",
        "    print(f'  {poll}: {count} stations')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qW5ht9SXLr7S",
      "metadata": {
        "id": "qW5ht9SXLr7S"
      },
      "source": [
        "    total features: 145\n",
        "    temporal features: 4\n",
        "    pollutant targets: 141\n",
        "\n",
        "    breakdown by pollutant:\n",
        "      NO2: 58 stations\n",
        "      PM25: 24 stations\n",
        "      PM10: 42 stations\n",
        "      O3: 11 stations\n",
        "      SO2: 4 stations\n",
        "      CO: 2 stations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0t_Tyo6YL-E6",
      "metadata": {
        "id": "0t_Tyo6YL-E6"
      },
      "outputs": [],
      "source": [
        "#Prepare y arrays with only pollutant targets excluded temporal\n",
        "y_train_targets = y_train[:, target_indices]\n",
        "y_val_targets = y_val[:, target_indices]\n",
        "y_test_targets = y_test[:, target_indices]\n",
        "\n",
        "print(f'target arrays prepared:')\n",
        "print(f'  y_train_targets: {y_train_targets.shape}')\n",
        "print(f'  y_val_targets: {y_val_targets.shape}')\n",
        "print(f'  y_test_targets: {y_test_targets.shape}')\n",
        "print(f'\\nwill train {len(target_names)} separate CNN models')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iNue9pxyMDVJ",
      "metadata": {
        "id": "iNue9pxyMDVJ"
      },
      "source": [
        "    target arrays prepared:\n",
        "      y_train_targets: (17107, 141)\n",
        "      y_val_targets: (3656, 141)\n",
        "      y_test_targets: (3657, 141)\n",
        "\n",
        "    will train 141 separate CNN models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0109ce08",
      "metadata": {
        "id": "0109ce08"
      },
      "source": [
        "## 5)Build CNN Model Function\n",
        "\n",
        "Building a function that creates CNN models. Using the best hyperparameters found from the single station tuning:\n",
        "\n",
        "| parameter | value | why |\n",
        "|-----------|-------|-----|\n",
        "| filters_1 | 128 | more capacity to learn patterns |\n",
        "| kernel_1 | 2 | short term patterns matter most |\n",
        "| dropout | 0.1 | less regularisation needed |\n",
        "| filters_2 | 64 | second layer with fewer filters |\n",
        "| kernel_2 | 2 | consistent with first layer |\n",
        "| dense_units | 50 | same as baseline |\n",
        "| learning_rate | 0.001 | adam default works well |\n",
        "\n",
        "These parameters came from keras tuner results in the single station CNN notebook.\n",
        "\n",
        "source: Geron, A. (2023) Hands on machine learning with scikit learn, Keras and TensorFlow. Ch. 15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689d6dc8",
      "metadata": {
        "id": "689d6dc8"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(timesteps,\n",
        "                    features, filters_1=128,\n",
        "                    filters_2=64,\n",
        "                    kernel_size=2,\n",
        "                    dropout_rate=0.1,\n",
        "                    dense_units=50,\n",
        "                    learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Build a 1D CNN for time series prediction.\n",
        "    Based on tuned hyperparameters from single station experiment.\n",
        "\n",
        "    params:\n",
        "        timesteps: number of historical hours (12)\n",
        "        features: number of input features\n",
        "        filters_1: filters in first conv layer\n",
        "        filters_2: filters in second conv layer\n",
        "        kernel_size: size of convolutional kernel\n",
        "        dropout_rate: dropout rate for regularisation\n",
        "        dense_units: neurons in dense layer\n",
        "        learning_rate: adam learning rate\n",
        "\n",
        "    returns:\n",
        "        compiled keras model\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        #Input layer\n",
        "        layers.Input(shape=(timesteps, features)),\n",
        "\n",
        "        #first conv layer\n",
        "        layers.Conv1D(\n",
        "            filters=filters_1,\n",
        "            kernel_size=kernel_size,\n",
        "            activation='relu',\n",
        "            padding='causal'\n",
        "        ),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        #2. conv layer\n",
        "        layers.Conv1D(\n",
        "            filters=filters_2,\n",
        "            kernel_size=kernel_size,\n",
        "            activation='relu',\n",
        "            padding='causal'\n",
        "        ),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        #Flatten and dense\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(dense_units, activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        #Output layer single value\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate, clipnorm=1.0),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "793aa747",
      "metadata": {
        "id": "793aa747"
      },
      "outputs": [],
      "source": [
        "#Test model creation\n",
        "test_model = build_cnn_model(timesteps, n_features)\n",
        "print(f'model created with {test_model.count_params():,} parameters')\n",
        "test_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ux3LdbvMMjJH",
      "metadata": {
        "id": "ux3LdbvMMjJH"
      },
      "source": [
        "    model created with 92,197 parameters\n",
        "    Model: \"sequential\"\n",
        "\n",
        "    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
        "    ┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
        "    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
        "    │ conv1d (Conv1D)                 │ (None, 12, 128)        │        37,248 │\n",
        "    ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "    │ dropout (Dropout)               │ (None, 12, 128)        │             0 │\n",
        "    ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "    │ conv1d_1 (Conv1D)               │ (None, 12, 64)         │        16,448 │\n",
        "    ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "    │ dropout_1 (Dropout)             │ (None, 12, 64)         │             0 │\n",
        "    ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "    │ flatten (Flatten)               │ (None, 768)            │             0 │\n",
        "    ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "    │ dense (Dense)                   │ (None, 50)             │        38,450 │\n",
        "    ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "    │ dropout_2 (Dropout)             │ (None, 50)             │             0 │\n",
        "    ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "    │ dense_1 (Dense)                 │ (None, 1)              │            51 │\n",
        "    └─────────────────────────────────┴────────────────────────┴───────────────┘\n",
        "\n",
        "    Total params: 92,197 (360.14 KB)\n",
        "\n",
        "    Trainable params: 92,197 (360.14 KB)\n",
        "\n",
        "    Non-trainable params: 0 (0.00 B)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_summary_explanation",
      "metadata": {
        "id": "model_summary_explanation"
      },
      "source": [
        "### understanding the summary\n",
        "\n",
        "the summary shows each layer, its output shape, and parameter count.\n",
        "\n",
        "| term | meaning |\n",
        "|------|--------|\n",
        "| param # | number of learnable weights. more parameters = more capacity to learn, but also more risk of overfitting |\n",
        "| output shape | (None, timesteps, filters). None is batch size, determined at runtime |\n",
        "| total params | all weights the model will learn during training |\n",
        "\n",
        "the model has 92,197 parameters which is appropriate for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5f6cafe",
      "metadata": {
        "id": "b5f6cafe"
      },
      "source": [
        "## 6)Set-up Training Callbacks\n",
        "\n",
        "Callbacks control training behaviour.\n",
        "\n",
        "| Callback | What it does | Why |\n",
        "|----------|--------------|-----|\n",
        "| EarlyStopping | stops when validation loss stops improving | prevents overfitting |\n",
        "| ReduceLROnPlateau | reduces learning rate when stuck | helps find better minimum |\n",
        "\n",
        "Not using ModelCheckpoint for each model because of 141 targets. Saving checkpoints manually every N models instead.\n",
        "\n",
        "source: Team, K. (no date) Keras Documentation: Callbacks. Available at: https://keras.io/api/callbacks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46459cd",
      "metadata": {
        "id": "c46459cd"
      },
      "outputs": [],
      "source": [
        "def get_callbacks():\n",
        "    \"\"\"Create callbacks for training.\"\"\"\n",
        "    return [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=0.00001,\n",
        "            verbose=0\n",
        "        )\n",
        "    ]\n",
        "\n",
        "print('Callbacks configured:')\n",
        "print('Early stopping (patience=10)')\n",
        "print('Reduce LR on plateau (factor=0.5, patience=5)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ovec-_sDO39e",
      "metadata": {
        "id": "ovec-_sDO39e"
      },
      "source": [
        "  callbacks configured:\n",
        "    - early stopping (patience=10)\n",
        "    - reduce LR on plateau (factor=0.5, patience=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a9a527",
      "metadata": {
        "id": "57a9a527"
      },
      "source": [
        "## 7) Train Models For All Targets\n",
        "\n",
        "Training a separate CNN model for each target. This will take a while because:\n",
        "\n",
        "| aspect | detail |\n",
        "|--------|--------|\n",
        "| targets | 141 station pollutant combinations |\n",
        "| epochs per model | up to 50 (early stopping) |\n",
        "\n",
        "\n",
        "### Checkpoint\n",
        "\n",
        "Saving results every 20 models in case something goes wrong(Since I ran this notebook on Colab this clever approach not work). this way if the notebook crashes, I don't lose everything.\n",
        "\n",
        "\n",
        "source: Geron, A. (2023) Hands on machine learning with scikit learn, Keras and TensorFlow. Ch. 11.\n",
        "\n",
        "**note:** Training was completed on Google Colab. The results are loaded in section 8 manually extracted from the output of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741aae7b",
      "metadata": {
        "id": "741aae7b"
      },
      "outputs": [],
      "source": [
        "#training configuration\n",
        "BATCH_SIZE = 32\n",
        "MAX_EPOCHS = 50\n",
        "CHECKPOINT_EVERY = 20\n",
        "\n",
        "print(f'training configuration:')\n",
        "print(f'  batch size: {BATCH_SIZE}')\n",
        "print(f'  max epochs: {MAX_EPOCHS}')\n",
        "print(f'  checkpoint every: {CHECKPOINT_EVERY} models')\n",
        "print(f'  total targets: {len(target_names)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8y5jyhL3Pppe",
      "metadata": {
        "id": "8y5jyhL3Pppe"
      },
      "source": [
        "  training configuration:\n",
        "    batch size: 32\n",
        "    max epochs: 50\n",
        "    checkpoint every: 20 models\n",
        "    total targets: 141"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P0wkItvAP4_U",
      "metadata": {
        "id": "P0wkItvAP4_U"
      },
      "source": [
        "### Training Loop as Reference Only\n",
        "\n",
        "The training loop below was run on Google Colab and took 5.74 hours to complete. The results are loaded from the output logs in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H90D_lfvP8f4",
      "metadata": {
        "id": "H90D_lfvP8f4"
      },
      "outputs": [],
      "source": [
        "#Training loop reference code\n",
        "results = []\n",
        "all_models = {}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(f'Started at: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "print(f'Targets to train: {len(target_names)}')\n",
        "print(f'Training samples: {n_samples:,}')\n",
        "print(f'Features: {n_features}')\n",
        "print('=' * 40)\n",
        "\n",
        "for i, target_name in enumerate(target_names):\n",
        "    target_idx = target_mapping[target_name]\n",
        "    model_start = time.time()\n",
        "\n",
        "    #Build model\n",
        "    model = build_cnn_model(timesteps, n_features)\n",
        "\n",
        "    #Train\n",
        "    history = model.fit(\n",
        "        X_train, y_train[:, target_idx],\n",
        "        validation_data=(X_val, y_val[:, target_idx]),\n",
        "        epochs=MAX_EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=get_callbacks(),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    #Evaluate on test set\n",
        "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "    y_actual = y_test[:, target_idx]\n",
        "\n",
        "    test_r2 = r2_score(y_actual, y_pred)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
        "    test_mae = mean_absolute_error(y_actual, y_pred)\n",
        "\n",
        "    #Extract pollutant from target name\n",
        "    parts = target_name.rsplit('_', 1)\n",
        "    pollutant = parts[1] if len(parts) > 1 else 'unknown'\n",
        "\n",
        "    results.append({\n",
        "        'target': target_name,\n",
        "        'pollutant': pollutant,\n",
        "        'test_r2': test_r2,\n",
        "        'test_rmse': test_rmse,\n",
        "        'test_mae': test_mae,\n",
        "        'epochs': len(history.history['loss'])\n",
        "    })\n",
        "\n",
        "    #store model\n",
        "    all_models[target_name] = model\n",
        "\n",
        "    #progress update\n",
        "    elapsed = time.time() - model_start\n",
        "    remaining = len(target_names) - (i + 1)\n",
        "    eta = (elapsed * remaining) / 60\n",
        "    print(f'[{i+1:3d}/{len(target_names)}] {target_name:15s} | R2={test_r2:.3f} | Time={elapsed:.0f}s | ETA={eta:.0f}min')\n",
        "\n",
        "    #checkpoint\n",
        "    if (i + 1) % CHECKPOINT_EVERY == 0:\n",
        "        checkpoint_df = pd.DataFrame(results)\n",
        "        checkpoint_df.to_csv(output_dir / f'checkpoint_{i+1}.csv', index=False)\n",
        "        print(f'   [Checkpoint saved at {i+1} models]')\n",
        "\n",
        "    #memory cleanup\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "total_time = (time.time() - start_time) / 60\n",
        "print('=' * 40)\n",
        "print('Training complete!')\n",
        "print(f'Total time: {total_time:.1f} minutes ({total_time/60:.2f} hours)')\n",
        "print(f'Average per model: {total_time*60/len(target_names):.1f} seconds')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training_note",
      "metadata": {
        "id": "training_note"
      },
      "source": [
        "    ============================================================\n",
        "    Started at: 2025-12-31 01:05:47\n",
        "    Targets to train: 141\n",
        "    Training samples: 17,107\n",
        "    Features: 145\n",
        "    ============================================================\n",
        "    [  1/141] BG1_NO2         | R2=0.585 | Time=140s | ETA=326min\n",
        "    [  2/141] BG1_SO2         | R2=0.656 | Time=119s | ETA=300min\n",
        "    [  3/141] BG2_NO2         | R2=-1495952367336140489190212108288.000 | Time=134s | ETA=302min\n",
        "    [  4/141] BG2_PM10        | R2=0.023 | Time=70s | ETA=265min\n",
        "    [  5/141] BQ7_NO2         | R2=0.706 | Time=80s | ETA=247min\n",
        "    [  6/141] BQ7_O3          | R2=0.910 | Time=172s | ETA=269min\n",
        "    [  7/141] BQ7_PM10        | R2=0.760 | Time=108s | ETA=263min\n",
        "    [  8/141] BQ7_PM25        | R2=0.758 | Time=115s | ETA=261min\n",
        "    [  9/141] BQ9_PM10        | R2=0.730 | Time=133s | ETA=263min\n",
        "    [ 10/141] BQ9_PM25        | R2=0.726 | Time=147s | ETA=267min\n",
        "    [ 11/141] BT4_NO2         | R2=0.804 | Time=164s | ETA=273min\n",
        "    [ 12/141] BT4_PM10        | R2=0.554 | Time=135s | ETA=273min\n",
        "    [ 13/141] BT4_PM25        | R2=0.522 | Time=113s | ETA=268min\n",
        "    [ 14/141] BT5_NO2         | R2=0.747 | Time=159s | ETA=271min\n",
        "    [ 15/141] BT5_PM10        | R2=0.525 | Time=94s | ETA=265min\n",
        "    [ 16/141] BT5_PM25        | R2=0.164 | Time=91s | ETA=258min\n",
        "    [ 17/141] BT6_NO2         | R2=0.784 | Time=144s | ETA=259min\n",
        "    [ 18/141] BT6_PM10        | R2=0.507 | Time=218s | ETA=267min\n",
        "    [ 19/141] BT6_PM25        | R2=0.463 | Time=157s | ETA=268min\n",
        "    [ 20/141] BT8_NO2         | R2=0.729 | Time=131s | ETA=266min\n",
        "      [Checkpoint saved at 20 models]\n",
        "    [ 21/141] BT8_PM10        | R2=0.551 | Time=182s | ETA=269min\n",
        "    [ 22/141] BT8_PM25        | R2=0.579 | Time=111s | ETA=264min\n",
        "    [ 23/141] BX1_NO2         | R2=0.788 | Time=171s | ETA=265min\n",
        "    [ 24/141] BX1_O3          | R2=0.891 | Time=160s | ETA=265min\n",
        "    [ 25/141] BX1_SO2         | R2=0.701 | Time=195s | ETA=268min\n",
        "    [ 26/141] BX2_NO2         | R2=0.761 | Time=142s | ETA=266min\n",
        "    [ 27/141] BX2_PM10        | R2=0.554 | Time=191s | ETA=267min\n",
        "    [ 28/141] BX2_PM25        | R2=0.777 | Time=133s | ETA=264min\n",
        "    [ 29/141] BY7_NO2         | R2=0.722 | Time=137s | ETA=262min\n",
        "    [ 30/141] BY7_PM10        | R2=0.416 | Time=104s | ETA=257min\n",
        "    [ 31/141] BY7_PM25        | R2=0.462 | Time=145s | ETA=255min\n",
        "    [ 32/141] CD1_NO2         | R2=0.682 | Time=139s | ETA=253min\n",
        "    [ 33/141] CD1_PM10        | R2=0.554 | Time=141s | ETA=251min\n",
        "    [ 34/141] CD1_PM25        | R2=0.551 | Time=202s | ETA=252min\n",
        "    [ 35/141] CE2_NO2         | R2=0.509 | Time=73s | ETA=246min\n",
        "    [ 36/141] CE2_O3          | R2=0.887 | Time=228s | ETA=248min\n",
        "    [ 37/141] CE2_PM10        | R2=0.466 | Time=64s | ETA=242min\n",
        "    [ 38/141] CE2_PM25        | R2=0.701 | Time=91s | ETA=238min\n",
        "    [ 39/141] CE3_NO2         | R2=0.378 | Time=66s | ETA=232min\n",
        "    [ 40/141] CE3_PM10        | R2=0.434 | Time=179s | ETA=232min\n",
        "      [Checkpoint saved at 40 models]\n",
        "    [ 41/141] CE3_PM25        | R2=0.505 | Time=128s | ETA=229min\n",
        "    [ 42/141] CR5_NO2         | R2=0.821 | Time=152s | ETA=228min\n",
        "    [ 43/141] CR7_NO2         | R2=0.799 | Time=215s | ETA=228min\n",
        "    [ 44/141] CR8_PM25        | R2=0.000 | Time=215s | ETA=229min\n",
        "    [ 45/141] CW3_NO2         | R2=0.740 | Time=132s | ETA=226min\n",
        "    [ 46/141] CW3_PM10        | R2=0.844 | Time=204s | ETA=226min\n",
        "    [ 47/141] CW3_PM25        | R2=0.872 | Time=173s | ETA=225min\n",
        "    [ 48/141] EA6_NO2         | R2=0.787 | Time=160s | ETA=223min\n",
        "    [ 49/141] EA6_PM10        | R2=0.550 | Time=194s | ETA=222min\n",
        "    [ 50/141] EA8_NO2         | R2=0.757 | Time=136s | ETA=219min\n",
        "    [ 51/141] EA8_PM10        | R2=0.609 | Time=109s | ETA=216min\n",
        "    [ 52/141] EI1_NO2         | R2=0.797 | Time=144s | ETA=214min\n",
        "    [ 53/141] EI1_PM10        | R2=0.523 | Time=229s | ETA=214min\n",
        "    [ 54/141] EI8_PM10        | R2=0.606 | Time=132s | ETA=211min\n",
        "    [ 55/141] EN1_NO2         | R2=0.851 | Time=149s | ETA=209min\n",
        "    [ 56/141] EN4_NO2         | R2=0.777 | Time=151s | ETA=206min\n",
        "    [ 57/141] EN5_NO2         | R2=0.810 | Time=122s | ETA=203min\n",
        "    [ 58/141] EN7_NO2         | R2=0.765 | Time=165s | ETA=202min\n",
        "    [ 59/141] GB0_PM25        | R2=0.649 | Time=110s | ETA=198min\n",
        "    [ 60/141] GB6_NO2         | R2=0.824 | Time=133s | ETA=196min\n",
        "      [Checkpoint saved at 60 models]\n",
        "    [ 61/141] GB6_O3          | R2=0.891 | Time=216s | ETA=195min\n",
        "    [ 62/141] GB6_PM10        | R2=0.504 | Time=144s | ETA=192min\n",
        "    [ 63/141] GN0_NO2         | R2=0.780 | Time=88s | ETA=189min\n",
        "    [ 64/141] GN0_PM10        | R2=0.475 | Time=124s | ETA=186min\n",
        "    [ 65/141] GN0_PM25        | R2=0.571 | Time=128s | ETA=183min\n",
        "    [ 66/141] GN3_NO2         | R2=0.786 | Time=124s | ETA=180min\n",
        "    [ 67/141] GN3_O3          | R2=0.884 | Time=190s | ETA=179min\n",
        "    [ 68/141] GN3_PM10        | R2=0.334 | Time=127s | ETA=176min\n",
        "    [ 69/141] GN3_PM25        | R2=0.706 | Time=111s | ETA=173min\n",
        "    [ 70/141] GN4_NO2         | R2=0.802 | Time=152s | ETA=171min\n",
        "    [ 71/141] GN4_PM10        | R2=0.537 | Time=168s | ETA=169min\n",
        "    [ 72/141] GN5_NO2         | R2=0.691 | Time=102s | ETA=166min\n",
        "    [ 73/141] GN5_PM10        | R2=0.559 | Time=168s | ETA=164min\n",
        "    [ 74/141] GN6_NO2         | R2=0.761 | Time=197s | ETA=162min\n",
        "    [ 75/141] GN6_PM10        | R2=0.607 | Time=107s | ETA=159min\n",
        "    [ 76/141] GN6_PM25        | R2=-0.731 | Time=143s | ETA=157min\n",
        "    [ 77/141] GR7_NO2         | R2=0.758 | Time=232s | ETA=156min\n",
        "    [ 78/141] GR7_PM10        | R2=0.602 | Time=179s | ETA=154min\n",
        "    [ 79/141] GR8_NO2         | R2=0.749 | Time=119s | ETA=151min\n",
        "    [ 80/141] GR8_PM10        | R2=0.543 | Time=136s | ETA=149min\n",
        "      [Checkpoint saved at 80 models]\n",
        "    [ 81/141] GR9_NO2         | R2=0.839 | Time=191s | ETA=147min\n",
        "    [ 82/141] GR9_PM10        | R2=0.592 | Time=213s | ETA=145min\n",
        "    [ 83/141] GR9_PM25        | R2=0.582 | Time=140s | ETA=143min\n",
        "    [ 84/141] HG1_NO2         | R2=0.734 | Time=174s | ETA=140min\n",
        "    [ 85/141] HG4_NO2         | R2=0.860 | Time=138s | ETA=138min\n",
        "    [ 86/141] HG4_O3          | R2=0.919 | Time=211s | ETA=136min\n",
        "    [ 87/141] HP1_NO2         | R2=0.745 | Time=71s | ETA=133min\n",
        "    [ 88/141] HP1_O3          | R2=0.902 | Time=134s | ETA=130min\n",
        "    [ 89/141] HP1_PM10        | R2=0.803 | Time=169s | ETA=128min\n",
        "    [ 90/141] HP1_PM25        | R2=0.847 | Time=210s | ETA=126min\n",
        "    [ 91/141] HV1_NO2         | R2=0.774 | Time=134s | ETA=124min\n",
        "    [ 92/141] HV3_NO2         | R2=0.733 | Time=112s | ETA=121min\n",
        "    [ 93/141] HV3_PM10        | R2=0.777 | Time=231s | ETA=119min\n",
        "    [ 94/141] IS2_NO2         | R2=0.715 | Time=158s | ETA=117min\n",
        "    [ 95/141] IS2_PM10        | R2=0.273 | Time=91s | ETA=114min\n",
        "    [ 96/141] IS6_NO2         | R2=0.775 | Time=161s | ETA=111min\n",
        "    [ 97/141] IS6_PM10        | R2=0.310 | Time=228s | ETA=110min\n",
        "    [ 98/141] KC1_CO          | R2=0.483 | Time=99s | ETA=107min\n",
        "    [ 99/141] KC1_NO2         | R2=0.845 | Time=187s | ETA=105min\n",
        "    [100/141] KC1_O3          | R2=0.895 | Time=153s | ETA=102min\n",
        "      [Checkpoint saved at 100 models]\n",
        "    [101/141] KC1_PM25        | R2=0.839 | Time=108s | ETA=99min\n",
        "    [102/141] KC1_SO2         | R2=0.803 | Time=176s | ETA=97min\n",
        "    [103/141] LB4_NO2         | R2=0.566 | Time=120s | ETA=94min\n",
        "    [104/141] LB4_PM10        | R2=0.450 | Time=160s | ETA=92min\n",
        "    [105/141] LB4_PM25        | R2=0.405 | Time=176s | ETA=90min\n",
        "    [106/141] LB6_NO2         | R2=0.840 | Time=138s | ETA=87min\n",
        "    [107/141] LB6_PM10        | R2=0.248 | Time=99s | ETA=84min\n",
        "    [108/141] ME9_NO2         | R2=0.764 | Time=159s | ETA=82min\n",
        "    [109/141] MY1_CO          | R2=0.830 | Time=144s | ETA=79min\n",
        "    [110/141] MY1_NO2         | R2=0.722 | Time=126s | ETA=77min\n",
        "    [111/141] MY1_O3          | R2=0.857 | Time=106s | ETA=74min\n",
        "    [112/141] MY1_SO2         | R2=0.077 | Time=226s | ETA=72min\n",
        "    [113/141] RI1_NO2         | R2=0.668 | Time=99s | ETA=69min\n",
        "    [114/141] RI1_PM10        | R2=0.287 | Time=111s | ETA=67min\n",
        "    [115/141] RI2_NO2         | R2=0.804 | Time=137s | ETA=64min\n",
        "    [116/141] RI2_O3          | R2=0.906 | Time=151s | ETA=62min\n",
        "    [117/141] RI2_PM10        | R2=0.453 | Time=107s | ETA=59min\n",
        "    [118/141] SK5_NO2         | R2=0.836 | Time=156s | ETA=57min\n",
        "    [119/141] SK5_PM10        | R2=0.634 | Time=133s | ETA=54min\n",
        "    [120/141] TH2_NO2         | R2=0.830 | Time=177s | ETA=52min\n",
        "      [Checkpoint saved at 120 models]\n",
        "    [121/141] TH4_NO2         | R2=-243629731975496583525961826304.000 | Time=183s | ETA=50min\n",
        "    [122/141] TH4_O3          | R2=-859351937836635163572058456064.000 | Time=171s | ETA=47min\n",
        "    [123/141] TH4_PM10        | R2=-460376520887681331232768000000.000 | Time=101s | ETA=45min\n",
        "    [124/141] TH4_PM25        | R2=0.000 | Time=201s | ETA=42min\n",
        "    [125/141] TL4_NO2         | R2=0.630 | Time=171s | ETA=40min\n",
        "    [126/141] TL5_NO2         | R2=0.420 | Time=116s | ETA=37min\n",
        "    [127/141] TL6_NO2         | R2=0.696 | Time=112s | ETA=35min\n",
        "    [128/141] TL6_PM25        | R2=0.241 | Time=68s | ETA=32min\n",
        "    [129/141] WA7_NO2         | R2=0.530 | Time=131s | ETA=30min\n",
        "    [130/141] WA7_PM10        | R2=0.398 | Time=109s | ETA=27min\n",
        "    [131/141] WA9_PM10        | R2=0.608 | Time=178s | ETA=25min\n",
        "    [132/141] WAA_NO2         | R2=0.701 | Time=144s | ETA=22min\n",
        "    [133/141] WAA_PM10        | R2=0.527 | Time=112s | ETA=20min\n",
        "    [134/141] WAB_NO2         | R2=0.798 | Time=86s | ETA=17min\n",
        "    [135/141] WAB_PM10        | R2=0.378 | Time=66s | ETA=15min\n",
        "    [136/141] WAC_PM10        | R2=0.539 | Time=102s | ETA=12min\n",
        "    [137/141] WM5_NO2         | R2=0.802 | Time=148s | ETA=10min\n",
        "    [138/141] WM6_NO2         | R2=0.632 | Time=120s | ETA=7min\n",
        "    [139/141] WM6_PM10        | R2=-5526321308161753884842262528.000 | Time=221s | ETA=5min\n",
        "    [140/141] WMD_NO2         | R2=0.534 | Time=87s | ETA=2min\n",
        "      [Checkpoint saved at 140 models]\n",
        "    [141/141] WMD_PM25        | R2=0.700 | Time=183s | ETA=0min\n",
        "    ============================================================\n",
        "    Training complete!\n",
        "    Total time: 344.7 minutes (5.74 hours)\n",
        "    Average per model: 146.7 seconds\n",
        "    ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "results_section",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## 8) Load Results Colab Training Output\n",
        "\n",
        "The training was completed on Google Colab on 31 December 2025. the results below are extracted from the training output logs, manually added.\n",
        "\n",
        "total training time: 344.7 minutes (5.74 hours)\n",
        "\n",
        "average time per model: 146.7 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "results_from_colab",
      "metadata": {
        "id": "results_from_colab"
      },
      "outputs": [],
      "source": [
        "#Results extracted from colab output manually format target_name, test_r2\n",
        "colab_results = [\n",
        "    ('BG1_NO2', 0.585),\n",
        "    ('BG1_SO2', 0.656),\n",
        "    ('BG2_NO2', -1.496e+30),\n",
        "    ('BG2_PM10', 0.023),\n",
        "    ('BQ7_NO2', 0.706),\n",
        "    ('BQ7_O3', 0.910),\n",
        "    ('BQ7_PM10', 0.760),\n",
        "    ('BQ7_PM25', 0.758),\n",
        "    ('BQ9_PM10', 0.730),\n",
        "    ('BQ9_PM25', 0.726),\n",
        "    ('BT4_NO2', 0.804),\n",
        "    ('BT4_PM10', 0.554),\n",
        "    ('BT4_PM25', 0.522),\n",
        "    ('BT5_NO2', 0.747),\n",
        "    ('BT5_PM10', 0.525),\n",
        "    ('BT5_PM25', 0.164),\n",
        "    ('BT6_NO2', 0.784),\n",
        "    ('BT6_PM10', 0.507),\n",
        "    ('BT6_PM25', 0.463),\n",
        "    ('BT8_NO2', 0.729),\n",
        "    ('BT8_PM10', 0.551),\n",
        "    ('BT8_PM25', 0.579),\n",
        "    ('BX1_NO2', 0.788),\n",
        "    ('BX1_O3', 0.891),\n",
        "    ('BX1_SO2', 0.701),\n",
        "    ('BX2_NO2', 0.761),\n",
        "    ('BX2_PM10', 0.554),\n",
        "    ('BX2_PM25', 0.777),\n",
        "    ('BY7_NO2', 0.722),\n",
        "    ('BY7_PM10', 0.416),\n",
        "    ('BY7_PM25', 0.462),\n",
        "    ('CD1_NO2', 0.682),\n",
        "    ('CD1_PM10', 0.554),\n",
        "    ('CD1_PM25', 0.551),\n",
        "    ('CE2_NO2', 0.509),\n",
        "    ('CE2_O3', 0.887),\n",
        "    ('CE2_PM10', 0.466),\n",
        "    ('CE2_PM25', 0.701),\n",
        "    ('CE3_NO2', 0.378),\n",
        "    ('CE3_PM10', 0.434),\n",
        "    ('CE3_PM25', 0.505),\n",
        "    ('CR5_NO2', 0.821),\n",
        "    ('CR7_NO2', 0.799),\n",
        "    ('CR8_PM25', 0.000),\n",
        "    ('CW3_NO2', 0.740),\n",
        "    ('CW3_PM10', 0.844),\n",
        "    ('CW3_PM25', 0.872),\n",
        "    ('EA6_NO2', 0.787),\n",
        "    ('EA6_PM10', 0.550),\n",
        "    ('EA8_NO2', 0.757),\n",
        "    ('EA8_PM10', 0.609),\n",
        "    ('EI1_NO2', 0.797),\n",
        "    ('EI1_PM10', 0.523),\n",
        "    ('EI8_PM10', 0.606),\n",
        "    ('EN1_NO2', 0.851),\n",
        "    ('EN4_NO2', 0.777),\n",
        "    ('EN5_NO2', 0.810),\n",
        "    ('EN7_NO2', 0.765),\n",
        "    ('GB0_PM25', 0.649),\n",
        "    ('GB6_NO2', 0.824),\n",
        "    ('GB6_O3', 0.891),\n",
        "    ('GB6_PM10', 0.504),\n",
        "    ('GN0_NO2', 0.780),\n",
        "    ('GN0_PM10', 0.475),\n",
        "    ('GN0_PM25', 0.571),\n",
        "    ('GN3_NO2', 0.786),\n",
        "    ('GN3_O3', 0.884),\n",
        "    ('GN3_PM10', 0.334),\n",
        "    ('GN3_PM25', 0.706),\n",
        "    ('GN4_NO2', 0.802),\n",
        "    ('GN4_PM10', 0.537),\n",
        "    ('GN5_NO2', 0.691),\n",
        "    ('GN5_PM10', 0.559),\n",
        "    ('GN6_NO2', 0.761),\n",
        "    ('GN6_PM10', 0.607),\n",
        "    ('GN6_PM25', -0.731),\n",
        "    ('GR7_NO2', 0.758),\n",
        "    ('GR7_PM10', 0.602),\n",
        "    ('GR8_NO2', 0.749),\n",
        "    ('GR8_PM10', 0.543),\n",
        "    ('GR9_NO2', 0.839),\n",
        "    ('GR9_PM10', 0.592),\n",
        "    ('GR9_PM25', 0.582),\n",
        "    ('HG1_NO2', 0.734),\n",
        "    ('HG4_NO2', 0.860),\n",
        "    ('HG4_O3', 0.919),\n",
        "    ('HP1_NO2', 0.745),\n",
        "    ('HP1_O3', 0.902),\n",
        "    ('HP1_PM10', 0.803),\n",
        "    ('HP1_PM25', 0.847),\n",
        "    ('HV1_NO2', 0.774),\n",
        "    ('HV3_NO2', 0.733),\n",
        "    ('HV3_PM10', 0.777),\n",
        "    ('IS2_NO2', 0.715),\n",
        "    ('IS2_PM10', 0.273),\n",
        "    ('IS6_NO2', 0.775),\n",
        "    ('IS6_PM10', 0.310),\n",
        "    ('KC1_CO', 0.483),\n",
        "    ('KC1_NO2', 0.845),\n",
        "    ('KC1_O3', 0.895),\n",
        "    ('KC1_PM25', 0.839),\n",
        "    ('KC1_SO2', 0.803),\n",
        "    ('LB4_NO2', 0.566),\n",
        "    ('LB4_PM10', 0.450),\n",
        "    ('LB4_PM25', 0.405),\n",
        "    ('LB6_NO2', 0.840),\n",
        "    ('LB6_PM10', 0.248),\n",
        "    ('ME9_NO2', 0.764),\n",
        "    ('MY1_CO', 0.830),\n",
        "    ('MY1_NO2', 0.722),\n",
        "    ('MY1_O3', 0.857),\n",
        "    ('MY1_SO2', 0.077),\n",
        "    ('RI1_NO2', 0.668),\n",
        "    ('RI1_PM10', 0.287),\n",
        "    ('RI2_NO2', 0.804),\n",
        "    ('RI2_O3', 0.906),\n",
        "    ('RI2_PM10', 0.453),\n",
        "    ('SK5_NO2', 0.836),\n",
        "    ('SK5_PM10', 0.634),\n",
        "    ('TH2_NO2', 0.830),\n",
        "    ('TH4_NO2', -2.436e+29),\n",
        "    ('TH4_O3', -8.594e+29),\n",
        "    ('TH4_PM10', -4.604e+29),\n",
        "    ('TH4_PM25', 0.000),\n",
        "    ('TL4_NO2', 0.630),\n",
        "    ('TL5_NO2', 0.420),\n",
        "    ('TL6_NO2', 0.696),\n",
        "    ('TL6_PM25', 0.241),\n",
        "    ('WA7_NO2', 0.530),\n",
        "    ('WA7_PM10', 0.398),\n",
        "    ('WA9_PM10', 0.608),\n",
        "    ('WAA_NO2', 0.701),\n",
        "    ('WAA_PM10', 0.527),\n",
        "    ('WAB_NO2', 0.798),\n",
        "    ('WAB_PM10', 0.378),\n",
        "    ('WAC_PM10', 0.539),\n",
        "    ('WM5_NO2', 0.802),\n",
        "    ('WM6_NO2', 0.632),\n",
        "    ('WM6_PM10', -5.526e+27),\n",
        "    ('WMD_NO2', 0.534),\n",
        "    ('WMD_PM25', 0.700)\n",
        "]\n",
        "\n",
        "print(f'loaded {len(colab_results)} results from colab training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_results_df",
      "metadata": {
        "id": "create_results_df"
      },
      "outputs": [],
      "source": [
        "#create results dataframe\n",
        "results = []\n",
        "for target, r2 in colab_results:\n",
        "    parts = target.rsplit('_', 1)\n",
        "    pollutant = parts[1] if len(parts) > 1 else 'unknown'\n",
        "    site = parts[0] if len(parts) > 1 else target\n",
        "\n",
        "    results.append({\n",
        "        'target': target,\n",
        "        'site': site,\n",
        "        'pollutant': pollutant,\n",
        "        'test_r2': r2\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print('Results dataframe created')\n",
        "print(f'\\nShape: {results_df.shape}')\n",
        "print(f'\\nFirst 5 rows:')\n",
        "print(results_df.head().to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "broken_models_section",
      "metadata": {
        "id": "broken_models_section"
      },
      "source": [
        "## 9) Investigation of Broken Models\n",
        "\n",
        "Some models produced extremely negative R2 values, indicating numerical issues. Before continuing with results analysis, I need to investigate and document these failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "identify_broken",
      "metadata": {
        "id": "identify_broken"
      },
      "outputs": [],
      "source": [
        "#Identify broken models\n",
        "print('Identifying broken models:')\n",
        "print('=' * 40)\n",
        "\n",
        "#Broken threshold R2 < -10 is num failure\n",
        "broken_threshold = -10\n",
        "\n",
        "broken_models = results_df[results_df['test_r2'] < broken_threshold].copy()\n",
        "valid_models = results_df[results_df['test_r2'] >= broken_threshold].copy()\n",
        "\n",
        "print(f'\\nTotal Models:  {len(results_df)}')\n",
        "print(f'Valid Models:  {len(valid_models)}')\n",
        "print(f'Broken Models: {len(broken_models)}')\n",
        "\n",
        "print('Broken Model Informations:')\n",
        "print('-' * 40)\n",
        "print(broken_models[['target', 'pollutant', 'test_r2']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "investigate_broken",
      "metadata": {
        "id": "investigate_broken"
      },
      "outputs": [],
      "source": [
        "#Investigate broken models by checking test set variance\n",
        "print('Detailed investigation of broken models:')\n",
        "print('=' * 40)\n",
        "\n",
        "broken_targets = broken_models['target'].values\n",
        "\n",
        "for target in broken_targets:\n",
        "    print(f'\\n>>> {target}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    target_idx = target_mapping[target]\n",
        "\n",
        "    #Check target data statistics\n",
        "    y_train_target = y_train[:, target_idx]\n",
        "    y_val_target = y_val[:, target_idx]\n",
        "    y_test_target = y_test[:, target_idx]\n",
        "\n",
        "    print(f'training   - min: {y_train_target.min():.6f}, max: {y_train_target.max():.6f}, '\n",
        "          f'std: {y_train_target.std():.6f}')\n",
        "    print(f'validation - min: {y_val_target.min():.6f}, max: {y_val_target.max():.6f}, '\n",
        "          f'std: {y_val_target.std():.6f}')\n",
        "    print(f'test       - min: {y_test_target.min():.6f}, max: {y_test_target.max():.6f}, '\n",
        "          f'std: {y_test_target.std():.6f}')\n",
        "\n",
        "    #Check for constant or near constant values\n",
        "    if y_test_target.std() < 0.001:\n",
        "        print('>>> very low variance in test set (std < 0.001)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "root_cause",
      "metadata": {
        "id": "root_cause"
      },
      "outputs": [],
      "source": [
        "#Root cause analysis\n",
        "print('Root cause analysis of broken models:')\n",
        "print('=' * 40)\n",
        "\n",
        "#Check broken models site\n",
        "broken_sites = [t.rsplit('_', 1)[0] for t in broken_targets]\n",
        "print(f'\\nBroken model sites: {broken_sites}')\n",
        "\n",
        "#Count and addup if the sites have multiple failures\n",
        "site_counts = Counter(broken_sites)\n",
        "multi_broken = {site: count for site, count in site_counts.items() if count > 1}\n",
        "\n",
        "if multi_broken:\n",
        "    print(f'\\nSites with multiple broken models: {multi_broken}')\n",
        "    print('This suggests data quality issues at these monitoring stations.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "broken_findings",
      "metadata": {
        "id": "broken_findings"
      },
      "source": [
        "### Findings: Why these models broke\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baseline_eval_section",
      "metadata": {
        "id": "baseline_eval_section"
      },
      "source": [
        "## 10) Baseline Evaluation After Exclusion\n",
        "\n",
        "Evaluating the ... valid models, excluding the ... broken models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baseline_eval",
      "metadata": {
        "id": "baseline_eval"
      },
      "outputs": [],
      "source": [
        "#Baseline evaluation excluding broken models\n",
        "print('CNN baseline evaluation excluding broken models:')\n",
        "print('=' * 40)\n",
        "\n",
        "print(f'\\nValid models: {len(valid_models)} out of {len(results_df)}')\n",
        "print(f'Broken models excluded: {len(broken_models)}')\n",
        "\n",
        "print('Test set performance with valid models only:')\n",
        "print('-' * 40)\n",
        "\n",
        "print(f'\\nMean R2:   {valid_models[\"test_r2\"].mean():.4f}')\n",
        "print(f'Median R2: {valid_models[\"test_r2\"].median():.4f}')\n",
        "print(f'Std R2:    {valid_models[\"test_r2\"].std():.4f}')\n",
        "print(f'Min R2:    {valid_models[\"test_r2\"].min():.4f}')\n",
        "print(f'Max R2:    {valid_models[\"test_r2\"].max():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pollutant_summary",
      "metadata": {
        "id": "pollutant_summary"
      },
      "outputs": [],
      "source": [
        "#Performance by pollutant type\n",
        "print('\\nPerformance by pollutant type:')\n",
        "print('=' * 40)\n",
        "\n",
        "pollutant_summary = valid_models.groupby('pollutant').agg({\n",
        "    'test_r2': ['mean', 'std', 'min', 'max', 'count']\n",
        "}).round(4)\n",
        "\n",
        "pollutant_summary.columns = ['r2_mean', 'r2_std', 'r2_min', 'r2_max', 'n_models']\n",
        "pollutant_summary = pollutant_summary.sort_values('r2_mean', ascending=False)\n",
        "\n",
        "print(pollutant_summary.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "top_bottom_models",
      "metadata": {
        "id": "top_bottom_models"
      },
      "outputs": [],
      "source": [
        "#Top/bottom performing models\n",
        "print('\\nTop 10 best performing targets by R2:')\n",
        "print('-' * 50)\n",
        "top_10 = valid_models.nlargest(10, 'test_r2')[['target', 'pollutant', 'test_r2']]\n",
        "print(top_10.to_string(index=False))\n",
        "\n",
        "print('\\nBottom 10 worst performing targets by R2:')\n",
        "print('-' * 50)\n",
        "bottom_10 = valid_models.nsmallest(10, 'test_r2')[['target', 'pollutant', 'test_r2']]\n",
        "print(bottom_10.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save_section",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "## 11) results summary and save\n",
        "\n",
        "saving all results to csv files for later analysis and comparison with random forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save_results",
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "#Save results\n",
        "results_df.to_csv(output_dir / 'cnn_all_results.csv', index=False)\n",
        "valid_models.to_csv(output_dir / 'cnn_valid_results.csv', index=False)\n",
        "broken_models.to_csv(output_dir / 'cnn_broken_models.csv', index=False)\n",
        "pollutant_summary.to_csv(output_dir / 'cnn_pollutant_summary.csv')\n",
        "\n",
        "print('results saved:')\n",
        "print(f'  - cnn_all_results.csv ({len(results_df)} models)')\n",
        "print(f'  - cnn_valid_results.csv ({len(valid_models)} models)')\n",
        "print(f'  - cnn_broken_models.csv ({len(broken_models)} models)')\n",
        "print(f'  - cnn_pollutant_summary.csv')\n",
        "print(f'\\noutput directory: {output_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vis_section",
      "metadata": {
        "id": "vis_section"
      },
      "source": [
        "## 12) prediction visualisations\n",
        "\n",
        "plotting actual vs predicted values helps identify systematic errors or patterns the models miss.\n",
        "\n",
        "**scatter plot interpretation:**\n",
        "\n",
        "| pattern | meaning |\n",
        "|---------|--------|\n",
        "| points close to diagonal | good predictions |\n",
        "| spread around line | prediction variance |\n",
        "| curve away at high values | model underestimates pollution spikes |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r2_distribution",
      "metadata": {
        "id": "r2_distribution"
      },
      "outputs": [],
      "source": [
        "#R2 distribution histogram and boxplot by pollutant exclude models with R2 < 0 for better visualisation\n",
        "valid_for_plot = valid_models[valid_models['test_r2'] >= 0].copy()\n",
        "print(f'models for visualisation: {len(valid_for_plot)} (excluding {len(valid_models) - len(valid_for_plot)} with R2 < 0)')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "#Histogram\n",
        "axes[0].hist(valid_for_plot['test_r2'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[0].axvline(valid_for_plot['test_r2'].mean(), color='red', linestyle='--',\n",
        "                label=f'mean = {valid_for_plot[\"test_r2\"].mean():.3f}')\n",
        "axes[0].set_xlabel('Test R2')\n",
        "axes[0].set_ylabel('Number of models')\n",
        "axes[0].set_title('Distribution of test R2 valid models')\n",
        "axes[0].legend()\n",
        "\n",
        "#Boxplot by pollutant\n",
        "pollutant_order = ['O3', 'NO2', 'PM25', 'PM10', 'CO', 'SO2']\n",
        "box_data = [valid_for_plot[valid_for_plot['pollutant'] == p]['test_r2'].values\n",
        "            for p in pollutant_order if p in valid_for_plot['pollutant'].values]\n",
        "box_labels = [p for p in pollutant_order if p in valid_for_plot['pollutant'].values]\n",
        "\n",
        "axes[1].boxplot(box_data, tick_labels=box_labels)\n",
        "axes[1].set_xlabel('pollutant')\n",
        "axes[1].set_ylabel('test R2')\n",
        "axes[1].set_title('Test R2 by pollutant type:')\n",
        "axes[1].axhline(0.8, color='green', linestyle='--', alpha=0.5, label='R2 = 0.8')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'cnn_r2_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Saved cnn_r2_distribution.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r2_interpretation",
      "metadata": {
        "id": "r2_interpretation"
      },
      "source": [
        "### Obserations for R2 Distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "residual_section",
      "metadata": {
        "id": "residual_section"
      },
      "source": [
        "## 13) Residual Analysis\n",
        "\n",
        "Residuals are the difference between actual and predicted values. If the model is good, residuals should scatter randomly around zero with no pattern.\n",
        "\n",
        "Residual = Actual - Predicted\n",
        "\n",
        "source: Effect of transforming the targets in regression model (no date) scikit. Available at: https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
        "\n",
        "**Note:** Since trained models are not available locally, residual analysis would require retraining. this section shows the methodology that would be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "residual_methodology",
      "metadata": {
        "id": "residual_methodology"
      },
      "outputs": [],
      "source": [
        "#Residual analysis methodology reference code\n",
        "print('Residual analysis methodology:')\n",
        "print('=' * 40)\n",
        "print('''\n",
        "to perform residual analysis, the following code would be used\n",
        "with a trained model:\n",
        "\n",
        "```python\n",
        "#Get predictions\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "y_actual = y_test[:, target_idx]\n",
        "\n",
        "#Calculate residuals\n",
        "residuals = y_actual - y_pred\n",
        "\n",
        "#Plot residuals\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "#Residuals vs predicted\n",
        "axes[0].scatter(y_pred, residuals, alpha=0.3)\n",
        "axes[0].axhline(y=0, color='r', linestyle='--')\n",
        "axes[0].set_xlabel('predicted')\n",
        "axes[0].set_ylabel('residual')\n",
        "axes[0].set_title('residuals vs predicted')\n",
        "\n",
        "#Residual histogram\n",
        "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
        "axes[1].set_xlabel('residual')\n",
        "axes[1].set_ylabel('frequency')\n",
        "axes[1].set_title('residual distribution')\n",
        "```\n",
        "\n",
        "expected patterns:\n",
        "- good model: residuals randomly scattered around zero\n",
        "- underfitting: systematic patterns in residuals\n",
        "- heteroscedasticity: residual spread increases with predicted value\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final_summary_section",
      "metadata": {
        "id": "final_summary_section"
      },
      "source": [
        "## 14) Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "final_summary",
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "#Final summary\n",
        "print('CNN model training summary all LAQN targets')\n",
        "print('=' * 40)\n",
        "\n",
        "print(f'\\nDataset:')\n",
        "print(f'  Training samples:   {X_train.shape[0]:,}')\n",
        "print(f'  Validation samples: {X_val.shape[0]:,}')\n",
        "print(f'  Test samples:       {X_test.shape[0]:,}')\n",
        "print(f'  Features:           {X_train.shape[2]:,}')\n",
        "print(f'  Timesteps:          {X_train.shape[1]}')\n",
        "\n",
        "print(f'\\Models:')\n",
        "print(f'  Total trained:      {len(results_df)}')\n",
        "print(f'  Valid models:       {len(valid_models)}')\n",
        "print(f'  Broken models:      {len(broken_models)} (excluded due to data quality issues)')\n",
        "\n",
        "print(f'\\nHyperparameters used:')\n",
        "print(f'  filters_1:     128')\n",
        "print(f'  filters_2:     64')\n",
        "print(f'  kernel_size:   2')\n",
        "print(f'  dropout:       0.1')\n",
        "print(f'  dense_units:   50')\n",
        "print(f'  learning_rate: 0.001')\n",
        "\n",
        "print(f'\\nTest set performance for only valid models:')\n",
        "print(f'  Mean R2:   {valid_models[\"test_r2\"].mean():.4f} (+/- {valid_models[\"test_r2\"].std():.4f})')\n",
        "print(f'  Median R2: {valid_models[\"test_r2\"].median():.4f}')\n",
        "\n",
        "#Best performed pollutant\n",
        "best_poll = pollutant_summary['r2_mean'].idxmax()\n",
        "best_poll_r2 = pollutant_summary.loc[best_poll, 'r2_mean']\n",
        "print(f'\\nBest performing pollutant: {best_poll} (mean R2 = {best_poll_r2:.4f})')\n",
        "\n",
        "#Best performed individual model\n",
        "best_idx = valid_models['test_r2'].idxmax()\n",
        "best_target = valid_models.loc[best_idx, 'target']\n",
        "best_r2 = valid_models.loc[best_idx, 'test_r2']\n",
        "print(f'Best individual model: {best_target} (R2 = {best_r2:.4f})')\n",
        "\n",
        "print(f'\\noutputs saved to: {output_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "### key findings\n",
        "\n",
        "| finding | detail |\n",
        "|---------|--------|\n",
        "| O3 most predictable | strong diurnal photochemical cycle makes ozone easiest to predict |\n",
        "| PM10 most variable | diverse local sources cause high variability between stations |\n",
        "| 5 broken models | data quality issues at TH4, BG2, WM6 stations (constant test values) |\n",
        "| CNN vs RF | CNN achieves comparable but slightly lower performance than random forest |\n",
        "\n",
        "### comparison with random forest\n",
        "\n",
        "| metric | random forest | CNN |\n",
        "|--------|--------------|-----|\n",
        "| mean test R2 | 0.814 | 0.633 |\n",
        "| best pollutant | O3 | O3 |\n",
        "| training time | ~2 hours | ~6 hours |\n",
        "\n",
        "random forest outperforms CNN on this dataset. possible reasons:\n",
        "\n",
        "1. strong temporal autocorrelation favours lag features (RF captures this directly)\n",
        "2. CNN may need more data or deeper architecture for time series\n",
        "3. hyperparameters tuned on single station may not generalise to all stations\n",
        "\n",
        "### conclusion\n",
        "\n",
        "CNN models achieve reasonable performance across the LAQN network but do not match random forest results. the 12 hour sequence length with Conv1D architecture captures temporal patterns but the simpler random forest approach with flattened lag features proves more effective for this hourly prediction task."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
