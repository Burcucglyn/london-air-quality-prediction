{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64359774",
   "metadata": {},
   "source": [
    "# DEFRA Dataset Assesment\n",
    "\n",
    "\n",
    "1) I'll be start adding my main paths and modules I will be using in this notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "432092e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible python modules i will be using below\n",
    "from curses import meta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# for parse pdf uk pollutant limitations to csv\n",
    "import re\n",
    "# pdfplumber for pdf parsing\n",
    "import pdfplumber\n",
    "\n",
    "# function 5. chi-square test\n",
    "from scipy import stats\n",
    "\n",
    "#define base path  without hardcoding\n",
    "base_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"optimised\"\n",
    "#metadata file for pollutant name, location and site names\n",
    "metadata_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" /\"test\"/\"std_london_sites_pollutant.csv\"\n",
    "\n",
    "# output path for saving statistics 1. function\n",
    "#the first analyse dataset created without inclitiong nan optimased files, and cross referencing that's why changed the name to dataset_statistics-noNAN-incl.csv\n",
    "os.makedirs(base_dir / \"report\", exist_ok=True)\n",
    "stats_output_path = base_dir/\"report\"/ \"defra_stats.csv\"\n",
    "\n",
    "# output paths for saving all the pollutant distribution and nan value analysis.\n",
    "pollutant_distrubution_path = base_dir / \"report\" / \"pollutant_distribution.csv\"\n",
    "nan_val_pollutant_split_path = base_dir / \"report\" / \"nan_values_by_pollutant.csv\"\n",
    "nan_val_stationPollutant_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"defra\" / \"report\" / \"nan_values_by_station_pollutant.csv\"\n",
    "\n",
    "\n",
    "# log file from nan replacement process\n",
    "nan_log_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"logs\" / \"NaN_values_record.csv\"\n",
    "\n",
    "# function for uk pollutant regulations pdf to parse csv file path\n",
    "pdf_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"capabilities\" / \"Air_Quality_Objectives_Update.pdf\"\n",
    "csv_output_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"capabilities\" / \"uk_pollutant_limits.csv\"\n",
    "\n",
    "\n",
    "# data quality metrics report output path\n",
    "\n",
    "quality_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\"/ \"report\" / \"quality_metrics_validation.csv\"\n",
    "quality_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#chi-square test output path func 5\n",
    "chi_square_output1 = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"report\" / \"chi_square_tests1.csv\"\n",
    "chi_square_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"report\" / \"chi_square_tests.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f95bb4",
   "metadata": {},
   "source": [
    "## 1) Initial Dataset Assessment: Raw Numbers\n",
    "\n",
    "Before conducting quality checks, I need to establish the baseline characteristics of the DEFRA dataset. This section calculates comprehensive statistics about the data collection effort, including file counts, measurement records, station coverage, and pollutant distribution.\n",
    "\n",
    "### Purpose\n",
    "- Document the scale and scope of data collection.\n",
    "- Establish baseline metrics for comparison with LAQN.\n",
    "- Provide context for subsequent quality analysis.\n",
    "\n",
    "### Methodology\n",
    "The function `get_defra_dataset_statistics()` performs the following:\n",
    "1. Loads standardised metadata to identify unique stations and pollutants.\n",
    "2. Counts files across all three yearly directories (2023, 2024, 2025).\n",
    "3. Calculates total measurement records by reading all CSV files.\n",
    "4. Determines spatial coverage from unique coordinate pairs.\n",
    "5. Documents temporal coverage (35 months: January 2023 to November 2025).\n",
    "\n",
    "### Notes\n",
    "- File counting is fast (scans directory structure only).\n",
    "- Record counting can be slow (reads every CSV file).\n",
    "- Results are saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e6f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defra_dataset_statistics(base_dir, metadata_path, nan_log_path):\n",
    "    \"\"\"\n",
    "    Calculate statistics at DEFRA dataset.\n",
    "    This function walks through the monthly data directories 2023, 2024, 2025 and calculates key metrics needed for reporting.\n",
    "    \n",
    "    Parameters:\n",
    "        base_dir : Path\n",
    "            Base directory containing defra data folders.\n",
    "        metadata_path : Path\n",
    "            Path to the standardised metadata csv file.\n",
    "        nan_log_path : Path\n",
    "            Path to the NaN values log file after notice data flags, changed them to NaN.\n",
    "            \n",
    "    Returns:\n",
    "        dict : Dictionary containing all calculated statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # read metadata to get station and pollutant info\n",
    "    print(\"\\nloading metadata from std_london_sites_pollutant.csv...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # calculate metadata statistics\n",
    "    stats['unique_stations'] = metadata['station_name'].nunique()\n",
    "    stats['total_combinations'] = len(metadata)\n",
    "    stats['unique_pollutants'] = metadata['pollutant_std'].nunique()\n",
    "    \n",
    "    # get pollutant breakdown\n",
    "    pollutant_counts = metadata['pollutant_std'].value_counts()\n",
    "    stats['pollutant_distribution'] = pollutant_counts.to_dict()\n",
    "    \n",
    "    # create set of expected station/pollutant pairs from metadata\n",
    "    expected_pairs = set(\n",
    "        zip(metadata['station_name'], metadata['pollutant_std'])\n",
    "    )\n",
    "    stats['expected_pairs'] = len(expected_pairs)\n",
    "    print(f\"  expected station/pollutant pairs from metadata: {len(expected_pairs)}\")\n",
    "    \n",
    "    # count unique coordinates for spatial coverage, i will be use this for laqn dataset as well\n",
    "    # group by lat/lon and count unique locations, instead of station names and will do the validation afterwards\n",
    "    unique_coords = metadata[['latitude', 'longitude']].drop_duplicates()\n",
    "    stats['unique_locations'] = len(unique_coords)\n",
    "    \n",
    "    # count files in monthly data directories\n",
    "    total_files = 0\n",
    "    files_by_year = {}\n",
    "    \n",
    "    # loop through each years measurement directory\n",
    "    print(\"\\nscanning optimised directory for collected data...\")\n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        \n",
    "        if year_dir.exists():\n",
    "            # count all CSV files in this years directory and subdirectories\n",
    "            year_files = list(year_dir.rglob('*.csv'))\n",
    "            files_by_year[year] = len(year_files)\n",
    "            total_files += len(year_files)\n",
    "            print(f\"  {year}: {len(year_files)} files\")\n",
    "        else:\n",
    "            files_by_year[year] = 0\n",
    "            print(f\"  {year}: directory not found\")\n",
    "    \n",
    "    stats['total_files'] = total_files\n",
    "    stats['files_by_year'] = files_by_year\n",
    "    \n",
    "    # calculate total measurement records, this requires reading all csv files and counting rows\n",
    "    total_records = 0\n",
    "    records_by_year = {}\n",
    "    total_missing = 0\n",
    "    missing_by_year = {}\n",
    "    \n",
    "    # concatenate all CSVs for missing value breakdown\n",
    "    all_csvs = []\n",
    "    \n",
    "    print(\"\\nreading all CSV files to calculate statistics...\")\n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        year_records = 0\n",
    "        year_missing = 0\n",
    "        \n",
    "        if year_dir.exists():\n",
    "            # read each csv, count rows and missing values\n",
    "            for csv_file in year_dir.rglob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    year_records += len(df)\n",
    "                    \n",
    "                    # count missing NaN or empty string values in value column\n",
    "                    # calculation: missing values in value column only\n",
    "                    if 'value' in df.columns:\n",
    "                        missing_in_file = df['value'].isna().sum() + (df['value'] == \"\").sum()\n",
    "                        year_missing += missing_in_file\n",
    "                    \n",
    "                    # store dataframe for later aggregation\n",
    "                    all_csvs.append(df)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  warning: could not read {csv_file.name}: {e}\")\n",
    "            \n",
    "            records_by_year[year] = year_records\n",
    "            missing_by_year[year] = year_missing\n",
    "            total_records += year_records\n",
    "            total_missing += year_missing\n",
    "            print(f\"  {year}: {year_records:,} records, {year_missing:,} missing ({(year_missing/year_records*100):.2f}%)\")\n",
    "        else:\n",
    "            records_by_year[year] = 0\n",
    "            missing_by_year[year] = 0\n",
    "    \n",
    "    stats['total_records'] = total_records\n",
    "    stats['records_by_year'] = records_by_year\n",
    "    stats['missing_by_year'] = missing_by_year\n",
    "    stats['total_missing'] = total_missing\n",
    "    stats['overall_completeness'] = ((total_records - total_missing) / total_records * 100) if total_records > 0 else 0\n",
    "    \n",
    "    # cross-reference metadata with collected data\n",
    "    print(\"\\ncross-referencing collected data with metadata...\")\n",
    "    \n",
    "    if all_csvs:\n",
    "        all_data = pd.concat(all_csvs, ignore_index=True)\n",
    "        \n",
    "        # check if required columns exist in csv files\n",
    "        # file structure: timestamp,value,timeseries_id,station_name,pollutant_name,pollutant_std,latitude,longitude\n",
    "        if 'station_name' in all_data.columns and 'pollutant_std' in all_data.columns:\n",
    "            # identify actual station/pollutant pairs in collected data\n",
    "            collected_pairs = set(\n",
    "                zip(all_data['station_name'], all_data['pollutant_std'])\n",
    "            )\n",
    "            stats['collected_pairs'] = len(collected_pairs)\n",
    "            \n",
    "            # find missing pairs (in metadata but not in collected data)\n",
    "            missing_pairs = expected_pairs - collected_pairs\n",
    "            stats['missing_pairs'] = list(missing_pairs)\n",
    "            stats['missing_pairs_count'] = len(missing_pairs)\n",
    "            \n",
    "            # find extra pairs (in collected data but not in metadata)\n",
    "            extra_pairs = collected_pairs - expected_pairs\n",
    "            stats['extra_pairs'] = list(extra_pairs)\n",
    "            stats['extra_pairs_count'] = len(extra_pairs)\n",
    "            \n",
    "            print(f\"  expected pairs from metadata: {len(expected_pairs)}\")\n",
    "            print(f\"  actually collected pairs: {len(collected_pairs)}\")\n",
    "            print(f\"  missing pairs (in metadata but not collected): {len(missing_pairs)}\")\n",
    "            print(f\"  extra pairs (collected but not in metadata): {len(extra_pairs)}\")\n",
    "            \n",
    "            # group by station and pollutant_std, count missing values\n",
    "            # calculation: (100 * missing value cell number) / (total number of row value col)\n",
    "            missing_breakdown = {}\n",
    "            \n",
    "            for (station, pollutant), group in all_data.groupby(['station_name', 'pollutant_std']):\n",
    "                total_rows = len(group)\n",
    "                # count missing in value column\n",
    "                if 'value' in group.columns:\n",
    "                    missing_rows = group['value'].isna().sum() + (group['value'] == \"\").sum()\n",
    "                else:\n",
    "                    missing_rows = 0\n",
    "                \n",
    "                missing_breakdown[(station, pollutant)] = (int(missing_rows), int(total_rows))\n",
    "            \n",
    "            stats['missing_by_station_pollutant'] = missing_breakdown\n",
    "        else:\n",
    "            print(\"  warning: station_name or pollutant_std columns not found\")\n",
    "            stats['missing_by_station_pollutant'] = {}\n",
    "            stats['collected_pairs'] = 0\n",
    "            stats['missing_pairs'] = []\n",
    "            stats['missing_pairs_count'] = 0\n",
    "            stats['extra_pairs'] = []\n",
    "            stats['extra_pairs_count'] = 0\n",
    "    else:\n",
    "        stats['missing_by_station_pollutant'] = {}\n",
    "        stats['collected_pairs'] = 0\n",
    "        stats['missing_pairs'] = list(expected_pairs)\n",
    "        stats['missing_pairs_count'] = len(expected_pairs)\n",
    "        stats['extra_pairs'] = []\n",
    "        stats['extra_pairs_count'] = 0\n",
    "    \n",
    "    # distribution of nan by pollutant over time\n",
    "    if stats['missing_by_station_pollutant']:\n",
    "        pollutant_missing_summary = {}\n",
    "        \n",
    "        for (station, pollutant), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            if pollutant not in pollutant_missing_summary:\n",
    "                pollutant_missing_summary[pollutant] = {'total_missing': 0, 'total_records': 0}\n",
    "            \n",
    "            pollutant_missing_summary[pollutant]['total_missing'] += missing\n",
    "            pollutant_missing_summary[pollutant]['total_records'] += total\n",
    "        \n",
    "        # calculate percentages\n",
    "        for pollutant in pollutant_missing_summary:\n",
    "            total_missing = pollutant_missing_summary[pollutant]['total_missing']\n",
    "            total_records = pollutant_missing_summary[pollutant]['total_records']\n",
    "            percentage = (total_missing / total_records * 100) if total_records > 0 else 0\n",
    "            pollutant_missing_summary[pollutant]['percentage_missing'] = percentage\n",
    "        \n",
    "        stats['missing_by_pollutant_type'] = pollutant_missing_summary\n",
    "    else:\n",
    "        stats['missing_by_pollutant_type'] = {}\n",
    "    \n",
    "    # log file created during data cleaning process\n",
    "    if Path(nan_log_path).exists():\n",
    "        nan_log = pd.read_csv(nan_log_path)\n",
    "        \n",
    "        # calculate replacement statistics per year\n",
    "        replacements_by_year = nan_log.groupby('year_folder')['invalid_flags_replaced'].sum().to_dict()\n",
    "        stats['nan_replacements_by_year'] = replacements_by_year\n",
    "        stats['total_nan_replacements'] = nan_log['invalid_flags_replaced'].sum()\n",
    "        \n",
    "        # get mean percentage of invalid flags\n",
    "        stats['mean_invalid_percentage'] = nan_log['percentage_invalid'].mean()\n",
    "        stats['max_invalid_percentage'] = nan_log['percentage_invalid'].max()\n",
    "        \n",
    "    else:\n",
    "        stats['nan_replacements_by_year'] = {}\n",
    "        stats['total_nan_replacements'] = 0\n",
    "        stats['mean_invalid_percentage'] = 0\n",
    "        stats['max_invalid_percentage'] = 0\n",
    "    \n",
    "    # calculate temporal coverage based on the files collected, understands which months have data\n",
    "    stats['temporal_coverage'] = {\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2025-11-19',  \n",
    "        'total_months': 35\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df5aaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(stats):\n",
    "    \"\"\"\n",
    "    Print dataset statistics\n",
    "    \n",
    "    Parameters:\n",
    "        stats : dict\n",
    "            returned by get_defra_dataset_statistics().\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Defra dataset statistics: initial assessment\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"\\nScale and scope:\")\n",
    "    print(f\"Total files collected: {stats['total_files']:,}\")\n",
    "    print(f\"Total measurement records: {stats['total_records']:,}\")\n",
    "    print(f\"Total missing values (nan): {stats['total_missing']:,}\")\n",
    "    print(f\"Overall completeness: {stats['overall_completeness']:.2f}%\")\n",
    "    print(f\"Unique monitoring stations: {stats['unique_stations']}\")\n",
    "    print(f\"Total station-pollutant combinations: {stats['total_combinations']}\")\n",
    "    print(f\"Unique pollutant types: {stats['unique_pollutants']}\")\n",
    "    print(f\"Unique geographic locations: {stats['unique_locations']}\")\n",
    "    \n",
    "    # data collection coverage\n",
    "    print(\"\\nData collection coverage:\")\n",
    "    print(f\"Expected pairs (from metadata): {stats.get('expected_pairs', 0)}\")\n",
    "    print(f\"Actually collected pairs: {stats.get('collected_pairs', 0)}\")\n",
    "    print(f\"Missing pairs (not collected): {stats.get('missing_pairs_count', 0)}\")\n",
    "    print(f\"Extra pairs (not in metadata): {stats.get('extra_pairs_count', 0)}\")\n",
    "    \n",
    "    if stats.get('missing_pairs_count', 0) > 0:\n",
    "        print(f\"\\nwarning: {stats['missing_pairs_count']} station/pollutant pairs from metadata were not found in collected data.\")\n",
    "        print(\"first 10 missing pairs:\")\n",
    "        for i, (station, pollutant) in enumerate(stats['missing_pairs'][:10], 1):\n",
    "            print(f\"  {i}. {station} - {pollutant}\")\n",
    "    \n",
    "    if stats.get('extra_pairs_count', 0) > 0:\n",
    "        print(f\"\\nNote: {stats['extra_pairs_count']} station/pollutant pairs in collected data are not in metadata.\")\n",
    "    \n",
    "    print(\"\\nfiles by year:\")\n",
    "    for year, count in stats['files_by_year'].items():\n",
    "        print(f\"  {year}: {count:,} files\")\n",
    "    \n",
    "    print(\"\\nrecords by year:\")\n",
    "    for year, count in stats['records_by_year'].items():\n",
    "        missing = stats['missing_by_year'].get(year, 0)\n",
    "        missing_pct = (missing / count * 100) if count > 0 else 0\n",
    "        print(f\"  {year}: {count:,} records, {missing:,} missing ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # adding nan value summary below\n",
    "    print(\"\\nnan replacement summary:\")\n",
    "    print(f\"Total invalid flags replaced: {stats['total_nan_replacements']:,}\")\n",
    "    print(f\"Mean invalid percentage per file: {stats['mean_invalid_percentage']:.2f}%\")\n",
    "    print(f\"Max invalid percentage: {stats['max_invalid_percentage']:.2f}%\")\n",
    "    \n",
    "    # count of replacements by year\n",
    "    if stats['nan_replacements_by_year']:\n",
    "        print(\"\\nreplacements by year:\")\n",
    "        for year_folder, count in stats['nan_replacements_by_year'].items():\n",
    "            print(f\"  {year_folder}: {count:,} flags replaced\")\n",
    "    \n",
    "    print(\"\\ntemporal coverage:\")\n",
    "    print(f\"start date: {stats['temporal_coverage']['start_date']}\")\n",
    "    print(f\"end date: {stats['temporal_coverage']['end_date']}\")\n",
    "    print(f\"total months: {stats['temporal_coverage']['total_months']}\")\n",
    "    \n",
    "    print(\"\\npollutant distribution:\")\n",
    "    print(\"station/pollutant combinations by type:\")\n",
    "    for pollutant, count in sorted(stats['pollutant_distribution'].items(), \n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_combinations']) * 100\n",
    "        print(f\"  {pollutant}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # missing value distribution by pollutant type\n",
    "    print(\"\\nMissing value distribution by pollutant type:\")\n",
    "    if stats.get('missing_by_pollutant_type'):\n",
    "        # sort by percentage missing (highest first)\n",
    "        sorted_pollutants = sorted(\n",
    "            stats['missing_by_pollutant_type'].items(),\n",
    "            key=lambda x: x[1]['percentage_missing'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(f\"{'pollutant':<20} {'total records':>15} {'missing':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for pollutant, data in sorted_pollutants:\n",
    "            print(f\"{pollutant:<20} {data['total_records']:>15,} {data['total_missing']:>12,} {data['percentage_missing']:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  no missing value distribution available.\")\n",
    "    \n",
    "    # print missing values by station/pollutant breakdown with row_number column\n",
    "    print(\"\\nMissing values by station/pollutant:\")\n",
    "    if stats.get('missing_by_station_pollutant'):\n",
    "        # prepare a sorted list by missing percentage descending\n",
    "        breakdown = []\n",
    "        for (station, pollutant), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            percent = (missing / total * 100) if total > 0 else 0\n",
    "            breakdown.append((station, pollutant, missing, total, percent))\n",
    "        # sort by percentage descending and take top 20\n",
    "        breakdown.sort(key=lambda x: x[4], reverse=True)\n",
    "        breakdown = breakdown[:20]\n",
    "        print(f\"{'station':<30} {'pollutant':<20} {'missing':>10} {'total_row':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for station, pollutant, missing, total, percent in breakdown:\n",
    "            print(f\"{station:<30} {pollutant:<20} {missing:>10,} {total:>12,} {percent:>11.2f}%\")\n",
    "    else:\n",
    "        print(\" No missing value breakdown available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading metadata from std_london_sites_pollutant.csv...\n",
      "  expected station/pollutant pairs from metadata: 141\n",
      "\n",
      "scanning optimised directory for collected data...\n",
      "  2023: 1431 files\n",
      "  2024: 1193 files\n",
      "  2025: 939 files\n",
      "\n",
      "reading all CSV files to calculate statistics...\n",
      "  2023: 1,000,126 records, 90,161 missing (9.01%)\n",
      "  2024: 868,320 records, 101,256 missing (11.66%)\n",
      "  2025: 657,545 records, 30,750 missing (4.68%)\n",
      "\n",
      "cross-referencing collected data with metadata...\n",
      "  expected pairs from metadata: 141\n",
      "  actually collected pairs: 141\n",
      "  missing pairs (in metadata but not collected): 0\n",
      "  extra pairs (collected but not in metadata): 0\n",
      "\n",
      "========================================\n",
      "Defra dataset statistics: initial assessment\n",
      "========================================\n",
      "\n",
      "Scale and scope:\n",
      "Total files collected: 3,563\n",
      "Total measurement records: 2,525,991\n",
      "Total missing values (nan): 222,167\n",
      "Overall completeness: 91.20%\n",
      "Unique monitoring stations: 18\n",
      "Total station-pollutant combinations: 144\n",
      "Unique pollutant types: 37\n",
      "Unique geographic locations: 20\n",
      "\n",
      "Data collection coverage:\n",
      "Expected pairs (from metadata): 141\n",
      "Actually collected pairs: 141\n",
      "Missing pairs (not collected): 0\n",
      "Extra pairs (not in metadata): 0\n",
      "\n",
      "files by year:\n",
      "  2023: 1,431 files\n",
      "  2024: 1,193 files\n",
      "  2025: 939 files\n",
      "\n",
      "records by year:\n",
      "  2023: 1,000,126 records, 90,161 missing (9.01%)\n",
      "  2024: 868,320 records, 101,256 missing (11.66%)\n",
      "  2025: 657,545 records, 30,750 missing (4.68%)\n",
      "\n",
      "nan replacement summary:\n",
      "Total invalid flags replaced: 222,167\n",
      "Mean invalid percentage per file: 9.61%\n",
      "Max invalid percentage: 100.00%\n",
      "\n",
      "replacements by year:\n",
      "  2023measurements: 90,161 flags replaced\n",
      "  2024measurements: 101,256 flags replaced\n",
      "  2025measurements: 30,750 flags replaced\n",
      "\n",
      "temporal coverage:\n",
      "start date: 2023-01-01\n",
      "end date: 2025-11-19\n",
      "total months: 35\n",
      "\n",
      "pollutant distribution:\n",
      "station/pollutant combinations by type:\n",
      "  PM10: 15 (10.4%)\n",
      "  PM2.5: 15 (10.4%)\n",
      "  NO2: 14 (9.7%)\n",
      "  NOx: 14 (9.7%)\n",
      "  NO: 14 (9.7%)\n",
      "  O3: 9 (6.2%)\n",
      "  SO2: 3 (2.1%)\n",
      "  n-Pentane: 2 (1.4%)\n",
      "  m,p-Xylene: 2 (1.4%)\n",
      "  n-Butane: 2 (1.4%)\n",
      "  n-Heptane: 2 (1.4%)\n",
      "  n-Hexane: 2 (1.4%)\n",
      "  n-Octane: 2 (1.4%)\n",
      "  Propene: 2 (1.4%)\n",
      "  o-Xylene: 2 (1.4%)\n",
      "  Propane: 2 (1.4%)\n",
      "  i-Pentane: 2 (1.4%)\n",
      "  Toluene: 2 (1.4%)\n",
      "  trans-2-Butene: 2 (1.4%)\n",
      "  trans-2-Pentene: 2 (1.4%)\n",
      "  Isoprene: 2 (1.4%)\n",
      "  Ethyne: 2 (1.4%)\n",
      "  i-Octane: 2 (1.4%)\n",
      "  i-Hexane: 2 (1.4%)\n",
      "  i-Butane: 2 (1.4%)\n",
      "  Ethylbenzene: 2 (1.4%)\n",
      "  Ethene: 2 (1.4%)\n",
      "  Ethane: 2 (1.4%)\n",
      "  cis-2-Butene: 2 (1.4%)\n",
      "  Benzene: 2 (1.4%)\n",
      "  1-Pentene: 2 (1.4%)\n",
      "  1-Butene: 2 (1.4%)\n",
      "  1,3-Butadiene: 2 (1.4%)\n",
      "  1,3,5-TMB: 2 (1.4%)\n",
      "  1,2,4-TMB: 2 (1.4%)\n",
      "  1,2,3-TMB: 2 (1.4%)\n",
      "  CO: 2 (1.4%)\n",
      "\n",
      "Missing value distribution by pollutant type:\n",
      "pollutant              total records      missing    % missing\n",
      "------------------------------------------------------------\n",
      "PM10                         227,142       37,580       16.54%\n",
      "O3                           194,333       27,184       13.99%\n",
      "PM2.5                        234,748       29,623       12.62%\n",
      "SO2                           72,928        7,181        9.85%\n",
      "NO                           326,061       25,444        7.80%\n",
      "NO2                          326,072       25,429        7.80%\n",
      "NOx                          325,387       24,964        7.67%\n",
      "n-Octane                      26,649        1,764        6.62%\n",
      "CO                            48,578        3,078        6.34%\n",
      "m,p-Xylene                    25,503        1,612        6.32%\n",
      "1,3,5-TMB                     26,649        1,641        6.16%\n",
      "Toluene                       26,649        1,640        6.15%\n",
      "i-Octane                      26,649        1,624        6.09%\n",
      "n-Heptane                     26,649        1,622        6.09%\n",
      "1,2,4-TMB                     26,649        1,610        6.04%\n",
      "Ethylbenzene                  26,649        1,592        5.97%\n",
      "Benzene                       26,649        1,586        5.95%\n",
      "o-Xylene                      26,649        1,568        5.88%\n",
      "1,2,3-TMB                     26,649        1,560        5.85%\n",
      "1-Pentene                     26,572        1,381        5.20%\n",
      "cis-2-Butene                  26,599        1,378        5.18%\n",
      "trans-2-Pentene               26,599        1,366        5.14%\n",
      "Isoprene                      26,618        1,341        5.04%\n",
      "Ethyne                        26,529        1,328        5.01%\n",
      "1,3-Butadiene                 26,568        1,320        4.97%\n",
      "i-Hexane                      26,599        1,321        4.97%\n",
      "trans-2-Butene                26,599        1,321        4.97%\n",
      "n-Hexane                      26,580        1,320        4.97%\n",
      "Propane                       26,618        1,316        4.94%\n",
      "Ethane                        26,599        1,315        4.94%\n",
      "Ethene                        26,618        1,312        4.93%\n",
      "Propene                       26,618        1,312        4.93%\n",
      "i-Butane                      26,599        1,308        4.92%\n",
      "1-Butene                      26,599        1,307        4.91%\n",
      "n-Butane                      26,599        1,307        4.91%\n",
      "i-Pentane                     26,618        1,306        4.91%\n",
      "n-Pentane                     26,618        1,306        4.91%\n",
      "\n",
      "Missing values by station/pollutant:\n",
      "station                        pollutant               missing    total_row    % missing\n",
      "----------------------------------------\n",
      "London Eltham                  PM10                     16,337       16,826       97.09%\n",
      "London Eltham                  NO2                      13,187       16,840       78.31%\n",
      "London Eltham                  NO                       13,182       16,835       78.30%\n",
      "London Eltham                  NOx                      13,125       16,793       78.16%\n",
      "London Eltham                  O3                       12,537       16,842       74.44%\n",
      "London Teddington Bushy Park   PM10                     10,525       24,327       43.26%\n",
      "London Teddington Bushy Park   PM2.5                    20,820       48,656       42.79%\n",
      "London Haringey Priory Park South O3                        8,171       24,288       33.64%\n",
      "London Marylebone Road         PM10                        632        2,355       26.84%\n",
      "London Marylebone Road         PM2.5                       479        2,355       20.34%\n",
      "London Norbury Manor School    PM10                        936        5,258       17.80%\n",
      "London Norbury Manor School    PM2.5                       936        5,258       17.80%\n",
      "London Bexley                  PM10                      4,012       24,273       16.53%\n",
      "Southwark A2 Old Kent Road     PM10                        388        2,355       16.48%\n",
      "Haringey Roadside              NOx                       3,725       24,250       15.36%\n",
      "Haringey Roadside              NO2                       3,708       24,285       15.27%\n",
      "Haringey Roadside              NO                        3,708       24,287       15.27%\n",
      "London Westminster             PM2.5                     3,463       24,299       14.25%\n",
      "London Marylebone Road         SO2                       2,987       24,290       12.30%\n",
      "London Marylebone Road         CO                        2,729       24,293       11.23%\n",
      "Pollutant distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/optimised/report/pollutant_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# run the analysis\n",
    "stats = get_defra_dataset_statistics(base_dir, metadata_path, nan_log_path)\n",
    "print_dataset_statistics(stats)\n",
    "\n",
    "# # Save statistics for later use as csv\n",
    "# save statistics for later use as csv\n",
    "# prepare flat data structure for csv\n",
    "stats_rows = []\n",
    "stats_rows.append([\"metric\", \"value\"])\n",
    "stats_rows.append([\"total_files\", stats['total_files']])\n",
    "stats_rows.append([\"total_records\", stats['total_records']])\n",
    "stats_rows.append([\"total_missing\", stats['total_missing']])\n",
    "stats_rows.append([\"overall_completeness_pct\", f\"{stats['overall_completeness']:.2f}\"])\n",
    "stats_rows.append([\"unique_stations\", stats['unique_stations']])\n",
    "stats_rows.append([\"total_combinations\", stats['total_combinations']])\n",
    "stats_rows.append([\"unique_pollutants\", stats['unique_pollutants']])\n",
    "stats_rows.append([\"unique_locations\", stats['unique_locations']])\n",
    "stats_rows.append([\"expected_pairs\", stats.get('expected_pairs', 0)])\n",
    "stats_rows.append([\"collected_pairs\", stats.get('collected_pairs', 0)])\n",
    "stats_rows.append([\"missing_pairs_count\", stats.get('missing_pairs_count', 0)])\n",
    "stats_rows.append([\"extra_pairs_count\", stats.get('extra_pairs_count', 0)])\n",
    "stats_rows.append([\"total_nan_replacements\", stats['total_nan_replacements']])\n",
    "stats_rows.append([\"mean_invalid_pct\", f\"{stats['mean_invalid_percentage']:.2f}\"])\n",
    "stats_rows.append([\"max_invalid_pct\", f\"{stats['max_invalid_percentage']:.2f}\"])\n",
    "\n",
    "# add year-specific metrics\n",
    "for year in ['2023', '2024', '2025']:\n",
    "    stats_rows.append([f\"files_{year}\", stats['files_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"records_{year}\", stats['records_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"missing_{year}\", stats['missing_by_year'].get(year, 0)])\n",
    "    year_key = f'{year}measurements'\n",
    "    stats_rows.append([f\"replacements_{year}\", stats['nan_replacements_by_year'].get(year_key, 0)])\n",
    "\n",
    "# # save to csv stats report save func below (commented out for now to overwrite previous report)\n",
    "# pd.DataFrame(stats_rows[1:], columns=stats_rows[0]).to_csv(stats_output_path, index=False)\n",
    "# print(f\"\\nstatistics saved to: {stats_output_path}\")\n",
    "\n",
    "# # save pollutant distribution to csv describe the path on top pollutant_distrubution_path\n",
    "# total_combinations = stats['total_combinations']\n",
    "# pollutant_distribution_df = pd.DataFrame(\n",
    "#     [\n",
    "#         {\n",
    "#             'pollutant': k,\n",
    "#             'count': v,\n",
    "#             'percentage': round((v / total_combinations) * 100, 2) if total_combinations > 0 else 0\n",
    "#         }\n",
    "#         for k, v in stats['pollutant_distribution'].items()\n",
    "#     ]\n",
    "# )\n",
    "# pollutant_distribution_df.to_csv(pollutant_distrubution_path, index=False)\n",
    "# print(f\"Pollutant distribution saved to: {pollutant_distrubution_path}\")\n",
    "\n",
    "# # Save missing value distribution by pollutant type to path described the path on top nan_val_pollutant_split_path\n",
    "# if stats.get('missing_by_pollutant_type'):\n",
    "#     missing_by_pollutant_df = pd.DataFrame([\n",
    "#         {\n",
    "#             'pollutant': k,\n",
    "#             'total_records': v['total_records'],\n",
    "#             'total_missing': v['total_missing'],\n",
    "#             'percentage_missing': v['percentage_missing']\n",
    "#         }\n",
    "#         for k, v in stats['missing_by_pollutant_type'].items()\n",
    "#     ])\n",
    "#     missing_by_pollutant_df.to_csv(nan_val_pollutant_split_path, index=False)\n",
    "#     print(f\"Missing value distribution by pollutant type saved to: {nan_val_pollutant_split_path}\")\n",
    "\n",
    "# # save missing values by station/pollutant to csv path on top nan_val_stationPollutant_path\n",
    "# if stats.get('missing_by_station_pollutant'):\n",
    "#     missing_by_station_pollutant_df = pd.DataFrame([\n",
    "#         {\n",
    "#             'station': k[0],\n",
    "#             'pollutant': k[1],\n",
    "#             'missing': v[0],\n",
    "#             'total_row': v[1],\n",
    "#             'percentage_missing': (v[0] / v[1] * 100) if v[1] > 0 else 0\n",
    "#         }\n",
    "#         for k, v in stats['missing_by_station_pollutant'].items()\n",
    "#     ])\n",
    "#     missing_by_station_pollutant_df.to_csv(nan_val_stationPollutant_path, index=False)\n",
    "#     print(f\"Missing values by station/pollutant saved to: {nan_val_stationPollutant_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f816ef",
   "metadata": {},
   "source": [
    "    loading metadata from std_london_sites_pollutant.csv...\n",
    "    expected station/pollutant pairs from metadata: 141\n",
    "\n",
    "    scanning optimised directory for collected data...\n",
    "    2023: 1431 files\n",
    "    2024: 1193 files\n",
    "    2025: 939 files\n",
    "\n",
    "    reading all CSV files to calculate statistics...\n",
    "    2023: 1,000,126 records, 90,161 missing (9.01%)\n",
    "    2024: 868,320 records, 101,256 missing (11.66%)\n",
    "    2025: 657,545 records, 30,750 missing (4.68%)\n",
    "\n",
    "    cross-referencing collected data with metadata...\n",
    "    expected pairs from metadata: 141\n",
    "    actually collected pairs: 141\n",
    "    missing pairs (in metadata but not collected): 0\n",
    "    extra pairs (collected but not in metadata): 0\n",
    "\n",
    "    ========================================\n",
    "    Defra dataset statistics: initial assessment\n",
    "    ========================================\n",
    "\n",
    "    Scale and scope:\n",
    "    Total files collected: 3,563\n",
    "    Total measurement records: 2,525,991\n",
    "    Total missing values (nan): 222,167\n",
    "    Overall completeness: 91.20%\n",
    "    Unique monitoring stations: 18\n",
    "    Total station-pollutant combinations: 144\n",
    "    Unique pollutant types: 37\n",
    "    Unique geographic locations: 20\n",
    "\n",
    "    Data collection coverage:\n",
    "    Expected pairs (from metadata): 141\n",
    "    Actually collected pairs: 141\n",
    "    Missing pairs (not collected): 0\n",
    "    Extra pairs (not in metadata): 0\n",
    "\n",
    "    files by year:\n",
    "    2023: 1,431 files\n",
    "    2024: 1,193 files\n",
    "    2025: 939 files\n",
    "\n",
    "    records by year:\n",
    "    2023: 1,000,126 records, 90,161 missing (9.01%)\n",
    "    2024: 868,320 records, 101,256 missing (11.66%)\n",
    "    2025: 657,545 records, 30,750 missing (4.68%)\n",
    "\n",
    "    nan replacement summary:\n",
    "    Total invalid flags replaced: 222,167\n",
    "    Mean invalid percentage per file: 9.61%\n",
    "    Max invalid percentage: 100.00%\n",
    "\n",
    "    replacements by year:\n",
    "    2023measurements: 90,161 flags replaced\n",
    "    2024measurements: 101,256 flags replaced\n",
    "    2025measurements: 30,750 flags replaced\n",
    "\n",
    "    temporal coverage:\n",
    "    start date: 2023-01-01\n",
    "    end date: 2025-11-19\n",
    "    total months: 35\n",
    "\n",
    "    pollutant distribution:\n",
    "    station/pollutant combinations by type:\n",
    "    PM10: 15 (10.4%)\n",
    "    PM2.5: 15 (10.4%)\n",
    "    NO2: 14 (9.7%)\n",
    "    NOx: 14 (9.7%)\n",
    "    NO: 14 (9.7%)\n",
    "    O3: 9 (6.2%)\n",
    "    SO2: 3 (2.1%)\n",
    "    n-Pentane: 2 (1.4%)\n",
    "    m,p-Xylene: 2 (1.4%)\n",
    "    n-Butane: 2 (1.4%)\n",
    "    n-Heptane: 2 (1.4%)\n",
    "    n-Hexane: 2 (1.4%)\n",
    "    n-Octane: 2 (1.4%)\n",
    "    Propene: 2 (1.4%)\n",
    "    o-Xylene: 2 (1.4%)\n",
    "    Propane: 2 (1.4%)\n",
    "    i-Pentane: 2 (1.4%)\n",
    "    Toluene: 2 (1.4%)\n",
    "    trans-2-Butene: 2 (1.4%)\n",
    "    trans-2-Pentene: 2 (1.4%)\n",
    "    Isoprene: 2 (1.4%)\n",
    "    Ethyne: 2 (1.4%)\n",
    "    i-Octane: 2 (1.4%)\n",
    "    i-Hexane: 2 (1.4%)\n",
    "    i-Butane: 2 (1.4%)\n",
    "    Ethylbenzene: 2 (1.4%)\n",
    "    Ethene: 2 (1.4%)\n",
    "    Ethane: 2 (1.4%)\n",
    "    cis-2-Butene: 2 (1.4%)\n",
    "    Benzene: 2 (1.4%)\n",
    "    1-Pentene: 2 (1.4%)\n",
    "    1-Butene: 2 (1.4%)\n",
    "    1,3-Butadiene: 2 (1.4%)\n",
    "    1,3,5-TMB: 2 (1.4%)\n",
    "    1,2,4-TMB: 2 (1.4%)\n",
    "    1,2,3-TMB: 2 (1.4%)\n",
    "    CO: 2 (1.4%)\n",
    "\n",
    "    Missing value distribution by pollutant type:\n",
    "    pollutant              total records      missing    % missing\n",
    "    ------------------------------------------------------------\n",
    "    PM10                         227,142       37,580       16.54%\n",
    "    O3                           194,333       27,184       13.99%\n",
    "    PM2.5                        234,748       29,623       12.62%\n",
    "    SO2                           72,928        7,181        9.85%\n",
    "    NO                           326,061       25,444        7.80%\n",
    "    NO2                          326,072       25,429        7.80%\n",
    "    NOx                          325,387       24,964        7.67%\n",
    "    n-Octane                      26,649        1,764        6.62%\n",
    "    CO                            48,578        3,078        6.34%\n",
    "    m,p-Xylene                    25,503        1,612        6.32%\n",
    "    1,3,5-TMB                     26,649        1,641        6.16%\n",
    "    Toluene                       26,649        1,640        6.15%\n",
    "    i-Octane                      26,649        1,624        6.09%\n",
    "    n-Heptane                     26,649        1,622        6.09%\n",
    "    1,2,4-TMB                     26,649        1,610        6.04%\n",
    "    Ethylbenzene                  26,649        1,592        5.97%\n",
    "    Benzene                       26,649        1,586        5.95%\n",
    "    o-Xylene                      26,649        1,568        5.88%\n",
    "    1,2,3-TMB                     26,649        1,560        5.85%\n",
    "    1-Pentene                     26,572        1,381        5.20%\n",
    "    cis-2-Butene                  26,599        1,378        5.18%\n",
    "    trans-2-Pentene               26,599        1,366        5.14%\n",
    "    Isoprene                      26,618        1,341        5.04%\n",
    "    Ethyne                        26,529        1,328        5.01%\n",
    "    1,3-Butadiene                 26,568        1,320        4.97%\n",
    "    i-Hexane                      26,599        1,321        4.97%\n",
    "    trans-2-Butene                26,599        1,321        4.97%\n",
    "    n-Hexane                      26,580        1,320        4.97%\n",
    "    Propane                       26,618        1,316        4.94%\n",
    "    Ethane                        26,599        1,315        4.94%\n",
    "    Ethene                        26,618        1,312        4.93%\n",
    "    Propene                       26,618        1,312        4.93%\n",
    "    i-Butane                      26,599        1,308        4.92%\n",
    "    1-Butene                      26,599        1,307        4.91%\n",
    "    n-Butane                      26,599        1,307        4.91%\n",
    "    i-Pentane                     26,618        1,306        4.91%\n",
    "    n-Pentane                     26,618        1,306        4.91%\n",
    "\n",
    "    Missing values by station/pollutant:\n",
    "    station                        pollutant               missing    total_row    % missing\n",
    "    ----------------------------------------\n",
    "    London Eltham                  PM10                     16,337       16,826       97.09%\n",
    "    London Eltham                  NO2                      13,187       16,840       78.31%\n",
    "    London Eltham                  NO                       13,182       16,835       78.30%\n",
    "    London Eltham                  NOx                      13,125       16,793       78.16%\n",
    "    London Eltham                  O3                       12,537       16,842       74.44%\n",
    "    London Teddington Bushy Park   PM10                     10,525       24,327       43.26%\n",
    "    London Teddington Bushy Park   PM2.5                    20,820       48,656       42.79%\n",
    "    London Haringey Priory Park South O3                        8,171       24,288       33.64%\n",
    "    London Marylebone Road         PM10                        632        2,355       26.84%\n",
    "    London Marylebone Road         PM2.5                       479        2,355       20.34%\n",
    "    London Norbury Manor School    PM10                        936        5,258       17.80%\n",
    "    London Norbury Manor School    PM2.5                       936        5,258       17.80%\n",
    "    London Bexley                  PM10                      4,012       24,273       16.53%\n",
    "    Southwark A2 Old Kent Road     PM10                        388        2,355       16.48%\n",
    "    Haringey Roadside              NOx                       3,725       24,250       15.36%\n",
    "    Haringey Roadside              NO2                       3,708       24,285       15.27%\n",
    "    Haringey Roadside              NO                        3,708       24,287       15.27%\n",
    "    London Westminster             PM2.5                     3,463       24,299       14.25%\n",
    "    London Marylebone Road         SO2                       2,987       24,290       12.30%\n",
    "    London Marylebone Road         CO                        2,729       24,293       11.23%\n",
    "    Pollutant distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/optimised/report/pollutant_distribution.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97ae28",
   "metadata": {},
   "source": [
    "## 2) Spatial Coverage Analysis\n",
    "\n",
    " analysing spatial distribution patterns before accepting the dataset. I need to understand where defra stations are located, identify any geographic biases, and compare coverage to laqn.\n",
    "\n",
    "### Purpose\n",
    "- Create maps showing station locations across London.\n",
    "- Analyse density by borough to identify coverage gaps\n",
    "- Compare spatial distribution to laqn network\n",
    "- Ensure no geographic areas are overrepresented or underrepresented\n",
    "\n",
    "### Methodology\n",
    "1. Load defra metadata with coordinates\n",
    "2. Create interactive folium map showing all stations\n",
    "3. Calculate station density by borough\n",
    "4. Identify coverage gaps in london\n",
    "5. Compare to laqn spatial distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364fa78",
   "metadata": {},
   "source": [
    "sources: \n",
    "- https://python-visualization.github.io/folium/latest/getting_started.html\n",
    "- https://pandas.pydata.org/docs/user_guide/groupby.html \n",
    "- plotting: https://geopandas.org/en/stable/docs/user_guide/data_structures.html#geoseries\n",
    "    - general: https://geopandas.org/en/stable/getting_started.html\n",
    "\n",
    "coordinates:\n",
    "  -  https://www.ordnancesurvey.co.uk/\n",
    "  - identifiers: https://www.ordnancesurvey.co.uk/products/search-for-os-products?category=387aa470-8f46-4b02-a4ea-b70d1835f812 \n",
    "  - WGS84 coordinate system used for latitude/longitude.\n",
    "  - london coordinates : 51.5072° N, 0.1276° W\n",
    "  - Latitude and longitude coordinates are: 51.509865, -0.118092."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_spatial_coverage(metadata_path):\n",
    "    \"\"\"\n",
    "    analyse the stations location on map \n",
    "    \n",
    "    function validates coordinates, identifies  locations, and visulise the spatial distribution\n",
    "    \n",
    "    Parameters:\n",
    "        metadata_path : \n",
    "             std metadata csv file.\n",
    "            \n",
    "    Returns:\n",
    "        dictionary containing spatial statistics and coordinate data.\n",
    "    \n",
    "        *i got help for this section, sources folium tuttorials, plotting for geopandas and google. Also asked for my friend help as well which\n",
    "        she works on geospatial data a lot for her phd research.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    spatial_stats = {}\n",
    "    \n",
    "    # read metadata for coordinate information\n",
    "    print(\"\\nloading station coordinates from metadata...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # check if coordinate columns exist\n",
    "    if 'latitude' not in metadata.columns or 'longitude' not in metadata.columns:\n",
    "        print(\"  error: latitude or longitude columns not found in metadata\")\n",
    "        return spatial_stats\n",
    "    \n",
    "    # validate coordinate completeness\n",
    "    total_stations = len(metadata)\n",
    "    missing_lat = metadata['latitude'].isna().sum()\n",
    "    missing_lon = metadata['longitude'].isna().sum()\n",
    "    missing_coords = metadata[['latitude', 'longitude']].isna().any(axis=1).sum()\n",
    "    \n",
    "    spatial_stats['total_stations'] = total_stations\n",
    "    spatial_stats['missing_coordinates'] = missing_coords\n",
    "    spatial_stats['missing_latitude'] = missing_lat\n",
    "    spatial_stats['missing_longitude'] = missing_lon\n",
    "    spatial_stats['coordinate_completeness'] = ((total_stations - missing_coords) / total_stations * 100) if total_stations > 0 else 0\n",
    "    \n",
    "    print(f\"  total stations in metadata: {total_stations}\")\n",
    "    print(f\"  missing coordinates: {missing_coords} ({(missing_coords/total_stations*100):.2f}%)\")\n",
    "    print(f\"  coordinate completeness: {spatial_stats['coordinate_completeness']:.2f}%\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spatial_statistics(spatial_stats):\n",
    "    \"\"\"\n",
    "    Print spatial coverage statistics \n",
    "    \n",
    "    Param:\n",
    "        spatial_stats : \n",
    "            Dic returned by analyse_spatial_coverage()\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8188523",
   "metadata": {},
   "source": [
    "## 3) uk air quality standards framework\n",
    "\n",
    "The UK has established legally binding air quality objectives, I'm missing in my dataset, so first i need to parse the pdf file to csv and std format to my dataset.\n",
    "- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://uk-air.defra.gov.uk/assets/documents/Air_Quality_Objectives_Update.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23b7a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_defra_aq_objectives(pdf_path, csv_output_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Parse defra air quality objectives pdf and export to csv\n",
    "\n",
    "    \n",
    "    Output columns:\n",
    "        pollutant: pollutant name from pdf\n",
    "        pollutant_std: standardised pollutant code from metadata\n",
    "        limit: numeric limit value extracted from objective column\n",
    "        unit: unit of measurement (µg/m³, mg/m³, etc)\n",
    "        objective: full objective text from pdf\n",
    "        concentration measured as: averaging period (24 hour mean, annual mean, etc)\n",
    "        applies: jurisdiction (uk only in this case)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"parsing defra air quality objectives pdf\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"pdf path: {pdf_path}\")\n",
    "    print(f\"metadata path: {metadata_path}\")\n",
    "    print(f\"output path: {csv_output_path}\")\n",
    "    \n",
    "    # load metadata for pollutant mapping\n",
    "    print(\"\\nloading metadata for pollutant standardisation...\")\n",
    "    meta = pd.read_csv(metadata_path)\n",
    "    print(f\"loaded {len(meta)} metadata records\")\n",
    "    \n",
    "    # build pollutant mapping dictionary\n",
    "    pollutant_map = {}\n",
    "    \n",
    "    # first try direct pollutant column\n",
    "    if 'pollutant' in meta.columns and 'pollutant_std' in meta.columns:\n",
    "        meta_clean = meta[['pollutant', 'pollutant_std']].dropna().drop_duplicates()\n",
    "        for _, row in meta_clean.iterrows():\n",
    "            key = str(row['pollutant']).strip().lower()\n",
    "            val = str(row['pollutant_std']).strip()\n",
    "            pollutant_map[key] = val\n",
    "    \n",
    "    # then try pollutant_available column\n",
    "    if 'pollutant_available' in meta.columns and 'pollutant_std' in meta.columns:\n",
    "        meta_avail = meta[['pollutant_available', 'pollutant_std']].dropna().copy()\n",
    "        \n",
    "        for _, row in meta_avail.iterrows():\n",
    "            pollutants = str(row['pollutant_available']).split(',')\n",
    "            std_code = str(row['pollutant_std']).strip()\n",
    "            \n",
    "            for poll in pollutants:\n",
    "                key = poll.strip().lower()\n",
    "                if key and key != 'nan':\n",
    "                    pollutant_map[key] = std_code\n",
    "    \n",
    "    print(f\"built pollutant mapping with {len(pollutant_map)} entries\")\n",
    "    \n",
    "    # extract tables from pdf using pdfplumber\n",
    "    print(\"\\nextracting tables from pdf...\")\n",
    "    all_rows = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        print(f\"pdf has {len(pdf.pages)} pages\")\n",
    "        \n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            tables = page.extract_tables()\n",
    "            \n",
    "            if tables:\n",
    "                print(f\"page {page_num}: found {len(tables)} table(s)\")\n",
    "                \n",
    "                for table in tables:\n",
    "                    for row in table:\n",
    "                        all_rows.append(row)\n",
    "    \n",
    "    if not all_rows:\n",
    "        print(\"\\nerror: no tables found in pdf\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"total rows extracted: {len(all_rows)}\")\n",
    "    \n",
    "    # convert to dataframe\n",
    "    df_raw = pd.DataFrame(all_rows)\n",
    "    \n",
    "    print(\"\\nprocessing extracted data...\")\n",
    "    \n",
    "    # remove completely empty rows\n",
    "    df_raw = df_raw.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "    df_raw = df_raw.dropna(how='all').reset_index(drop=True)\n",
    "    \n",
    "    # find header row\n",
    "    header_idx = None\n",
    "    for i in range(min(len(df_raw), 20)):\n",
    "        row_text = ' '.join([str(x).lower() for x in df_raw.iloc[i].tolist() if pd.notna(x)])\n",
    "        \n",
    "        if 'pollutant' in row_text and 'applies' in row_text and 'objective' in row_text:\n",
    "            header_idx = i\n",
    "            print(f\"found header at row {i}\")\n",
    "            break\n",
    "    \n",
    "    if header_idx is None:\n",
    "        print(\"error: could not find header row\")\n",
    "        return None\n",
    "    \n",
    "    # use detected header\n",
    "    header_row = [str(x).strip() if pd.notna(x) else '' for x in df_raw.iloc[header_idx].tolist()]\n",
    "    df_raw.columns = header_row\n",
    "    df_raw = df_raw.iloc[header_idx + 1:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"original columns: {df_raw.columns.tolist()}\")\n",
    "    \n",
    "    # find and map the concentration column (may be split or abbreviated)\n",
    "    col_map = {}\n",
    "    concentration_col = None\n",
    "    \n",
    "    for i, col in enumerate(df_raw.columns):\n",
    "        col_lower = str(col).lower().strip()\n",
    "        \n",
    "        if col_lower == 'pollutant':\n",
    "            col_map[col] = 'pollutant'\n",
    "        elif col_lower == 'applies':\n",
    "            col_map[col] = 'applies'\n",
    "        elif col_lower == 'objective':\n",
    "            col_map[col] = 'objective'\n",
    "        elif 'concentration' in col_lower or col_lower == 'measured as':\n",
    "            # this is the concentration measured as column\n",
    "            concentration_col = col\n",
    "            col_map[col] = 'concentration_measured_as'\n",
    "    \n",
    "    # if concentration column not found by name, try by position\n",
    "    # typically it's the 4th column (index 3)\n",
    "    if concentration_col is None:\n",
    "        if len(df_raw.columns) > 3:\n",
    "            concentration_col = df_raw.columns[3]\n",
    "            col_map[concentration_col] = 'concentration_measured_as'\n",
    "            print(f\"using column position 3 for concentration: {concentration_col}\")\n",
    "    \n",
    "    df_raw = df_raw.rename(columns=col_map)\n",
    "    \n",
    "    print(f\"mapped columns: {list(col_map.values())}\")\n",
    "    \n",
    "    # check required columns exist\n",
    "    required_cols = ['pollutant', 'applies', 'objective', 'concentration_measured_as']\n",
    "    missing_cols = [col for col in required_cols if col not in df_raw.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\nerror: missing required columns: {missing_cols}\")\n",
    "        print(f\"mapped columns: {df_raw.columns.tolist()}\")\n",
    "        \n",
    "        # if only concentration is missing, check if we can merge columns\n",
    "        if missing_cols == ['concentration_measured_as']:\n",
    "            print(\"\\nattempting to find concentration column by content...\")\n",
    "            \n",
    "            # look for columns containing time period keywords\n",
    "            for col in df_raw.columns:\n",
    "                if col not in ['pollutant', 'applies', 'objective']:\n",
    "                    # check if column contains time period text\n",
    "                    sample_text = ' '.join(df_raw[col].dropna().astype(str).head(10).tolist()).lower()\n",
    "                    if any(word in sample_text for word in ['hour', 'mean', 'annual', 'day', 'running']):\n",
    "                        print(f\"found concentration column by content: {col}\")\n",
    "                        df_raw = df_raw.rename(columns={col: 'concentration_measured_as'})\n",
    "                        break\n",
    "        \n",
    "        # check again after attempted fix\n",
    "        missing_cols = [col for col in required_cols if col not in df_raw.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"still missing: {missing_cols}\")\n",
    "            return None\n",
    "    \n",
    "    # select only needed columns\n",
    "    df = df_raw[required_cols].copy()\n",
    "    \n",
    "    # clean text in all columns\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    # remove rows with missing critical data\n",
    "    df = df.replace(['nan', 'None', '<NA>', ''], pd.NA)\n",
    "    \n",
    "    print(f\"rows after cleanup: {len(df)}\")\n",
    "    \n",
    "    # forward fill pollutant names\n",
    "    df['pollutant'] = df['pollutant'].fillna(method='ffill')\n",
    "    \n",
    "    print(\"\\nfiltering for uk only limits...\")\n",
    "    # filter for uk only\n",
    "    df_uk = df[df['applies'].str.strip().str.upper() == 'UK'].copy()\n",
    "    print(f\"uk rows found: {len(df_uk)}\")\n",
    "    \n",
    "    if len(df_uk) == 0:\n",
    "        print(\"\\nerror: no uk rows found after filtering\")\n",
    "        print(\"sample applies values found:\")\n",
    "        print(df['applies'].value_counts().head(10))\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nextracting limit values and units from objectives...\")\n",
    "    \n",
    "    # extract numeric limit from objective\n",
    "    df_uk['limit'] = df_uk['objective'].str.extract(r'([\\d,]+(?:\\.\\d+)?)', expand=False)\n",
    "    df_uk['limit'] = df_uk['limit'].str.replace(',', '', regex=False)\n",
    "    df_uk['limit'] = pd.to_numeric(df_uk['limit'], errors='coerce')\n",
    "    \n",
    "    # extract unit from objective\n",
    "    df_uk['unit'] = df_uk['objective'].str.extract(r'[\\d,]+(?:\\.\\d+)?\\s*([^\\s]+)', expand=False)\n",
    "    \n",
    "    # clean up unit extraction\n",
    "    df_uk['unit'] = df_uk['unit'].str.extract(r'^([µμmng]+/m[²³3])', expand=False)\n",
    "    \n",
    "    # fallback for missing units\n",
    "    mask_missing_unit = df_uk['unit'].isna()\n",
    "    df_uk.loc[mask_missing_unit, 'unit'] = df_uk.loc[mask_missing_unit, 'objective'].str.extract(\n",
    "        r'(µg/m³|μg/m³|mg/m³|ng/m³|ug/m3)', \n",
    "        expand=False\n",
    "    )\n",
    "    \n",
    "    print(f\"extracted limits for {df_uk['limit'].notna().sum()} rows\")\n",
    "    print(f\"extracted units for {df_uk['unit'].notna().sum()} rows\")\n",
    "    \n",
    "    # map pollutant names to standardised codes\n",
    "    print(\"\\nmapping pollutants to standardised codes...\")\n",
    "    \n",
    "    df_uk['pollutant_std'] = df_uk['pollutant'].str.strip().str.lower().map(pollutant_map)\n",
    "    \n",
    "    # manual mappings for common pdf pollutant names\n",
    "    manual_map = {\n",
    "        'particles (pm10)': 'PM10',\n",
    "        'particles (pm2.5)': 'PM2.5',\n",
    "        'particles (pm2.5) exposure reduction': 'PM2.5',\n",
    "        'pm10': 'PM10',\n",
    "        'pm2.5': 'PM2.5',\n",
    "        'nitrogen dioxide': 'NO2',\n",
    "        'ozone': 'O3',\n",
    "        'sulphur dioxide': 'SO2',\n",
    "        'carbon monoxide': 'CO',\n",
    "        'benzene': 'BENZENE',\n",
    "        'lead': 'LEAD',\n",
    "        '1,3-butadiene': 'BUTADIENE',\n",
    "        'nitrogen oxides': 'NOX',\n",
    "        'polycyclic aromatic hydrocarbons': 'PAH'\n",
    "    }\n",
    "    \n",
    "    # apply manual mappings where metadata mapping failed\n",
    "    mask_no_std = df_uk['pollutant_std'].isna()\n",
    "    df_uk.loc[mask_no_std, 'pollutant_std'] = df_uk.loc[mask_no_std, 'pollutant'].str.strip().str.lower().map(manual_map)\n",
    "    \n",
    "    print(f\"mapped {df_uk['pollutant_std'].notna().sum()} pollutants to standardised codes\")\n",
    "    \n",
    "    # show pollutants that could not be mapped\n",
    "    unmapped = df_uk[df_uk['pollutant_std'].isna()]\n",
    "    if len(unmapped) > 0:\n",
    "        print(f\"\\nwarning: {len(unmapped)} pollutants could not be mapped:\")\n",
    "        for poll in unmapped['pollutant'].unique():\n",
    "            print(f\"  {poll}\")\n",
    "    \n",
    "    # rename column to match requirements\n",
    "    df_uk = df_uk.rename(columns={'concentration_measured_as': 'concentration measured as'})\n",
    "    \n",
    "    # select final columns in specified order\n",
    "    final_cols = [\n",
    "        'pollutant',\n",
    "        'pollutant_std', \n",
    "        'limit',\n",
    "        'unit',\n",
    "        'objective',\n",
    "        'concentration measured as',\n",
    "        'applies'\n",
    "    ]\n",
    "    \n",
    "    df_final = df_uk[final_cols].copy()\n",
    "    \n",
    "    # warn about missing limits\n",
    "    missing_limits = df_final['limit'].isna().sum()\n",
    "    if missing_limits > 0:\n",
    "        print(f\"\\nwarning: {missing_limits} rows have no numeric limit extracted\")\n",
    "    \n",
    "    print(f\"\\nfinal dataset: {len(df_final)} rows\")\n",
    "    \n",
    "    # show summary by pollutant\n",
    "    print(\"\\nsummary by pollutant:\")\n",
    "    print(\"-\" * 40)\n",
    "    summary = df_final.groupby('pollutant', dropna=False).agg({\n",
    "        'limit': 'count',\n",
    "        'pollutant_std': lambda x: x.mode()[0] if len(x.mode()) > 0 else None\n",
    "    }).rename(columns={'limit': 'num_limits', 'pollutant_std': 'std_code'})\n",
    "    \n",
    "    for idx, row in summary.iterrows():\n",
    "        std_code = row['std_code'] if pd.notna(row['std_code']) else 'unmapped'\n",
    "        print(f\"{idx}: {row['num_limits']} limit(s) [{std_code}]\")\n",
    "    \n",
    "    # save to csv\n",
    "    print(f\"\\nsaving to csv: {csv_output_path}\")\n",
    "    csv_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_final.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"done\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03ac0faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting pdf parsing...\n",
      "\n",
      "========================================\n",
      "parsing defra air quality objectives pdf\n",
      "========================================\n",
      "pdf path: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/Air_Quality_Objectives_Update.pdf\n",
      "metadata path: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/test/std_london_sites_pollutant.csv\n",
      "output path: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/uk_pollutant_limits.csv\n",
      "\n",
      "loading metadata for pollutant standardisation...\n",
      "loaded 144 metadata records\n",
      "built pollutant mapping with 185 entries\n",
      "\n",
      "extracting tables from pdf...\n",
      "pdf has 4 pages\n",
      "page 1: found 1 table(s)\n",
      "page 2: found 1 table(s)\n",
      "page 3: found 1 table(s)\n",
      "page 4: found 1 table(s)\n",
      "total rows extracted: 120\n",
      "\n",
      "processing extracted data...\n",
      "found header at row 1\n",
      "original columns: ['Pollutant', 'Applies', 'Objective', 'Concentration', '', 'Date to be', '', 'European Obligations', '', 'Date to be', '']\n",
      "mapped columns: ['pollutant', 'applies', 'objective', 'concentration_measured_as']\n",
      "rows after cleanup: 67\n",
      "\n",
      "filtering for uk only limits...\n",
      "uk rows found: 17\n",
      "\n",
      "extracting limit values and units from objectives...\n",
      "extracted limits for 17 rows\n",
      "extracted units for 17 rows\n",
      "\n",
      "mapping pollutants to standardised codes...\n",
      "mapped 12 pollutants to standardised codes\n",
      "\n",
      "warning: 5 pollutants could not be mapped:\n",
      "  <NA>\n",
      "  Pollutant\n",
      "  Ozone: protection of vegetation and ecosystems\n",
      "\n",
      "final dataset: 17 rows\n",
      "\n",
      "summary by pollutant:\n",
      "----------------------------------------\n",
      "1,3-butadiene: 1 limit(s) [BUTADIENE]\n",
      "Carbon monoxide: 1 limit(s) [CO]\n",
      "Lead: 1 limit(s) [LEAD]\n",
      "Nitrogen dioxide: 1 limit(s) [NO2]\n",
      "Nitrogen oxides: 1 limit(s) [NOx]\n",
      "Ozone: 3 limit(s) [O3]\n",
      "Ozone: protection of vegetation and ecosystems: 1 limit(s) [unmapped]\n",
      "Pollutant: 2 limit(s) [unmapped]\n",
      "Polycyclic Aromatic Hydrocarbons: 1 limit(s) [PAH]\n",
      "Sulphur dioxide: 3 limit(s) [SO2]\n",
      "nan: 2 limit(s) [unmapped]\n",
      "\n",
      "saving to csv: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/uk_pollutant_limits.csv\n",
      "done\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "preview of parsed data\n",
      "========================================\n",
      "                                     pollutant pollutant_std    limit  unit                                                                                                                                         objective concentration measured as applies\n",
      "                                          <NA>           NaN    50.00 µg/m3                                                                                                                                50 µg/m3 not to be              24 hour mean      UK\n",
      "                                          <NA>           NaN    40.00 µg/m3                                                                                                                                          40 µg/m3               annual mean      UK\n",
      "                                     Pollutant           NaN   200.00 µg/m3                                                                                                                               200 µg/m3 not to be               1 hour mean      UK\n",
      "                              Nitrogen dioxide           NO2    40.00 µg/m3                                                                                                                                          40 µg/m3               annual mean      UK\n",
      "                                         Ozone            O3   100.00 µg/m3                                                                                            100 µg/m3 not to be exceeded more than 10 times a year               8 hour mean      UK\n",
      "                                         Ozone            O3   266.00 µg/m3                                                                                                                               266 µg/m3 not to be                 15 minute      UK\n",
      "                                         Ozone            O3   350.00 µg/m3                                                                                                                               350 µg/m3 not to be               1 hour mean      UK\n",
      "                               Sulphur dioxide           SO2   125.00 µg/m3                                                                                                                               125 µg/m3 not to be              24 hour mean      UK\n",
      "              Polycyclic Aromatic Hydrocarbons           PAH     0.25 ng/m3                                                                                                                                  0.25 ng/m3 B[a]P         as annual average      UK\n",
      "                                     Pollutant           NaN    16.25 µg/m3                                                                                                                                       16.25 µg/m3            running annual      UK\n",
      "                                 1,3-butadiene     BUTADIENE     2.25 µg/m3                                                                                                                                        2.25 µg/m3       running annual mean      UK\n",
      "                               Carbon monoxide            CO    10.00 mg/m3                                                                                                                                          10 mg/m3             maximum daily      UK\n",
      "                                          Lead          LEAD     0.50 µg/m3                                                                                                                                         0.5 µg/m3               annual mean      UK\n",
      "                               Nitrogen oxides           NOx    30.00 µg/m3                                                                                                                                          30 µg/m3               annual mean      UK\n",
      "                               Sulphur dioxide           SO2    20.00 µg/m3                                                                                                                                          20 µg/m3               annual mean      UK\n",
      "                               Sulphur dioxide           SO2    20.00 µg/m3                                                                                                                                          20 µg/m3            winter average      UK\n",
      "Ozone: protection of vegetation and ecosystems           NaN 18000.00 µg/m3 Target value of 18,000 µg/m3 based on AOT40 to be calculated from 1 hour values from May to July, and to be achieved, so far as possible, by 2010      Average over 5 years      UK\n",
      "\n",
      "========================================\n",
      "checking output file\n",
      "========================================\n",
      "file created: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/uk_pollutant_limits.csv\n",
      "file size: 1.26 kb\n",
      "csv readable: 17 rows\n",
      "columns: ['pollutant', 'pollutant_std', 'limit', 'unit', 'objective', 'concentration measured as', 'applies']\n",
      "\n",
      "all pollutants found:\n",
      "----------------------------------------\n",
      "nan: 0 limit(s)\n",
      "Pollutant: 2 limit(s)\n",
      "Nitrogen dioxide: 1 limit(s)\n",
      "Ozone: 3 limit(s)\n",
      "Sulphur dioxide: 3 limit(s)\n",
      "Polycyclic Aromatic Hydrocarbons: 1 limit(s)\n",
      "1,3-butadiene: 1 limit(s)\n",
      "Carbon monoxide: 1 limit(s)\n",
      "Lead: 1 limit(s)\n",
      "Nitrogen oxides: 1 limit(s)\n",
      "Ozone: protection of vegetation and ecosystems: 1 limit(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting pdf parsing...\n",
      "\n",
      "========================================\n",
      "parsing defra air quality objectives pdf\n",
      "========================================\n",
      "pdf path: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/Air_Quality_Objectives_Update.pdf\n",
      "metadata path: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/test/std_london_sites_pollutant.csv\n",
      "output path: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/uk_pollutant_limits.csv\n",
      "\n",
      "loading metadata for pollutant standardisation...\n",
      "loaded 144 metadata records\n",
      "built pollutant mapping with 185 entries\n",
      "\n",
      "extracting tables from pdf...\n",
      "pdf has 4 pages\n",
      "page 1: found 1 table(s)\n",
      "page 2: found 1 table(s)\n",
      "page 3: found 1 table(s)\n",
      "page 4: found 1 table(s)\n",
      "total rows extracted: 120\n",
      "\n",
      "processing extracted data...\n",
      "found header at row 1\n",
      "original columns: ['Pollutant', 'Applies', 'Objective', 'Concentration', '', 'Date to be', '', 'European Obligations', '', 'Date to be', '']\n",
      "mapped columns: ['pollutant', 'applies', 'objective', 'concentration_measured_as']\n",
      "rows after cleanup: 67\n",
      "\n",
      "filtering for uk only limits...\n",
      "uk rows found: 17\n",
      "\n",
      "extracting limit values and units from objectives...\n",
      "extracted limits for 17 rows\n",
      "extracted units for 17 rows\n",
      "\n",
      "mapping pollutants to standardised codes...\n",
      "mapped 12 pollutants to standardised codes\n",
      "\n",
      "warning: 5 pollutants could not be mapped:\n",
      "  <NA>\n",
      "  Pollutant\n",
      "  Ozone: protection of vegetation and ecosystems\n",
      "\n",
      "final dataset: 17 rows\n",
      "\n",
      "summary by pollutant:\n",
      "----------------------------------------\n",
      "1,3-butadiene: 1 limit(s) [BUTADIENE]\n",
      "Carbon monoxide: 1 limit(s) [CO]\n",
      "Lead: 1 limit(s) [LEAD]\n",
      "Nitrogen dioxide: 1 limit(s) [NO2]\n",
      "Nitrogen oxides: 1 limit(s) [NOx]\n",
      "Ozone: 3 limit(s) [O3]\n",
      "Ozone: protection of vegetation and ecosystems: 1 limit(s) [unmapped]\n",
      "Pollutant: 2 limit(s) [unmapped]\n",
      "Polycyclic Aromatic Hydrocarbons: 1 limit(s) [PAH]\n",
      "Sulphur dioxide: 3 limit(s) [SO2]\n",
      "nan: 2 limit(s) [unmapped]\n",
      "\n",
      "saving to csv: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/uk_pollutant_limits.csv\n",
      "done\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "preview of parsed data\n",
      "========================================\n",
      "                                     pollutant pollutant_std    limit  unit                                                                                                                                         objective concentration measured as applies\n",
      "                                          <NA>           NaN    50.00 µg/m3                                                                                                                                50 µg/m3 not to be              24 hour mean      UK\n",
      "                                          <NA>           NaN    40.00 µg/m3                                                                                                                                          40 µg/m3               annual mean      UK\n",
      "                                     Pollutant           NaN   200.00 µg/m3                                                                                                                               200 µg/m3 not to be               1 hour mean      UK\n",
      "                              Nitrogen dioxide           NO2    40.00 µg/m3                                                                                                                                          40 µg/m3               annual mean      UK\n",
      "                                         Ozone            O3   100.00 µg/m3                                                                                            100 µg/m3 not to be exceeded more than 10 times a year               8 hour mean      UK\n",
      "                                         Ozone            O3   266.00 µg/m3                                                                                                                               266 µg/m3 not to be                 15 minute      UK\n",
      "                                         Ozone            O3   350.00 µg/m3                                                                                                                               350 µg/m3 not to be               1 hour mean      UK\n",
      "                               Sulphur dioxide           SO2   125.00 µg/m3                                                                                                                               125 µg/m3 not to be              24 hour mean      UK\n",
      "              Polycyclic Aromatic Hydrocarbons           PAH     0.25 ng/m3                                                                                                                                  0.25 ng/m3 B[a]P         as annual average      UK\n",
      "                                     Pollutant           NaN    16.25 µg/m3                                                                                                                                       16.25 µg/m3            running annual      UK\n",
      "                                 1,3-butadiene     BUTADIENE     2.25 µg/m3                                                                                                                                        2.25 µg/m3       running annual mean      UK\n",
      "                               Carbon monoxide            CO    10.00 mg/m3                                                                                                                                          10 mg/m3             maximum daily      UK\n",
      "                                          Lead          LEAD     0.50 µg/m3                                                                                                                                         0.5 µg/m3               annual mean      UK\n",
      "                               Nitrogen oxides           NOx    30.00 µg/m3                                                                                                                                          30 µg/m3               annual mean      UK\n",
      "                               Sulphur dioxide           SO2    20.00 µg/m3                                                                                                                                          20 µg/m3               annual mean      UK\n",
      "                               Sulphur dioxide           SO2    20.00 µg/m3                                                                                                                                          20 µg/m3            winter average      UK\n",
      "Ozone: protection of vegetation and ecosystems           NaN 18000.00 µg/m3 Target value of 18,000 µg/m3 based on AOT40 to be calculated from 1 hour values from May to July, and to be achieved, so far as possible, by 2010      Average over 5 years      UK\n",
      "\n",
      "========================================\n",
      "checking output file\n",
      "========================================\n",
      "file created: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/capabilities/uk_pollutant_limits.csv\n",
      "file size: 1.26 kb\n",
      "csv readable: 17 rows\n",
      "columns: ['pollutant', 'pollutant_std', 'limit', 'unit', 'objective', 'concentration measured as', 'applies']\n",
      "\n",
      "all pollutants found:\n",
      "----------------------------------------\n",
      "nan: 0 limit(s)\n",
      "Pollutant: 2 limit(s)\n",
      "Nitrogen dioxide: 1 limit(s)\n",
      "Ozone: 3 limit(s)\n",
      "Sulphur dioxide: 3 limit(s)\n",
      "Polycyclic Aromatic Hydrocarbons: 1 limit(s)\n",
      "1,3-butadiene: 1 limit(s)\n",
      "Carbon monoxide: 1 limit(s)\n",
      "Lead: 1 limit(s)\n",
      "Nitrogen oxides: 1 limit(s)\n",
      "Ozone: protection of vegetation and ecosystems: 1 limit(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/l4lrwh4d4qv962_vkt4bvly80000gn/T/ipykernel_29321/1374998651.py:179: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['pollutant'] = df['pollutant'].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# run the parsing function\n",
    "print(\"starting pdf parsing...\")\n",
    "\n",
    "result_df = parse_defra_aq_objectives(\n",
    "    pdf_path=pdf_path,\n",
    "    csv_output_path=csv_output_path,\n",
    "    metadata_path=metadata_path\n",
    ")\n",
    "\n",
    "if result_df is not None:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"preview of parsed data\")\n",
    "    print(\"=\"*40)\n",
    "    print(result_df.head(20).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"checking output file\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if csv_output_path.exists():\n",
    "        print(f\"file created: {csv_output_path}\")\n",
    "        print(f\"file size: {csv_output_path.stat().st_size / 1024:.2f} kb\")\n",
    "        \n",
    "        verify_df = pd.read_csv(csv_output_path)\n",
    "        print(f\"csv readable: {len(verify_df)} rows\")\n",
    "        print(f\"columns: {verify_df.columns.tolist()}\")\n",
    "        \n",
    "        print(\"\\nall pollutants found:\")\n",
    "        print(\"-\" * 40)\n",
    "        for poll in verify_df['pollutant'].unique():\n",
    "            count = len(verify_df[verify_df['pollutant'] == poll])\n",
    "            print(f\"{poll}: {count} limit(s)\")\n",
    "    else:\n",
    "        print(\"file was not created\")\n",
    "else:\n",
    "    print(\"\\nparsing failed, check error messages above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd26c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042b68d2",
   "metadata": {},
   "source": [
    "## 4) Data Quality validations:\n",
    "\n",
    "\n",
    "A critical gap from the laqn report by applying formal statistical tests to validate data quality patterns. While descriptive statistics show 0% (before I notice the flags of the dataset) issue rate, I need statistical evidence that this pattern is real and not due to chance.\n",
    "\n",
    "\n",
    "#### Purpuse:\n",
    " Checking data qualities if it is in the limits of eea, and make sence for general logic.\n",
    "- Outlier detection in pollutant measurements.\n",
    "- Data validity ranges based on WHO/EEA standards.\n",
    "- Measurement consistency across time periods.\n",
    "- Quality flags and suspicious patterns.\n",
    "\n",
    "### methodology\n",
    " applies environmental data quality assessment standards:\n",
    "1. Load aggregated measurement data from all csv files.\n",
    "2. Calculate statistical distributions for each pollutant type.\n",
    "3. Identify outliers using IQR method and domain knowledge.\n",
    "4. Check values against established valid ranges.\n",
    "5. Flag suspicious patterns constant values, extreme spikes.\n",
    "6. Calculate quality scores for each station-pollutant combination.\n",
    "\n",
    "#### air quality measurement standards\n",
    "\n",
    "- Uk air quality objectives, limits and policy.\n",
    "- https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://uk-air.defra.gov.uk/assets/documents/Air_Quality_Objectives_Update_20230403.pdf\n",
    "\n",
    "- DEFRA. (2023). *Air Pollution in the UK 2022*.\n",
    "  - Source: https://uk-air.defra.gov.uk/library/annualreport/\n",
    "  - Air Quality Objectives and limit values\n",
    "  - Compliance assessment methodology\n",
    "\n",
    "- UK Air Information Resource. (2024). *Air Pollution: UK Limits*.\n",
    "  - Source: https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "  - Current UK air quality objectives\n",
    "  - Legal limit values and target dates\n",
    "  - Measurement unit specifications (µg/m³)\n",
    "\n",
    "  -  for the rest of the pollutants\n",
    "\n",
    "\n",
    "- uk voc policy:\n",
    "  - https://assets.publishing.service.gov.uk/media/5d7a2912ed915d522e4164a5/VO__statement_Final_12092019_CS__1_.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272828da",
   "metadata": {},
   "source": [
    "- uk_pollutant_limit.css uk policy base logicl flaw:\n",
    "    - data I fetched hourly measurements.\n",
    "    - UK limits: different averaging periods annual mean, 24-hour mean, 8-hour mean...\n",
    "    - I need to iterate my raw data according to uk_limit csv file format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f60f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_metrics(base_dir, csv_output_path):\n",
    "    \"\"\"\n",
    "    Checks if measurements are realistic using uk_pollutant_limit.csv 3rd section parsed from uk air pollution policy pdf.\n",
    "    function validates all measurements against official uk air quality objectives\n",
    "    \n",
    "    - Loads all measurement files.\n",
    "    - Reads uk legal limits from parsed pdf csv.\n",
    "    - For each pollutant, checks if values exceed uk limits.\n",
    "    - Finds negative values.\n",
    "    - Finds extreme values probably sensor err.\n",
    "    - Calculates uk stD.\n",
    "    \n",
    "    Parameters:\n",
    "        base_dir : \n",
    "        uk_limits_path : uk_pollutant_limits.csv from parsed pdf\n",
    "            \n",
    "    \"\"\"\n",
    "    if not Path(csv_output_path).exists():\n",
    "        print(f\"error: uk limits file not found at {csv_output_path}\")\n",
    "        return {}\n",
    "\n",
    "    # load uk legal limits from parsed pdf\n",
    "    uk_limits = pd.read_csv(csv_output_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Create uk limits lookup dict structure = {pollutant_std: {limit_type: limit_value}}\n",
    "    uk_limits_dict = {}\n",
    "    \n",
    "    for _, row in uk_limits.iterrows():\n",
    "        poll_std = row['pollutant_std']\n",
    "        limit_val = row['limit']\n",
    "        conc_type = str(row['concentration measured as']).lower().strip()\n",
    "        unit = row['unit']\n",
    "        \n",
    "        if pd.notna(poll_std) and pd.notna(limit_val):\n",
    "            if poll_std not in uk_limits_dict:\n",
    "                uk_limits_dict[poll_std] = []\n",
    "            \n",
    "        if pd.notna(poll_std) and pd.notna(limit_val):\n",
    "            if poll_std not in uk_limits_dict:\n",
    "                uk_limits_dict[poll_std] = []\n",
    "\n",
    "        #  averaging period detection\n",
    "        avg_period = 'unknown'\n",
    "        if 'annual' in conc_type and 'running' in conc_type:\n",
    "            avg_period = 'running_annual'\n",
    "        elif 'running annual' in conc_type:\n",
    "            avg_period = 'running_annual'\n",
    "        elif 'annual' in conc_type:\n",
    "            avg_period = 'annual'\n",
    "        elif '24 hour' in conc_type or '24-hour' in conc_type:\n",
    "            avg_period = '24hour'\n",
    "        elif '8 hour' in conc_type or '8-hour' in conc_type:\n",
    "            avg_period = '8hour'\n",
    "        elif '1 hour' in conc_type or '1-hour' in conc_type or 'hour mean' in conc_type:\n",
    "            avg_period = '1hour'\n",
    "        elif 'maximum daily' in conc_type:\n",
    "            avg_period = 'daily_max'\n",
    "\n",
    "        uk_limits_dict[poll_std].append({\n",
    "            'limit': float(limit_val),\n",
    "            'type': conc_type,\n",
    "            'unit': unit,\n",
    "            'avg_period': avg_period\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nUK limits loaded for {len(uk_limits_dict)} pollutants:\")\n",
    "    for poll, limits in uk_limits_dict.items():\n",
    "        period_info = ', '.join([f\"{lim['avg_period']}: {lim['limit']}\" for lim in limits])\n",
    "        print(f\"  {poll}: {period_info}\")\n",
    "    \n",
    "    # load all measurement data with timestamp\n",
    "    print(\"\\nLoading measurement data.\")\n",
    "    all_data = []\n",
    "    \n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        if year_dir.exists():\n",
    "            for csv_file in year_dir.rglob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    if not df.empty and 'timestamp' in df.columns:\n",
    "                        all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"err no measurement data found\")\n",
    "        return {}\n",
    "    \n",
    "    df_all = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"loaded {len(df_all):,} total records\")\n",
    "    \n",
    "    # filter valid values and parse timestamp\n",
    "    df_valid = df_all[df_all['value'].notna()].copy()\n",
    "    df_valid['value'] = pd.to_numeric(df_valid['value'], errors='coerce')\n",
    "    df_valid = df_valid[df_valid['value'].notna()]\n",
    "    \n",
    "    # parse timestamp to datetime\n",
    "    df_valid['timestamp'] = pd.to_datetime(df_valid['timestamp'], errors='coerce')\n",
    "    df_valid = df_valid[df_valid['timestamp'].notna()]\n",
    "    \n",
    "    print(f\"Analysing {len(df_valid):,} valid measurements with timestamps\")\n",
    "    \n",
    "    # calculate quality metrics for each pollutant\n",
    "    print(\"\\nProcessing quality metrics by pollutant...\")\n",
    "    quality_results = {}\n",
    "    \n",
    "    for pollutant in df_valid['pollutant_std'].unique():\n",
    "        if pd.isna(pollutant):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nprocessing {pollutant}...\")\n",
    "        \n",
    "        poll_data = df_valid[df_valid['pollutant_std'] == pollutant].copy()\n",
    "        \n",
    "        if len(poll_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # basic statistics on raw hourly data\n",
    "        q_metrics = {\n",
    "            'pollutant': pollutant,\n",
    "            'count': int(len(poll_data)),\n",
    "            'mean_hourly': float(poll_data['value'].mean()),\n",
    "            'median_hourly': float(poll_data['value'].median()),\n",
    "            'std_hourly': float(poll_data['value'].std()),\n",
    "            'min': float(poll_data['value'].min()),\n",
    "            'max': float(poll_data['value'].max()),\n",
    "            'p95': float(poll_data['value'].quantile(0.95)),\n",
    "            'p99': float(poll_data['value'].quantile(0.99))\n",
    "        }\n",
    "        \n",
    "        # check for suspicious values\n",
    "        negative_count = (poll_data['value'] < 0).sum()\n",
    "        zero_count = (poll_data['value'] == 0).sum()\n",
    "        \n",
    "        q_metrics['negative_values'] = int(negative_count)\n",
    "        q_metrics['negative_pct'] = float((negative_count / len(poll_data) * 100))\n",
    "        q_metrics['zero_values'] = int(zero_count)\n",
    "        q_metrics['zero_pct'] = float((zero_count / len(poll_data) * 100))\n",
    "        \n",
    "        # now check against uk limits with proper averaging\n",
    "        if pollutant in uk_limits_dict:\n",
    "            uk_poll_limits = uk_limits_dict[pollutant]\n",
    "            \n",
    "            for limit_info in uk_poll_limits:\n",
    "                avg_period = limit_info['avg_period']\n",
    "                limit_value = limit_info['limit']\n",
    "                \n",
    "                if avg_period == 'annual':\n",
    "                    # calculate annual mean\n",
    "                    poll_data['year'] = poll_data['timestamp'].dt.year\n",
    "                    annual_means = poll_data.groupby('year')['value'].mean()\n",
    "                    \n",
    "                    q_metrics['uk_annual_limit'] = limit_value\n",
    "                    q_metrics['mean_annual'] = float(annual_means.mean())\n",
    "                    q_metrics['exceeds_uk_annual'] = q_metrics['mean_annual'] > limit_value\n",
    "                    \n",
    "                    print(f\"  annual mean: {q_metrics['mean_annual']:.2f} vs limit {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '24hour':\n",
    "                    # calculate daily means\n",
    "                    poll_data['date'] = poll_data['timestamp'].dt.date\n",
    "                    daily_means = poll_data.groupby('date')['value'].mean()\n",
    "                    \n",
    "                    exceedances = (daily_means > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_24hour_limit'] = limit_value\n",
    "                    q_metrics['daily_exceedances'] = int(exceedances)\n",
    "                    q_metrics['daily_exceedances_pct'] = float((exceedances / len(daily_means) * 100))\n",
    "                    \n",
    "                    print(f\"  24-hour: {exceedances} days exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '8hour':\n",
    "                    # calculate 8-hour rolling mean\n",
    "                    poll_data_sorted = poll_data.sort_values('timestamp')\n",
    "                    poll_data_sorted['rolling_8h'] = poll_data_sorted['value'].rolling(window=8, min_periods=6).mean()\n",
    "                    \n",
    "                    exceedances = (poll_data_sorted['rolling_8h'] > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_8hour_limit'] = limit_value\n",
    "                    q_metrics['8hour_exceedances'] = int(exceedances)\n",
    "                    q_metrics['8hour_exceedances_pct'] = float((exceedances / len(poll_data_sorted) * 100))\n",
    "                    \n",
    "                    print(f\"  8-hour: {exceedances} periods exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '1hour':\n",
    "                    # compare hourly values directly\n",
    "                    exceedances = (poll_data['value'] > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_1hour_limit'] = limit_value\n",
    "                    q_metrics['hourly_exceedances'] = int(exceedances)\n",
    "                    q_metrics['hourly_exceedances_pct'] = float((exceedances / len(poll_data) * 100))\n",
    "                    \n",
    "                    print(f\"  1-hour: {exceedances} hours exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == 'running_annual':\n",
    "                    # running annual mean (365-day rolling average)\n",
    "                    poll_data_sorted = poll_data.sort_values('timestamp')\n",
    "                    poll_data_sorted['rolling_annual'] = poll_data_sorted['value'].rolling(window=24*365, min_periods=24*300).mean()\n",
    "                    \n",
    "                    q_metrics['uk_running_annual_limit'] = limit_value\n",
    "                    q_metrics['mean_running_annual'] = float(poll_data_sorted['rolling_annual'].mean())\n",
    "                    q_metrics['exceeds_running_annual'] = q_metrics['mean_running_annual'] > limit_value\n",
    "                    \n",
    "                    print(f\"  running annual: {q_metrics['mean_running_annual']:.2f} vs limit {limit_value}\")\n",
    "                \n",
    "                elif avg_period == 'daily_max':\n",
    "                    # maximum daily 8-hour running mean\n",
    "                    poll_data_sorted = poll_data.sort_values('timestamp')\n",
    "                    poll_data_sorted['date'] = poll_data_sorted['timestamp'].dt.date\n",
    "                    poll_data_sorted['rolling_8h'] = poll_data_sorted['value'].rolling(window=8, min_periods=6).mean()\n",
    "                    \n",
    "                    daily_max = poll_data_sorted.groupby('date')['rolling_8h'].max()\n",
    "                    exceedances = (daily_max > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_daily_max_limit'] = limit_value\n",
    "                    q_metrics['daily_max_exceedances'] = int(exceedances)\n",
    "                    \n",
    "                    print(f\"  daily max 8h: {exceedances} days exceed {limit_value}\")\n",
    "            \n",
    "            # overall assessment: use most restrictive limit for out of range check\n",
    "            all_limits = [lim['limit'] for lim in uk_poll_limits]\n",
    "            max_limit = max(all_limits)\n",
    "            \n",
    "            # define extreme threshold as 10x highest uk limit\n",
    "            extreme_threshold = max_limit * 10\n",
    "            out_of_range = (poll_data['value'] > extreme_threshold).sum()\n",
    "            \n",
    "            q_metrics['extreme_threshold'] = extreme_threshold\n",
    "            q_metrics['out_of_range'] = int(out_of_range)\n",
    "            q_metrics['out_of_range_pct'] = float((out_of_range / len(poll_data) * 100))\n",
    "            \n",
    "        else:\n",
    "            # no uk limit defined for this pollutant\n",
    "            print(f\"  no uk limits defined\")\n",
    "            q_metrics['uk_annual_limit'] = None\n",
    "            q_metrics['exceeds_uk_annual'] = False\n",
    "            q_metrics['out_of_range'] = 0\n",
    "            q_metrics['out_of_range_pct'] = 0.0\n",
    "        \n",
    "        quality_results[pollutant] = q_metrics\n",
    "    \n",
    "    return quality_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fd6e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_quality_metrics(quality_results):\n",
    "    \"\"\"\n",
    "    Print comprehensive quality metrics report with uk compliance.\n",
    "    \n",
    "    Parameters:\n",
    "        quality_metrics : dict\n",
    "            Dictionary returned by calculate_quality_metrics_uk_limits\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Quality metrics report\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    " \n",
    "    for poll, metrics in quality_results.items():\n",
    "        print(f\"\\n{poll}:\")\n",
    "        print(f\"  total measurements: {metrics['count']:,}\")\n",
    "        print(f\"  hourly mean: {metrics['mean_hourly']:.2f}\")\n",
    "        \n",
    "        if 'mean_annual' in metrics:\n",
    "            print(f\"  annual mean: {metrics['mean_annual']:.2f} (limit: {metrics['uk_annual_limit']})\")\n",
    "            status = \"exceeds\" if metrics['exceeds_uk_annual'] else \"compliant\"\n",
    "            print(f\"    status: {status}\")\n",
    "        \n",
    "        if 'daily_exceedances' in metrics:\n",
    "            print(f\"  24-hour exceedances: {metrics['daily_exceedances']} days\")\n",
    "        \n",
    "        if 'hourly_exceedances' in metrics:\n",
    "            print(f\"  1-hour exceedances: {metrics['hourly_exceedances']} hours\")\n",
    "        \n",
    "        if metrics['negative_values'] > 0:\n",
    "            print(f\"  warning: {metrics['negative_values']} negative values\")\n",
    "        \n",
    "        if metrics['out_of_range'] > 0:\n",
    "            print(f\"  warning: {metrics['out_of_range']} extreme values\")\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    return quality_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8a5b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting quality metrics calculation...\n",
      "\n",
      "UK limits loaded for 11 pollutants:\n",
      "  PM10: 24hour: 50.0, annual: 40.0\n",
      "  PM2.5: annual: 20.0\n",
      "  NO2: annual: 40.0\n",
      "  O3: 8hour: 100.0\n",
      "  SO2: 24hour: 125.0\n",
      "  PAH: annual: 0.25\n",
      "  Benzene: running_annual: 16.25\n",
      "  BUTADIENE: running_annual: 2.25\n",
      "  CO: daily_max: 10.0\n",
      "  LEAD: annual: 0.5\n",
      "  NOx: annual: 30.0\n",
      "\n",
      "Loading measurement data.\n",
      "loaded 2,525,991 total records\n",
      "Analysing 2,303,824 valid measurements with timestamps\n",
      "\n",
      "Processing quality metrics by pollutant...\n",
      "\n",
      "processing Toluene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing i-Butane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing 1,2,3-TMB...\n",
      "  no uk limits defined\n",
      "\n",
      "processing Ethyne...\n",
      "  no uk limits defined\n",
      "\n",
      "processing 1-Butene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing O3...\n",
      "  8-hour: 2089 periods exceed 100.0\n",
      "\n",
      "processing 1,2,4-TMB...\n",
      "  no uk limits defined\n",
      "\n",
      "processing cis-2-Butene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing trans-2-Pentene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing NOx...\n",
      "  annual mean: 32.75 vs limit 30.0\n",
      "\n",
      "processing 1,3-Butadiene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing i-Hexane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing NO...\n",
      "  no uk limits defined\n",
      "\n",
      "processing Isoprene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing n-Hexane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing Benzene...\n",
      "  running annual: 0.50 vs limit 16.25\n",
      "\n",
      "processing Ethene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing n-Pentane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing Propene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing n-Butane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing trans-2-Butene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing n-Heptane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing 1,3,5-TMB...\n",
      "  no uk limits defined\n",
      "\n",
      "processing NO2...\n",
      "  annual mean: 20.68 vs limit 40.0\n",
      "\n",
      "processing SO2...\n",
      "  24-hour: 0 days exceed 125.0\n",
      "\n",
      "processing i-Pentane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing CO...\n",
      "  daily max 8h: 0 days exceed 10.0\n",
      "\n",
      "processing i-Octane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing n-Octane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing Propane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing PM2.5...\n",
      "  annual mean: 7.75 vs limit 20.0\n",
      "\n",
      "processing m,p-Xylene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing 1-Pentene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing Ethane...\n",
      "  no uk limits defined\n",
      "\n",
      "processing o-Xylene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing Ethylbenzene...\n",
      "  no uk limits defined\n",
      "\n",
      "processing PM10...\n",
      "  24-hour: 2 days exceed 50.0\n",
      "  annual mean: 13.19 vs limit 40.0\n",
      "saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/report/quality_metrics_validation.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# run quality metrics with proper averaging periods\n",
    "print(\"starting quality metrics calculation...\")\n",
    "\n",
    "# Calculate quality metrics\n",
    "quality_results = calculate_quality_metrics(base_dir, csv_output_path)\n",
    "\n",
    "print_quality_metrics =(quality_results)\n",
    "\n",
    "\n",
    "if quality_results:\n",
    "    # # save comprehensive report\n",
    "    # print(\"\\nsaving quality metrics report...\")\n",
    "    \n",
    "\n",
    "    \n",
    "    quality_rows = []\n",
    "    for poll, metrics in quality_results.items():\n",
    "        row = {\n",
    "            'pollutant': metrics['pollutant'],\n",
    "            'total_measurements': metrics['count'],\n",
    "            'mean_hourly': f\"{metrics['mean_hourly']:.2f}\",\n",
    "            'min': f\"{metrics['min']:.2f}\",\n",
    "            'max': f\"{metrics['max']:.2f}\",\n",
    "            'p95': f\"{metrics['p95']:.2f}\",\n",
    "            'negative_values': metrics['negative_values'],\n",
    "            'zero_values': metrics['zero_values'],\n",
    "            'out_of_range': metrics['out_of_range']\n",
    "        }\n",
    "        \n",
    "        # add uk limit compliance fields\n",
    "        if 'uk_annual_limit' in metrics and metrics['uk_annual_limit']:\n",
    "            row['uk_annual_limit'] = metrics['uk_annual_limit']\n",
    "            row['mean_annual'] = f\"{metrics['mean_annual']:.2f}\" if 'mean_annual' in metrics else 'n/a'\n",
    "            row['exceeds_annual'] = 'yes' if metrics.get('exceeds_uk_annual', False) else 'no'\n",
    "        \n",
    "        if 'daily_exceedances' in metrics:\n",
    "            row['uk_24hour_limit'] = metrics['uk_24hour_limit']\n",
    "            row['daily_exceedances'] = metrics['daily_exceedances']\n",
    "        \n",
    "        if 'hourly_exceedances' in metrics:\n",
    "            row['uk_1hour_limit'] = metrics['uk_1hour_limit']\n",
    "            row['hourly_exceedances'] = metrics['hourly_exceedances']\n",
    "        \n",
    "        quality_rows.append(row)\n",
    "    \n",
    "    pd.DataFrame(quality_rows).to_csv(quality_output, index=False)\n",
    "    print(f\"saved to: {quality_output}\")\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"quality metrics calculation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b780d4",
   "metadata": {},
   "source": [
    "## 5) Chi-Square test\n",
    "Uses statistical tests to mathematically prove that defra data collection process was consistent and reliable across time. \n",
    "It as a quality control check that ensures didn't accidentally collect more data in some months than others, which could bias defra analysis.\n",
    "\n",
    "#### Why Chi-square test?\n",
    " - The chi-square test answers one simple question: Are my monthly file counts similar enough to trust, or are some months suspiciously different? And environmental dataset chi square test most common why, according to google.\n",
    "\n",
    "- Air pollution varies by season\n",
    "- Policy decisions need unbiased evidence\n",
    "- Academic reviewers will question imbalanced datasets\n",
    "\n",
    "### What Chi-Square Test Does\n",
    "\n",
    "The chi-square test answers one simple question: Are my monthly file counts similar enough to trust, or are some months suspiciously different?\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. What  observe: Count how many data files  have for each month.\n",
    "2. What  expect: If data collection was perfect, each month should have roughly the same count.\n",
    "3. The test: Measures how far observed counts are from the expected counts.\n",
    "4. The result: Gives  a p-value that tells  if the differences are just random variation or a real problem.\n",
    "\n",
    "\n",
    "### P-Value Meaning\n",
    "\n",
    "The p-value tells the probability that  observed pattern happened by random chance:\n",
    "\n",
    "| P-Value | Interpretation | What It Means for DEFRA Data |\n",
    "|---------|---------------|----------------------------|\n",
    "| p greater than or equal to 0.05 | Accept null hypothesis | Data is evenly distributed. Small differences between months are just normal variation.  data collection was consistent. |\n",
    "| p less than 0.05 | Reject null hypothesis | Data is unevenly distributed. Some months have significantly more or less data than others.  should investigate why. |\n",
    "\n",
    "\n",
    "\n",
    "### 1. Methodological Rigor\n",
    " data collection needs to be reliable\n",
    "\n",
    "- Mathematical evidence not just visual inspection\n",
    "- A standardized statistical measure p-value\n",
    "- Reproducible results \n",
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "The test produces:\n",
    "\n",
    "   - Test name: \"Chi-square uniformity\"\n",
    "   - Chi-square statistic (χ²)\n",
    "   - P-value\n",
    "   - Interpretation (evenly/unevenly distributed)\n",
    "\n",
    "2. Console output showing:\n",
    "   - Null hypothesis statement\n",
    "   - Test statistic value\n",
    "   - P-value\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Results for  Dataset\n",
    "\n",
    "Based on  data collection using the DEFRA API:\n",
    "\n",
    "- Expected p-value: greater than 0.05 (likely around 0.3-0.7)\n",
    "- Why:  API calls were automated and systematic\n",
    "- What this proves: Each month has 249 station-pollutant files (one per combination)\n",
    "\n",
    "### If Get p less than 0.05\n",
    "\n",
    "This would suggest:\n",
    "1. Some months might have missing API data\n",
    "2. New monitoring stations came online mid-year\n",
    "3. Some stations stopped reporting in certain months\n",
    "\n",
    "\n",
    "### Results Data Quality Section\n",
    "\n",
    "Include the statistical test results as evidence that dataset is:\n",
    "- Temporally balanced\n",
    "- Methodologically sound\n",
    "- Suitable for seasonal analysis\n",
    "\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|---------|\n",
    "| Test Used | Chi-square test for uniformity |\n",
    "| What It Tests | Whether monthly file counts are evenly distributed |\n",
    "| Null Hypothesis | Data is uniformly distributed across months |\n",
    "| Alternative Hypothesis | Data shows significant monthly imbalance |\n",
    "| Acceptance Criterion | p-value greater than or equal to 0.05 |\n",
    "| What p greater than or equal to 0.05 Means | Data collection was consistent and reliable |\n",
    "| What p less than 0.05 Means | Some months have significantly different data volumes |\n",
    "| Why This Matters | Proves  dataset is methodologically sound for thesis |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### If p greater than or equal to 0.05 (Expected Result)\n",
    "\n",
    "1. Document result in thesis methodology\n",
    "2. Include p-value in data quality section\n",
    "3. Proceed with confidence to seasonal analysis\n",
    "\n",
    "### If p less than 0.05 (Unexpected Result)\n",
    "\n",
    "1. Review monthly counts to identify outliers\n",
    "2. Check API logs for that month\n",
    "3. Document known issues (e.g., \"Station X offline in April 2024\")\n",
    "4. Consider excluding problematic months OR\n",
    "5. Use weighted analysis to account for imbalance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1a6c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chi_square_tests(base_dir):\n",
    "    \"\"\"\n",
    "    Run statistical tests to prove data collection was consistent.\n",
    "    \n",
    "    - Chi-square test: Checks if  similar amounts of data for each month\n",
    "    - If p-value < 0.05 Data isn't evenly spreats problem!\n",
    "    - If p-value > 0.05 Data is evenly spreats good!\n",
    "    \n",
    "    Parameters:\n",
    "        base_dir : \n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Count files per month, 2025 only 19th of nov.\n",
    "    yearly_data = {'2023': 0, '2024': 0, '2025': 0}\n",
    "    year_months = {'2023': 12, '2024': 12, '2025': 11}\n",
    "    \n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        if not year_dir.exists():\n",
    "            continue\n",
    "        pattern = f'*__{year}_*.csv'\n",
    "        files = list(year_dir.rglob(pattern))\n",
    "        yearly_data[year] = len(files)\n",
    "    \n",
    "    # prep for chi-square\n",
    "    year_counts = [yearly_data[y] for y in ['2023', '2024', '2025']]\n",
    "    total_files = sum(year_counts)\n",
    "    total_months = 35  # 12 + 12 + 11\n",
    "    \n",
    "    expected_counts = [\n",
    "        total_files * (year_months[year] / total_months)\n",
    "        for year in ['2023', '2024', '2025']\n",
    "    ]\n",
    "    \n",
    "    # run test\n",
    "    chi2, p_value = stats.chisquare(\n",
    "        f_obs=year_counts, \n",
    "        f_exp=expected_counts\n",
    "    )\n",
    "    for year, count, expected in zip(['2023', '2024', '2025'], \n",
    "                                      year_counts, expected_counts):\n",
    "        print(f\"  {year}: {count:5d} files (expected: {expected:7.1f})\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"Result reject null hypothesis p < 0.05\")\n",
    "        print(f\"Interpretation: Years NOT evenly distributed\")\n",
    "    else:\n",
    "        print(f\"Result: accept null hypothesis p >= 0.05\")\n",
    "        print(f\"Interpretation: Years evenly distributed\")\n",
    "    \n",
    "    return {\n",
    "        'test': 'Chi-square year-wise',\n",
    "        'chi2_statistic': chi2,\n",
    "        'p_value': p_value,\n",
    "        'year_counts': year_counts,\n",
    "        'expected_counts': expected_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c408b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2023:  1431 files (expected:  1221.6)\n",
      "  2024:  1193 files (expected:  1221.6)\n",
      "  2025:   939 files (expected:  1119.8)\n",
      "\n",
      "Chi-square statistic: 65.7553\n",
      "P-value: 0.0000\n",
      "Result reject null hypothesis p < 0.05\n",
      "Interpretation: Years NOT evenly distributed\n",
      "\n",
      "Statistical test results saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/report/chi_square_tests.csv\n"
     ]
    }
   ],
   "source": [
    "# Run tests\n",
    "test_results = chi_square_tests(base_dir)\n",
    "\n",
    "# Save results\n",
    "\n",
    "pd.DataFrame([{\n",
    "    'test_name': test_results['test'],\n",
    "    'statistic': f\"{test_results['chi2_statistic']:.4f}\",\n",
    "    'p_value': f\"{test_results['p_value']:.4f}\",\n",
    "    'interpretation': ('Evenly distributed' \n",
    "                      if test_results['p_value'] >= 0.05 \n",
    "                      else 'Unevenly distributed')\n",
    "}]).to_csv(chi_square_output, index=False)\n",
    "\n",
    "print(f\"\\nStatistical test results saved to: {chi_square_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdb0a2",
   "metadata": {},
   "source": [
    "    2023:  1431 files (expected:  1221.6)\n",
    "    2024:  1193 files (expected:  1221.6)\n",
    "    2025:   939 files (expected:  1119.8)\n",
    "\n",
    "    Chi-square statistic: 65.7553\n",
    "    P-value: 0.0000\n",
    "    Result reject null hypothesis p < 0.05\n",
    "    Interpretation: Years NOT evenly distributed\n",
    "\n",
    "    Statistical test results saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/report/chi_square_tests.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e851d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_year_difference(base_dir):\n",
    "    \"\"\"\n",
    "    Find which stations/pollutants are missing in later years.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_dir = Path(base_dir)\n",
    "    \n",
    "    # unique station-pollutant combinations per year\n",
    "    year_files = {}\n",
    "    \n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = base_dir / f'{year}measurements'\n",
    "        if not year_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        files = list(year_dir.rglob(f'*__{year}_*.csv'))\n",
    "        \n",
    "        # Extract station/pollutant combinations\n",
    "        combinations = set()\n",
    "        for f in files:\n",
    "            # File format station_pollutant_YY_MM\n",
    "            parts = f.stem.split('__')\n",
    "            if len(parts) >= 2:\n",
    "                station = parts[0]\n",
    "                pollutant = parts[1]\n",
    "                combinations.add((station, pollutant))\n",
    "        \n",
    "        year_files[year] = combinations\n",
    "    \n",
    "    # Find whats in 23 but missing in 2024/2025\n",
    "    lost_2024 = year_files['2023'] - year_files['2024']\n",
    "    lost_2025 = year_files['2023'] - year_files['2025']\n",
    "    \n",
    "    print(\"\\nStation-pollutant combinations lost over time:\")\n",
    "    print(f\" 2023 total: {len(year_files['2023'])}\")\n",
    "    print(f\"2024 total: {len(year_files['2024'])}\")\n",
    "    print(f\"2025 total: {len(year_files['2025'])}\")\n",
    "    print()\n",
    "    print(f\"Lost in 2024 (vs 2023): {len(lost_2024)}\")\n",
    "    print(f\"Lost in 2025 (vs 2023): {len(lost_2025)}\")\n",
    "    \n",
    "    if lost_2024:\n",
    "        print(\"\\nExamples lost in 2024:\")\n",
    "        for station, pollutant in list(lost_2024)[:10]:\n",
    "            print(f\"    {station} - {pollutant}\")\n",
    "    \n",
    "    if lost_2025:\n",
    "        print(\"\\nExamples lost in 2025:\")\n",
    "        for station, pollutant in list(lost_2025)[:10]:\n",
    "            print(f\"    {station} - {pollutant}\")\n",
    "    \n",
    "    return {\n",
    "        '2023_count': len(year_files['2023']),\n",
    "        '2024_count': len(year_files['2024']),\n",
    "        '2025_count': len(year_files['2025']),\n",
    "        'lost_2024': lost_2024,\n",
    "        'lost_2025': lost_2025\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86cdca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Station-pollutant combinations lost over time:\n",
      " 2023 total: 444\n",
      "2024 total: 444\n",
      "2025 total: 370\n",
      "\n",
      "Lost in 2024 (vs 2023): 444\n",
      "Lost in 2025 (vs 2023): 444\n",
      "\n",
      "Examples lost in 2024:\n",
      "    NOx - 2023_09\n",
      "    m,p-Xylene - 2023_02\n",
      "    n-Heptane - 2023_03\n",
      "    Isoprene - 2023_03\n",
      "    i-Hexane - 2023_02\n",
      "    O3 - 2023_08\n",
      "    1,2,4-TMB - 2023_10\n",
      "    CO - 2023_11\n",
      "    SO2 - 2023_09\n",
      "    Propene - 2023_10\n",
      "\n",
      "Examples lost in 2025:\n",
      "    NOx - 2023_09\n",
      "    m,p-Xylene - 2023_02\n",
      "    n-Heptane - 2023_03\n",
      "    Isoprene - 2023_03\n",
      "    i-Hexane - 2023_02\n",
      "    O3 - 2023_08\n",
      "    1,2,4-TMB - 2023_10\n",
      "    CO - 2023_11\n",
      "    SO2 - 2023_09\n",
      "    Propene - 2023_10\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "analysis = analyse_year_difference(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf7d04",
   "metadata": {},
   "source": [
    "    Station-pollutant combinations lost over time:\n",
    "    2023 total: 444\n",
    "    2024 total: 444\n",
    "    2025 total: 370\n",
    "\n",
    "    Lost in 2024 (vs 2023): 444\n",
    "    Lost in 2025 (vs 2023): 444\n",
    "\n",
    "    Examples lost in 2024:\n",
    "        NOx - 2023_09\n",
    "        m,p-Xylene - 2023_02\n",
    "        n-Heptane - 2023_03\n",
    "        Isoprene - 2023_03\n",
    "        i-Hexane - 2023_02\n",
    "        O3 - 2023_08\n",
    "        1,2,4-TMB - 2023_10\n",
    "        CO - 2023_11\n",
    "        SO2 - 2023_09\n",
    "        Propene - 2023_10\n",
    "\n",
    "    Examples lost in 2025:\n",
    "        NOx - 2023_09\n",
    "        m,p-Xylene - 2023_02\n",
    "        n-Heptane - 2023_03\n",
    "        Isoprene - 2023_03\n",
    "        i-Hexane - 2023_02\n",
    "        O3 - 2023_08\n",
    "        1,2,4-TMB - 2023_10\n",
    "        CO - 2023_11\n",
    "        SO2 - 2023_09\n",
    "        Propene - 2023_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f0e9b",
   "metadata": {},
   "source": [
    "#### adding monthly 2025 data completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adf6fe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025 monthly file counts:\n",
      "  2025-01:   95 files\n",
      "  2025-02:    0 files\n",
      "  2025-03:   92 files\n",
      "  2025-04:   94 files\n",
      "  2025-05:   94 files\n",
      "  2025-06:   94 files\n",
      "  2025-07:   94 files\n",
      "  2025-08:   94 files\n",
      "  2025-09:   94 files\n",
      "  2025-10:   94 files\n",
      "  2025-11:   94 files\n",
      "\n",
      "  Early 2025 avg (Jan-Mar): 62 files/month\n",
      "  Late 2025 avg (Sep-Nov):  94 files/month\n"
     ]
    }
   ],
   "source": [
    "def months_25 (base_dir):\n",
    "    \"\"\"\n",
    "    See which 2025 months have data.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_dir = Path(base_dir)\n",
    "    year_dir = base_dir / '2025measurements'\n",
    "    \n",
    "    monthly_counts = {}\n",
    "    \n",
    "    for month in range(1, 12):  # Jan/Nov\n",
    "        pattern = f'*__2025_{month:02d}.csv'\n",
    "        files = list(year_dir.rglob(pattern))\n",
    "        monthly_counts[f'2025-{month:02d}'] = len(files)\n",
    "    \n",
    "    print(\"\\n2025 monthly file counts:\")\n",
    "    for month, count in monthly_counts.items():\n",
    "        print(f\"  {month}: {count:4d} files\")\n",
    "    \n",
    "    # Check if recent months have less data\n",
    "    avg_early = sum(list(monthly_counts.values())[:3]) / 3\n",
    "    avg_late = sum(list(monthly_counts.values())[-3:]) / 3\n",
    "    \n",
    "    print(f\"\\n  Early 2025 avg (Jan-Mar): {avg_early:.0f} files/month\")\n",
    "    print(f\"  Late 2025 avg (Sep-Nov):  {avg_late:.0f} files/month\")\n",
    "    \n",
    "    if avg_late < avg_early * 0.9:\n",
    "        print(\"Recent months have noticeably less data\")\n",
    "\n",
    "months_25 (base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c53247",
   "metadata": {},
   "source": [
    "  2025 monthly file counts:\n",
    "    2025-01:   95 files\n",
    "    2025-02:    0 files\n",
    "    2025-03:   92 files\n",
    "    2025-04:   94 files\n",
    "    2025-05:   94 files\n",
    "    2025-06:   94 files\n",
    "    2025-07:   94 files\n",
    "    2025-08:   94 files\n",
    "    2025-09:   94 files\n",
    "    2025-10:   94 files\n",
    "    2025-11:   94 files\n",
    "\n",
    "    Early 2025 avg (Jan-Mar): 62 files/month\n",
    "    Late 2025 avg (Sep-Nov):  94 files/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2e797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
