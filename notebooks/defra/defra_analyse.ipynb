{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64359774",
   "metadata": {},
   "source": [
    "# DEFRA Dataset Assesment\n",
    "\n",
    "\n",
    "1) I'll be start adding my main paths and modules I will be using in this notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "432092e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible python modules i will be using below\n",
    "from curses import meta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "#define base path  without hardcoding\n",
    "base_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"optimised\"\n",
    "#metadata file for pollutant name, location and site names\n",
    "metadata_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" /\"test\"/\"std_london_sites_pollutant.csv\"\n",
    "\n",
    "# output path for saving statistics 1. function\n",
    "#the first analyse dataset created without inclitiong nan optimased files, and cross referencing that's why changed the name to dataset_statistics-noNAN-incl.csv\n",
    "os.makedirs(base_dir / \"report\", exist_ok=True)\n",
    "stats_output_path = base_dir/\"report\"/ \"defra_stats.csv\"\n",
    "\n",
    "# output paths for saving all the pollutant distribution and nan value analysis.\n",
    "pollutant_distrubution_path = base_dir / \"report\" / \"pollutant_distribution.csv\"\n",
    "nan_val_pollutant_split_path = base_dir / \"report\" / \"nan_values_by_pollutant.csv\"\n",
    "nan_val_stationPollutant_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"defra\" / \"report\" / \"nan_values_by_station_pollutant.csv\"\n",
    "\n",
    "\n",
    "# log file from nan replacement process\n",
    "nan_log_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"logs\" / \"NaN_values_record.csv\"\n",
    "\n",
    "# possible python modules i will be using below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f95bb4",
   "metadata": {},
   "source": [
    "## 1) Initial Dataset Assessment: Raw Numbers\n",
    "\n",
    "Before conducting quality checks, I need to establish the baseline characteristics of the DEFRA dataset. This section calculates comprehensive statistics about the data collection effort, including file counts, measurement records, station coverage, and pollutant distribution.\n",
    "\n",
    "### Purpose\n",
    "- Document the scale and scope of data collection.\n",
    "- Establish baseline metrics for comparison with LAQN.\n",
    "- Provide context for subsequent quality analysis.\n",
    "\n",
    "### Methodology\n",
    "The function `get_defra_dataset_statistics()` performs the following:\n",
    "1. Loads standardised metadata to identify unique stations and pollutants.\n",
    "2. Counts files across all three yearly directories (2023, 2024, 2025).\n",
    "3. Calculates total measurement records by reading all CSV files.\n",
    "4. Determines spatial coverage from unique coordinate pairs.\n",
    "5. Documents temporal coverage (35 months: January 2023 to November 2025).\n",
    "\n",
    "### Notes\n",
    "- File counting is fast (scans directory structure only).\n",
    "- Record counting can be slow (reads every CSV file).\n",
    "- Results are saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e6f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defra_dataset_statistics(base_dir, metadata_path, nan_log_path):\n",
    "    \"\"\"\n",
    "    Calculate statistics at DEFRA dataset.\n",
    "    This function walks through the monthly data directories 2023, 2024, 2025 and calculates key metrics needed for reporting.\n",
    "    \n",
    "    Parameters:\n",
    "        base_dir : Path\n",
    "            Base directory containing defra data folders.\n",
    "        metadata_path : Path\n",
    "            Path to the standardised metadata csv file.\n",
    "        nan_log_path : Path\n",
    "            Path to the NaN values log file after notice data flags, changed them to NaN.\n",
    "            \n",
    "    Returns:\n",
    "        dict : Dictionary containing all calculated statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # read metadata to get station and pollutant info\n",
    "    print(\"\\nloading metadata from std_london_sites_pollutant.csv...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # calculate metadata statistics\n",
    "    stats['unique_stations'] = metadata['station_name'].nunique()\n",
    "    stats['total_combinations'] = len(metadata)\n",
    "    stats['unique_pollutants'] = metadata['pollutant_std'].nunique()\n",
    "    \n",
    "    # get pollutant breakdown\n",
    "    pollutant_counts = metadata['pollutant_std'].value_counts()\n",
    "    stats['pollutant_distribution'] = pollutant_counts.to_dict()\n",
    "    \n",
    "    # create set of expected station/pollutant pairs from metadata\n",
    "    expected_pairs = set(\n",
    "        zip(metadata['station_name'], metadata['pollutant_std'])\n",
    "    )\n",
    "    stats['expected_pairs'] = len(expected_pairs)\n",
    "    print(f\"  expected station/pollutant pairs from metadata: {len(expected_pairs)}\")\n",
    "    \n",
    "    # count unique coordinates for spatial coverage, i will be use this for laqn dataset as well\n",
    "    # group by lat/lon and count unique locations, instead of station names and will do the validation afterwards\n",
    "    unique_coords = metadata[['latitude', 'longitude']].drop_duplicates()\n",
    "    stats['unique_locations'] = len(unique_coords)\n",
    "    \n",
    "    # count files in monthly data directories\n",
    "    total_files = 0\n",
    "    files_by_year = {}\n",
    "    \n",
    "    # loop through each years measurement directory\n",
    "    print(\"\\nscanning optimised directory for collected data...\")\n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        \n",
    "        if year_dir.exists():\n",
    "            # count all CSV files in this years directory and subdirectories\n",
    "            year_files = list(year_dir.rglob('*.csv'))\n",
    "            files_by_year[year] = len(year_files)\n",
    "            total_files += len(year_files)\n",
    "            print(f\"  {year}: {len(year_files)} files\")\n",
    "        else:\n",
    "            files_by_year[year] = 0\n",
    "            print(f\"  {year}: directory not found\")\n",
    "    \n",
    "    stats['total_files'] = total_files\n",
    "    stats['files_by_year'] = files_by_year\n",
    "    \n",
    "    # calculate total measurement records, this requires reading all csv files and counting rows\n",
    "    total_records = 0\n",
    "    records_by_year = {}\n",
    "    total_missing = 0\n",
    "    missing_by_year = {}\n",
    "    \n",
    "    # concatenate all CSVs for missing value breakdown\n",
    "    all_csvs = []\n",
    "    \n",
    "    print(\"\\nreading all CSV files to calculate statistics...\")\n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        year_records = 0\n",
    "        year_missing = 0\n",
    "        \n",
    "        if year_dir.exists():\n",
    "            # read each csv, count rows and missing values\n",
    "            for csv_file in year_dir.rglob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    year_records += len(df)\n",
    "                    \n",
    "                    # count missing NaN or empty string values in value column\n",
    "                    # calculation: missing values in value column only\n",
    "                    if 'value' in df.columns:\n",
    "                        missing_in_file = df['value'].isna().sum() + (df['value'] == \"\").sum()\n",
    "                        year_missing += missing_in_file\n",
    "                    \n",
    "                    # store dataframe for later aggregation\n",
    "                    all_csvs.append(df)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  warning: could not read {csv_file.name}: {e}\")\n",
    "            \n",
    "            records_by_year[year] = year_records\n",
    "            missing_by_year[year] = year_missing\n",
    "            total_records += year_records\n",
    "            total_missing += year_missing\n",
    "            print(f\"  {year}: {year_records:,} records, {year_missing:,} missing ({(year_missing/year_records*100):.2f}%)\")\n",
    "        else:\n",
    "            records_by_year[year] = 0\n",
    "            missing_by_year[year] = 0\n",
    "    \n",
    "    stats['total_records'] = total_records\n",
    "    stats['records_by_year'] = records_by_year\n",
    "    stats['missing_by_year'] = missing_by_year\n",
    "    stats['total_missing'] = total_missing\n",
    "    stats['overall_completeness'] = ((total_records - total_missing) / total_records * 100) if total_records > 0 else 0\n",
    "    \n",
    "    # cross-reference metadata with collected data\n",
    "    print(\"\\ncross-referencing collected data with metadata...\")\n",
    "    \n",
    "    if all_csvs:\n",
    "        all_data = pd.concat(all_csvs, ignore_index=True)\n",
    "        \n",
    "        # check if required columns exist in csv files\n",
    "        # file structure: timestamp,value,timeseries_id,station_name,pollutant_name,pollutant_std,latitude,longitude\n",
    "        if 'station_name' in all_data.columns and 'pollutant_std' in all_data.columns:\n",
    "            # identify actual station/pollutant pairs in collected data\n",
    "            collected_pairs = set(\n",
    "                zip(all_data['station_name'], all_data['pollutant_std'])\n",
    "            )\n",
    "            stats['collected_pairs'] = len(collected_pairs)\n",
    "            \n",
    "            # find missing pairs (in metadata but not in collected data)\n",
    "            missing_pairs = expected_pairs - collected_pairs\n",
    "            stats['missing_pairs'] = list(missing_pairs)\n",
    "            stats['missing_pairs_count'] = len(missing_pairs)\n",
    "            \n",
    "            # find extra pairs (in collected data but not in metadata)\n",
    "            extra_pairs = collected_pairs - expected_pairs\n",
    "            stats['extra_pairs'] = list(extra_pairs)\n",
    "            stats['extra_pairs_count'] = len(extra_pairs)\n",
    "            \n",
    "            print(f\"  expected pairs from metadata: {len(expected_pairs)}\")\n",
    "            print(f\"  actually collected pairs: {len(collected_pairs)}\")\n",
    "            print(f\"  missing pairs (in metadata but not collected): {len(missing_pairs)}\")\n",
    "            print(f\"  extra pairs (collected but not in metadata): {len(extra_pairs)}\")\n",
    "            \n",
    "            # group by station and pollutant_std, count missing values\n",
    "            # calculation: (100 * missing value cell number) / (total number of row value col)\n",
    "            missing_breakdown = {}\n",
    "            \n",
    "            for (station, pollutant), group in all_data.groupby(['station_name', 'pollutant_std']):\n",
    "                total_rows = len(group)\n",
    "                # count missing in value column\n",
    "                if 'value' in group.columns:\n",
    "                    missing_rows = group['value'].isna().sum() + (group['value'] == \"\").sum()\n",
    "                else:\n",
    "                    missing_rows = 0\n",
    "                \n",
    "                missing_breakdown[(station, pollutant)] = (int(missing_rows), int(total_rows))\n",
    "            \n",
    "            stats['missing_by_station_pollutant'] = missing_breakdown\n",
    "        else:\n",
    "            print(\"  warning: station_name or pollutant_std columns not found\")\n",
    "            stats['missing_by_station_pollutant'] = {}\n",
    "            stats['collected_pairs'] = 0\n",
    "            stats['missing_pairs'] = []\n",
    "            stats['missing_pairs_count'] = 0\n",
    "            stats['extra_pairs'] = []\n",
    "            stats['extra_pairs_count'] = 0\n",
    "    else:\n",
    "        stats['missing_by_station_pollutant'] = {}\n",
    "        stats['collected_pairs'] = 0\n",
    "        stats['missing_pairs'] = list(expected_pairs)\n",
    "        stats['missing_pairs_count'] = len(expected_pairs)\n",
    "        stats['extra_pairs'] = []\n",
    "        stats['extra_pairs_count'] = 0\n",
    "    \n",
    "    # distribution of nan by pollutant over time\n",
    "    if stats['missing_by_station_pollutant']:\n",
    "        pollutant_missing_summary = {}\n",
    "        \n",
    "        for (station, pollutant), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            if pollutant not in pollutant_missing_summary:\n",
    "                pollutant_missing_summary[pollutant] = {'total_missing': 0, 'total_records': 0}\n",
    "            \n",
    "            pollutant_missing_summary[pollutant]['total_missing'] += missing\n",
    "            pollutant_missing_summary[pollutant]['total_records'] += total\n",
    "        \n",
    "        # calculate percentages\n",
    "        for pollutant in pollutant_missing_summary:\n",
    "            total_missing = pollutant_missing_summary[pollutant]['total_missing']\n",
    "            total_records = pollutant_missing_summary[pollutant]['total_records']\n",
    "            percentage = (total_missing / total_records * 100) if total_records > 0 else 0\n",
    "            pollutant_missing_summary[pollutant]['percentage_missing'] = percentage\n",
    "        \n",
    "        stats['missing_by_pollutant_type'] = pollutant_missing_summary\n",
    "    else:\n",
    "        stats['missing_by_pollutant_type'] = {}\n",
    "    \n",
    "    # log file created during data cleaning process\n",
    "    if Path(nan_log_path).exists():\n",
    "        nan_log = pd.read_csv(nan_log_path)\n",
    "        \n",
    "        # calculate replacement statistics per year\n",
    "        replacements_by_year = nan_log.groupby('year_folder')['invalid_flags_replaced'].sum().to_dict()\n",
    "        stats['nan_replacements_by_year'] = replacements_by_year\n",
    "        stats['total_nan_replacements'] = nan_log['invalid_flags_replaced'].sum()\n",
    "        \n",
    "        # get mean percentage of invalid flags\n",
    "        stats['mean_invalid_percentage'] = nan_log['percentage_invalid'].mean()\n",
    "        stats['max_invalid_percentage'] = nan_log['percentage_invalid'].max()\n",
    "        \n",
    "    else:\n",
    "        stats['nan_replacements_by_year'] = {}\n",
    "        stats['total_nan_replacements'] = 0\n",
    "        stats['mean_invalid_percentage'] = 0\n",
    "        stats['max_invalid_percentage'] = 0\n",
    "    \n",
    "    # calculate temporal coverage based on the files collected, understands which months have data\n",
    "    stats['temporal_coverage'] = {\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2025-11-19',  \n",
    "        'total_months': 35\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df5aaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(stats):\n",
    "    \"\"\"\n",
    "    Print dataset statistics\n",
    "    \n",
    "    Parameters:\n",
    "        stats : dict\n",
    "            returned by get_defra_dataset_statistics().\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Defra dataset statistics: initial assessment\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"\\nScale and scope:\")\n",
    "    print(f\"Total files collected: {stats['total_files']:,}\")\n",
    "    print(f\"Total measurement records: {stats['total_records']:,}\")\n",
    "    print(f\"Total missing values (nan): {stats['total_missing']:,}\")\n",
    "    print(f\"Overall completeness: {stats['overall_completeness']:.2f}%\")\n",
    "    print(f\"Unique monitoring stations: {stats['unique_stations']}\")\n",
    "    print(f\"Total station-pollutant combinations: {stats['total_combinations']}\")\n",
    "    print(f\"Unique pollutant types: {stats['unique_pollutants']}\")\n",
    "    print(f\"Unique geographic locations: {stats['unique_locations']}\")\n",
    "    \n",
    "    # data collection coverage\n",
    "    print(\"\\nData collection coverage:\")\n",
    "    print(f\"Expected pairs (from metadata): {stats.get('expected_pairs', 0)}\")\n",
    "    print(f\"Actually collected pairs: {stats.get('collected_pairs', 0)}\")\n",
    "    print(f\"Missing pairs (not collected): {stats.get('missing_pairs_count', 0)}\")\n",
    "    print(f\"Extra pairs (not in metadata): {stats.get('extra_pairs_count', 0)}\")\n",
    "    \n",
    "    if stats.get('missing_pairs_count', 0) > 0:\n",
    "        print(f\"\\nwarning: {stats['missing_pairs_count']} station/pollutant pairs from metadata were not found in collected data.\")\n",
    "        print(\"first 10 missing pairs:\")\n",
    "        for i, (station, pollutant) in enumerate(stats['missing_pairs'][:10], 1):\n",
    "            print(f\"  {i}. {station} - {pollutant}\")\n",
    "    \n",
    "    if stats.get('extra_pairs_count', 0) > 0:\n",
    "        print(f\"\\nNote: {stats['extra_pairs_count']} station/pollutant pairs in collected data are not in metadata.\")\n",
    "    \n",
    "    print(\"\\nfiles by year:\")\n",
    "    for year, count in stats['files_by_year'].items():\n",
    "        print(f\"  {year}: {count:,} files\")\n",
    "    \n",
    "    print(\"\\nrecords by year:\")\n",
    "    for year, count in stats['records_by_year'].items():\n",
    "        missing = stats['missing_by_year'].get(year, 0)\n",
    "        missing_pct = (missing / count * 100) if count > 0 else 0\n",
    "        print(f\"  {year}: {count:,} records, {missing:,} missing ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # adding nan value summary below\n",
    "    print(\"\\nnan replacement summary:\")\n",
    "    print(f\"Total invalid flags replaced: {stats['total_nan_replacements']:,}\")\n",
    "    print(f\"Mean invalid percentage per file: {stats['mean_invalid_percentage']:.2f}%\")\n",
    "    print(f\"Max invalid percentage: {stats['max_invalid_percentage']:.2f}%\")\n",
    "    \n",
    "    # count of replacements by year\n",
    "    if stats['nan_replacements_by_year']:\n",
    "        print(\"\\nreplacements by year:\")\n",
    "        for year_folder, count in stats['nan_replacements_by_year'].items():\n",
    "            print(f\"  {year_folder}: {count:,} flags replaced\")\n",
    "    \n",
    "    print(\"\\ntemporal coverage:\")\n",
    "    print(f\"start date: {stats['temporal_coverage']['start_date']}\")\n",
    "    print(f\"end date: {stats['temporal_coverage']['end_date']}\")\n",
    "    print(f\"total months: {stats['temporal_coverage']['total_months']}\")\n",
    "    \n",
    "    print(\"\\npollutant distribution:\")\n",
    "    print(\"station/pollutant combinations by type:\")\n",
    "    for pollutant, count in sorted(stats['pollutant_distribution'].items(), \n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_combinations']) * 100\n",
    "        print(f\"  {pollutant}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # missing value distribution by pollutant type\n",
    "    print(\"\\nMissing value distribution by pollutant type:\")\n",
    "    if stats.get('missing_by_pollutant_type'):\n",
    "        # sort by percentage missing (highest first)\n",
    "        sorted_pollutants = sorted(\n",
    "            stats['missing_by_pollutant_type'].items(),\n",
    "            key=lambda x: x[1]['percentage_missing'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(f\"{'pollutant':<20} {'total records':>15} {'missing':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for pollutant, data in sorted_pollutants:\n",
    "            print(f\"{pollutant:<20} {data['total_records']:>15,} {data['total_missing']:>12,} {data['percentage_missing']:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  no missing value distribution available.\")\n",
    "    \n",
    "    # print missing values by station/pollutant breakdown with row_number column\n",
    "    print(\"\\nMissing values by station/pollutant:\")\n",
    "    if stats.get('missing_by_station_pollutant'):\n",
    "        # prepare a sorted list by missing percentage descending\n",
    "        breakdown = []\n",
    "        for (station, pollutant), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            percent = (missing / total * 100) if total > 0 else 0\n",
    "            breakdown.append((station, pollutant, missing, total, percent))\n",
    "        # sort by percentage descending and take top 20\n",
    "        breakdown.sort(key=lambda x: x[4], reverse=True)\n",
    "        breakdown = breakdown[:20]\n",
    "        print(f\"{'station':<30} {'pollutant':<20} {'missing':>10} {'total_row':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for station, pollutant, missing, total, percent in breakdown:\n",
    "            print(f\"{station:<30} {pollutant:<20} {missing:>10,} {total:>12,} {percent:>11.2f}%\")\n",
    "    else:\n",
    "        print(\" No missing value breakdown available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading metadata from std_london_sites_pollutant.csv...\n",
      "  expected station/pollutant pairs from metadata: 141\n",
      "\n",
      "scanning optimised directory for collected data...\n",
      "  2023: 1431 files\n",
      "  2024: 1193 files\n",
      "  2025: 939 files\n",
      "\n",
      "reading all CSV files to calculate statistics...\n",
      "  2023: 1,000,126 records, 90,161 missing (9.01%)\n",
      "  2024: 868,320 records, 101,256 missing (11.66%)\n",
      "  2025: 657,545 records, 30,750 missing (4.68%)\n",
      "\n",
      "cross-referencing collected data with metadata...\n",
      "  expected pairs from metadata: 141\n",
      "  actually collected pairs: 141\n",
      "  missing pairs (in metadata but not collected): 0\n",
      "  extra pairs (collected but not in metadata): 0\n",
      "\n",
      "========================================\n",
      "Defra dataset statistics: initial assessment\n",
      "========================================\n",
      "\n",
      "Scale and scope:\n",
      "Total files collected: 3,563\n",
      "Total measurement records: 2,525,991\n",
      "Total missing values (nan): 222,167\n",
      "Overall completeness: 91.20%\n",
      "Unique monitoring stations: 18\n",
      "Total station-pollutant combinations: 144\n",
      "Unique pollutant types: 37\n",
      "Unique geographic locations: 20\n",
      "\n",
      "Data collection coverage:\n",
      "Expected pairs (from metadata): 141\n",
      "Actually collected pairs: 141\n",
      "Missing pairs (not collected): 0\n",
      "Extra pairs (not in metadata): 0\n",
      "\n",
      "files by year:\n",
      "  2023: 1,431 files\n",
      "  2024: 1,193 files\n",
      "  2025: 939 files\n",
      "\n",
      "records by year:\n",
      "  2023: 1,000,126 records, 90,161 missing (9.01%)\n",
      "  2024: 868,320 records, 101,256 missing (11.66%)\n",
      "  2025: 657,545 records, 30,750 missing (4.68%)\n",
      "\n",
      "nan replacement summary:\n",
      "Total invalid flags replaced: 222,167\n",
      "Mean invalid percentage per file: 9.61%\n",
      "Max invalid percentage: 100.00%\n",
      "\n",
      "replacements by year:\n",
      "  2023measurements: 90,161 flags replaced\n",
      "  2024measurements: 101,256 flags replaced\n",
      "  2025measurements: 30,750 flags replaced\n",
      "\n",
      "temporal coverage:\n",
      "start date: 2023-01-01\n",
      "end date: 2025-11-19\n",
      "total months: 35\n",
      "\n",
      "pollutant distribution:\n",
      "station/pollutant combinations by type:\n",
      "  PM10: 15 (10.4%)\n",
      "  PM2.5: 15 (10.4%)\n",
      "  NO2: 14 (9.7%)\n",
      "  NOx: 14 (9.7%)\n",
      "  NO: 14 (9.7%)\n",
      "  O3: 9 (6.2%)\n",
      "  SO2: 3 (2.1%)\n",
      "  n-Pentane: 2 (1.4%)\n",
      "  m,p-Xylene: 2 (1.4%)\n",
      "  n-Butane: 2 (1.4%)\n",
      "  n-Heptane: 2 (1.4%)\n",
      "  n-Hexane: 2 (1.4%)\n",
      "  n-Octane: 2 (1.4%)\n",
      "  Propene: 2 (1.4%)\n",
      "  o-Xylene: 2 (1.4%)\n",
      "  Propane: 2 (1.4%)\n",
      "  i-Pentane: 2 (1.4%)\n",
      "  Toluene: 2 (1.4%)\n",
      "  trans-2-Butene: 2 (1.4%)\n",
      "  trans-2-Pentene: 2 (1.4%)\n",
      "  Isoprene: 2 (1.4%)\n",
      "  Ethyne: 2 (1.4%)\n",
      "  i-Octane: 2 (1.4%)\n",
      "  i-Hexane: 2 (1.4%)\n",
      "  i-Butane: 2 (1.4%)\n",
      "  Ethylbenzene: 2 (1.4%)\n",
      "  Ethene: 2 (1.4%)\n",
      "  Ethane: 2 (1.4%)\n",
      "  cis-2-Butene: 2 (1.4%)\n",
      "  Benzene: 2 (1.4%)\n",
      "  1-Pentene: 2 (1.4%)\n",
      "  1-Butene: 2 (1.4%)\n",
      "  1,3-Butadiene: 2 (1.4%)\n",
      "  1,3,5-TMB: 2 (1.4%)\n",
      "  1,2,4-TMB: 2 (1.4%)\n",
      "  1,2,3-TMB: 2 (1.4%)\n",
      "  CO: 2 (1.4%)\n",
      "\n",
      "Missing value distribution by pollutant type:\n",
      "pollutant              total records      missing    % missing\n",
      "------------------------------------------------------------\n",
      "PM10                         227,142       37,580       16.54%\n",
      "O3                           194,333       27,184       13.99%\n",
      "PM2.5                        234,748       29,623       12.62%\n",
      "SO2                           72,928        7,181        9.85%\n",
      "NO                           326,061       25,444        7.80%\n",
      "NO2                          326,072       25,429        7.80%\n",
      "NOx                          325,387       24,964        7.67%\n",
      "n-Octane                      26,649        1,764        6.62%\n",
      "CO                            48,578        3,078        6.34%\n",
      "m,p-Xylene                    25,503        1,612        6.32%\n",
      "1,3,5-TMB                     26,649        1,641        6.16%\n",
      "Toluene                       26,649        1,640        6.15%\n",
      "i-Octane                      26,649        1,624        6.09%\n",
      "n-Heptane                     26,649        1,622        6.09%\n",
      "1,2,4-TMB                     26,649        1,610        6.04%\n",
      "Ethylbenzene                  26,649        1,592        5.97%\n",
      "Benzene                       26,649        1,586        5.95%\n",
      "o-Xylene                      26,649        1,568        5.88%\n",
      "1,2,3-TMB                     26,649        1,560        5.85%\n",
      "1-Pentene                     26,572        1,381        5.20%\n",
      "cis-2-Butene                  26,599        1,378        5.18%\n",
      "trans-2-Pentene               26,599        1,366        5.14%\n",
      "Isoprene                      26,618        1,341        5.04%\n",
      "Ethyne                        26,529        1,328        5.01%\n",
      "1,3-Butadiene                 26,568        1,320        4.97%\n",
      "i-Hexane                      26,599        1,321        4.97%\n",
      "trans-2-Butene                26,599        1,321        4.97%\n",
      "n-Hexane                      26,580        1,320        4.97%\n",
      "Propane                       26,618        1,316        4.94%\n",
      "Ethane                        26,599        1,315        4.94%\n",
      "Ethene                        26,618        1,312        4.93%\n",
      "Propene                       26,618        1,312        4.93%\n",
      "i-Butane                      26,599        1,308        4.92%\n",
      "1-Butene                      26,599        1,307        4.91%\n",
      "n-Butane                      26,599        1,307        4.91%\n",
      "i-Pentane                     26,618        1,306        4.91%\n",
      "n-Pentane                     26,618        1,306        4.91%\n",
      "\n",
      "Missing values by station/pollutant:\n",
      "station                        pollutant               missing    total_row    % missing\n",
      "----------------------------------------\n",
      "London Eltham                  PM10                     16,337       16,826       97.09%\n",
      "London Eltham                  NO2                      13,187       16,840       78.31%\n",
      "London Eltham                  NO                       13,182       16,835       78.30%\n",
      "London Eltham                  NOx                      13,125       16,793       78.16%\n",
      "London Eltham                  O3                       12,537       16,842       74.44%\n",
      "London Teddington Bushy Park   PM10                     10,525       24,327       43.26%\n",
      "London Teddington Bushy Park   PM2.5                    20,820       48,656       42.79%\n",
      "London Haringey Priory Park South O3                        8,171       24,288       33.64%\n",
      "London Marylebone Road         PM10                        632        2,355       26.84%\n",
      "London Marylebone Road         PM2.5                       479        2,355       20.34%\n",
      "London Norbury Manor School    PM10                        936        5,258       17.80%\n",
      "London Norbury Manor School    PM2.5                       936        5,258       17.80%\n",
      "London Bexley                  PM10                      4,012       24,273       16.53%\n",
      "Southwark A2 Old Kent Road     PM10                        388        2,355       16.48%\n",
      "Haringey Roadside              NOx                       3,725       24,250       15.36%\n",
      "Haringey Roadside              NO2                       3,708       24,285       15.27%\n",
      "Haringey Roadside              NO                        3,708       24,287       15.27%\n",
      "London Westminster             PM2.5                     3,463       24,299       14.25%\n",
      "London Marylebone Road         SO2                       2,987       24,290       12.30%\n",
      "London Marylebone Road         CO                        2,729       24,293       11.23%\n",
      "Pollutant distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/optimised/report/pollutant_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# run the analysis\n",
    "stats = get_defra_dataset_statistics(base_dir, metadata_path, nan_log_path)\n",
    "print_dataset_statistics(stats)\n",
    "\n",
    "# # Save statistics for later use as csv\n",
    "# save statistics for later use as csv\n",
    "# prepare flat data structure for csv\n",
    "stats_rows = []\n",
    "stats_rows.append([\"metric\", \"value\"])\n",
    "stats_rows.append([\"total_files\", stats['total_files']])\n",
    "stats_rows.append([\"total_records\", stats['total_records']])\n",
    "stats_rows.append([\"total_missing\", stats['total_missing']])\n",
    "stats_rows.append([\"overall_completeness_pct\", f\"{stats['overall_completeness']:.2f}\"])\n",
    "stats_rows.append([\"unique_stations\", stats['unique_stations']])\n",
    "stats_rows.append([\"total_combinations\", stats['total_combinations']])\n",
    "stats_rows.append([\"unique_pollutants\", stats['unique_pollutants']])\n",
    "stats_rows.append([\"unique_locations\", stats['unique_locations']])\n",
    "stats_rows.append([\"expected_pairs\", stats.get('expected_pairs', 0)])\n",
    "stats_rows.append([\"collected_pairs\", stats.get('collected_pairs', 0)])\n",
    "stats_rows.append([\"missing_pairs_count\", stats.get('missing_pairs_count', 0)])\n",
    "stats_rows.append([\"extra_pairs_count\", stats.get('extra_pairs_count', 0)])\n",
    "stats_rows.append([\"total_nan_replacements\", stats['total_nan_replacements']])\n",
    "stats_rows.append([\"mean_invalid_pct\", f\"{stats['mean_invalid_percentage']:.2f}\"])\n",
    "stats_rows.append([\"max_invalid_pct\", f\"{stats['max_invalid_percentage']:.2f}\"])\n",
    "\n",
    "# add year-specific metrics\n",
    "for year in ['2023', '2024', '2025']:\n",
    "    stats_rows.append([f\"files_{year}\", stats['files_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"records_{year}\", stats['records_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"missing_{year}\", stats['missing_by_year'].get(year, 0)])\n",
    "    year_key = f'{year}measurements'\n",
    "    stats_rows.append([f\"replacements_{year}\", stats['nan_replacements_by_year'].get(year_key, 0)])\n",
    "\n",
    "# # save to csv stats report save func below (commented out for now to overwrite previous report)\n",
    "# pd.DataFrame(stats_rows[1:], columns=stats_rows[0]).to_csv(stats_output_path, index=False)\n",
    "# print(f\"\\nstatistics saved to: {stats_output_path}\")\n",
    "\n",
    "# # save pollutant distribution to csv describe the path on top pollutant_distrubution_path\n",
    "# total_combinations = stats['total_combinations']\n",
    "# pollutant_distribution_df = pd.DataFrame(\n",
    "#     [\n",
    "#         {\n",
    "#             'pollutant': k,\n",
    "#             'count': v,\n",
    "#             'percentage': round((v / total_combinations) * 100, 2) if total_combinations > 0 else 0\n",
    "#         }\n",
    "#         for k, v in stats['pollutant_distribution'].items()\n",
    "#     ]\n",
    "# )\n",
    "# pollutant_distribution_df.to_csv(pollutant_distrubution_path, index=False)\n",
    "# print(f\"Pollutant distribution saved to: {pollutant_distrubution_path}\")\n",
    "\n",
    "# # Save missing value distribution by pollutant type to path described the path on top nan_val_pollutant_split_path\n",
    "# if stats.get('missing_by_pollutant_type'):\n",
    "#     missing_by_pollutant_df = pd.DataFrame([\n",
    "#         {\n",
    "#             'pollutant': k,\n",
    "#             'total_records': v['total_records'],\n",
    "#             'total_missing': v['total_missing'],\n",
    "#             'percentage_missing': v['percentage_missing']\n",
    "#         }\n",
    "#         for k, v in stats['missing_by_pollutant_type'].items()\n",
    "#     ])\n",
    "#     missing_by_pollutant_df.to_csv(nan_val_pollutant_split_path, index=False)\n",
    "#     print(f\"Missing value distribution by pollutant type saved to: {nan_val_pollutant_split_path}\")\n",
    "\n",
    "# # save missing values by station/pollutant to csv path on top nan_val_stationPollutant_path\n",
    "# if stats.get('missing_by_station_pollutant'):\n",
    "#     missing_by_station_pollutant_df = pd.DataFrame([\n",
    "#         {\n",
    "#             'station': k[0],\n",
    "#             'pollutant': k[1],\n",
    "#             'missing': v[0],\n",
    "#             'total_row': v[1],\n",
    "#             'percentage_missing': (v[0] / v[1] * 100) if v[1] > 0 else 0\n",
    "#         }\n",
    "#         for k, v in stats['missing_by_station_pollutant'].items()\n",
    "#     ])\n",
    "#     missing_by_station_pollutant_df.to_csv(nan_val_stationPollutant_path, index=False)\n",
    "#     print(f\"Missing values by station/pollutant saved to: {nan_val_stationPollutant_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f816ef",
   "metadata": {},
   "source": [
    "    loading metadata from std_london_sites_pollutant.csv...\n",
    "    expected station/pollutant pairs from metadata: 141\n",
    "\n",
    "    scanning optimised directory for collected data...\n",
    "    2023: 1431 files\n",
    "    2024: 1193 files\n",
    "    2025: 939 files\n",
    "\n",
    "    reading all CSV files to calculate statistics...\n",
    "    2023: 1,000,126 records, 90,161 missing (9.01%)\n",
    "    2024: 868,320 records, 101,256 missing (11.66%)\n",
    "    2025: 657,545 records, 30,750 missing (4.68%)\n",
    "\n",
    "    cross-referencing collected data with metadata...\n",
    "    expected pairs from metadata: 141\n",
    "    actually collected pairs: 141\n",
    "    missing pairs (in metadata but not collected): 0\n",
    "    extra pairs (collected but not in metadata): 0\n",
    "\n",
    "    ========================================\n",
    "    Defra dataset statistics: initial assessment\n",
    "    ========================================\n",
    "\n",
    "    Scale and scope:\n",
    "    Total files collected: 3,563\n",
    "    Total measurement records: 2,525,991\n",
    "    Total missing values (nan): 222,167\n",
    "    Overall completeness: 91.20%\n",
    "    Unique monitoring stations: 18\n",
    "    Total station-pollutant combinations: 144\n",
    "    Unique pollutant types: 37\n",
    "    Unique geographic locations: 20\n",
    "\n",
    "    Data collection coverage:\n",
    "    Expected pairs (from metadata): 141\n",
    "    Actually collected pairs: 141\n",
    "    Missing pairs (not collected): 0\n",
    "    Extra pairs (not in metadata): 0\n",
    "\n",
    "    files by year:\n",
    "    2023: 1,431 files\n",
    "    2024: 1,193 files\n",
    "    2025: 939 files\n",
    "\n",
    "    records by year:\n",
    "    2023: 1,000,126 records, 90,161 missing (9.01%)\n",
    "    2024: 868,320 records, 101,256 missing (11.66%)\n",
    "    2025: 657,545 records, 30,750 missing (4.68%)\n",
    "\n",
    "    nan replacement summary:\n",
    "    Total invalid flags replaced: 222,167\n",
    "    Mean invalid percentage per file: 9.61%\n",
    "    Max invalid percentage: 100.00%\n",
    "\n",
    "    replacements by year:\n",
    "    2023measurements: 90,161 flags replaced\n",
    "    2024measurements: 101,256 flags replaced\n",
    "    2025measurements: 30,750 flags replaced\n",
    "\n",
    "    temporal coverage:\n",
    "    start date: 2023-01-01\n",
    "    end date: 2025-11-19\n",
    "    total months: 35\n",
    "\n",
    "    pollutant distribution:\n",
    "    station/pollutant combinations by type:\n",
    "    PM10: 15 (10.4%)\n",
    "    PM2.5: 15 (10.4%)\n",
    "    NO2: 14 (9.7%)\n",
    "    NOx: 14 (9.7%)\n",
    "    NO: 14 (9.7%)\n",
    "    O3: 9 (6.2%)\n",
    "    SO2: 3 (2.1%)\n",
    "    n-Pentane: 2 (1.4%)\n",
    "    m,p-Xylene: 2 (1.4%)\n",
    "    n-Butane: 2 (1.4%)\n",
    "    n-Heptane: 2 (1.4%)\n",
    "    n-Hexane: 2 (1.4%)\n",
    "    n-Octane: 2 (1.4%)\n",
    "    Propene: 2 (1.4%)\n",
    "    o-Xylene: 2 (1.4%)\n",
    "    Propane: 2 (1.4%)\n",
    "    i-Pentane: 2 (1.4%)\n",
    "    Toluene: 2 (1.4%)\n",
    "    trans-2-Butene: 2 (1.4%)\n",
    "    trans-2-Pentene: 2 (1.4%)\n",
    "    Isoprene: 2 (1.4%)\n",
    "    Ethyne: 2 (1.4%)\n",
    "    i-Octane: 2 (1.4%)\n",
    "    i-Hexane: 2 (1.4%)\n",
    "    i-Butane: 2 (1.4%)\n",
    "    Ethylbenzene: 2 (1.4%)\n",
    "    Ethene: 2 (1.4%)\n",
    "    Ethane: 2 (1.4%)\n",
    "    cis-2-Butene: 2 (1.4%)\n",
    "    Benzene: 2 (1.4%)\n",
    "    1-Pentene: 2 (1.4%)\n",
    "    1-Butene: 2 (1.4%)\n",
    "    1,3-Butadiene: 2 (1.4%)\n",
    "    1,3,5-TMB: 2 (1.4%)\n",
    "    1,2,4-TMB: 2 (1.4%)\n",
    "    1,2,3-TMB: 2 (1.4%)\n",
    "    CO: 2 (1.4%)\n",
    "\n",
    "    Missing value distribution by pollutant type:\n",
    "    pollutant              total records      missing    % missing\n",
    "    ------------------------------------------------------------\n",
    "    PM10                         227,142       37,580       16.54%\n",
    "    O3                           194,333       27,184       13.99%\n",
    "    PM2.5                        234,748       29,623       12.62%\n",
    "    SO2                           72,928        7,181        9.85%\n",
    "    NO                           326,061       25,444        7.80%\n",
    "    NO2                          326,072       25,429        7.80%\n",
    "    NOx                          325,387       24,964        7.67%\n",
    "    n-Octane                      26,649        1,764        6.62%\n",
    "    CO                            48,578        3,078        6.34%\n",
    "    m,p-Xylene                    25,503        1,612        6.32%\n",
    "    1,3,5-TMB                     26,649        1,641        6.16%\n",
    "    Toluene                       26,649        1,640        6.15%\n",
    "    i-Octane                      26,649        1,624        6.09%\n",
    "    n-Heptane                     26,649        1,622        6.09%\n",
    "    1,2,4-TMB                     26,649        1,610        6.04%\n",
    "    Ethylbenzene                  26,649        1,592        5.97%\n",
    "    Benzene                       26,649        1,586        5.95%\n",
    "    o-Xylene                      26,649        1,568        5.88%\n",
    "    1,2,3-TMB                     26,649        1,560        5.85%\n",
    "    1-Pentene                     26,572        1,381        5.20%\n",
    "    cis-2-Butene                  26,599        1,378        5.18%\n",
    "    trans-2-Pentene               26,599        1,366        5.14%\n",
    "    Isoprene                      26,618        1,341        5.04%\n",
    "    Ethyne                        26,529        1,328        5.01%\n",
    "    1,3-Butadiene                 26,568        1,320        4.97%\n",
    "    i-Hexane                      26,599        1,321        4.97%\n",
    "    trans-2-Butene                26,599        1,321        4.97%\n",
    "    n-Hexane                      26,580        1,320        4.97%\n",
    "    Propane                       26,618        1,316        4.94%\n",
    "    Ethane                        26,599        1,315        4.94%\n",
    "    Ethene                        26,618        1,312        4.93%\n",
    "    Propene                       26,618        1,312        4.93%\n",
    "    i-Butane                      26,599        1,308        4.92%\n",
    "    1-Butene                      26,599        1,307        4.91%\n",
    "    n-Butane                      26,599        1,307        4.91%\n",
    "    i-Pentane                     26,618        1,306        4.91%\n",
    "    n-Pentane                     26,618        1,306        4.91%\n",
    "\n",
    "    Missing values by station/pollutant:\n",
    "    station                        pollutant               missing    total_row    % missing\n",
    "    ----------------------------------------\n",
    "    London Eltham                  PM10                     16,337       16,826       97.09%\n",
    "    London Eltham                  NO2                      13,187       16,840       78.31%\n",
    "    London Eltham                  NO                       13,182       16,835       78.30%\n",
    "    London Eltham                  NOx                      13,125       16,793       78.16%\n",
    "    London Eltham                  O3                       12,537       16,842       74.44%\n",
    "    London Teddington Bushy Park   PM10                     10,525       24,327       43.26%\n",
    "    London Teddington Bushy Park   PM2.5                    20,820       48,656       42.79%\n",
    "    London Haringey Priory Park South O3                        8,171       24,288       33.64%\n",
    "    London Marylebone Road         PM10                        632        2,355       26.84%\n",
    "    London Marylebone Road         PM2.5                       479        2,355       20.34%\n",
    "    London Norbury Manor School    PM10                        936        5,258       17.80%\n",
    "    London Norbury Manor School    PM2.5                       936        5,258       17.80%\n",
    "    London Bexley                  PM10                      4,012       24,273       16.53%\n",
    "    Southwark A2 Old Kent Road     PM10                        388        2,355       16.48%\n",
    "    Haringey Roadside              NOx                       3,725       24,250       15.36%\n",
    "    Haringey Roadside              NO2                       3,708       24,285       15.27%\n",
    "    Haringey Roadside              NO                        3,708       24,287       15.27%\n",
    "    London Westminster             PM2.5                     3,463       24,299       14.25%\n",
    "    London Marylebone Road         SO2                       2,987       24,290       12.30%\n",
    "    London Marylebone Road         CO                        2,729       24,293       11.23%\n",
    "    Pollutant distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/optimised/report/pollutant_distribution.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97ae28",
   "metadata": {},
   "source": [
    "## 2) Spatial Coverage Analysis\n",
    "\n",
    " analysing spatial distribution patterns before accepting the dataset. I need to understand where defra stations are located, identify any geographic biases, and compare coverage to laqn.\n",
    "\n",
    "### Purpose\n",
    "- Create maps showing station locations across London.\n",
    "- Analyse density by borough to identify coverage gaps\n",
    "- Compare spatial distribution to laqn network\n",
    "- Ensure no geographic areas are overrepresented or underrepresented\n",
    "\n",
    "### Methodology\n",
    "1. Load defra metadata with coordinates\n",
    "2. Create interactive folium map showing all stations\n",
    "3. Calculate station density by borough\n",
    "4. Identify coverage gaps in london\n",
    "5. Compare to laqn spatial distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364fa78",
   "metadata": {},
   "source": [
    "sources: \n",
    "- https://python-visualization.github.io/folium/latest/getting_started.html\n",
    "- https://pandas.pydata.org/docs/user_guide/groupby.html \n",
    "- plotting: https://geopandas.org/en/stable/docs/user_guide/data_structures.html#geoseries\n",
    "    - general: https://geopandas.org/en/stable/getting_started.html\n",
    "\n",
    "coordinates:\n",
    "  -  https://www.ordnancesurvey.co.uk/\n",
    "  - identifiers: https://www.ordnancesurvey.co.uk/products/search-for-os-products?category=387aa470-8f46-4b02-a4ea-b70d1835f812 \n",
    "  - WGS84 coordinate system used for latitude/longitude.\n",
    "  - london coordinates : 51.5072° N, 0.1276° W\n",
    "  - Latitude and longitude coordinates are: 51.509865, -0.118092."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_spatial_coverage(metadata_path):\n",
    "    \"\"\"\n",
    "    analyse the stations location on map \n",
    "    \n",
    "    function validates coordinates, identifies  locations, and visulise the spatial distribution\n",
    "    \n",
    "    Parameters:\n",
    "        metadata_path : \n",
    "             std metadata csv file.\n",
    "            \n",
    "    Returns:\n",
    "        dictionary containing spatial statistics and coordinate data.\n",
    "    \n",
    "        *i got help for this section, sources folium tuttorials, plotting for geopandas and google. Also asked for my friend help as well which\n",
    "        she works on geospatial data a lot for her phd research.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    spatial_stats = {}\n",
    "    \n",
    "    # read metadata for coordinate information\n",
    "    print(\"\\nloading station coordinates from metadata...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # check if coordinate columns exist\n",
    "    if 'latitude' not in metadata.columns or 'longitude' not in metadata.columns:\n",
    "        print(\"  error: latitude or longitude columns not found in metadata\")\n",
    "        return spatial_stats\n",
    "    \n",
    "    # validate coordinate completeness\n",
    "    total_stations = len(metadata)\n",
    "    missing_lat = metadata['latitude'].isna().sum()\n",
    "    missing_lon = metadata['longitude'].isna().sum()\n",
    "    missing_coords = metadata[['latitude', 'longitude']].isna().any(axis=1).sum()\n",
    "    \n",
    "    spatial_stats['total_stations'] = total_stations\n",
    "    spatial_stats['missing_coordinates'] = missing_coords\n",
    "    spatial_stats['missing_latitude'] = missing_lat\n",
    "    spatial_stats['missing_longitude'] = missing_lon\n",
    "    spatial_stats['coordinate_completeness'] = ((total_stations - missing_coords) / total_stations * 100) if total_stations > 0 else 0\n",
    "    \n",
    "    print(f\"  total stations in metadata: {total_stations}\")\n",
    "    print(f\"  missing coordinates: {missing_coords} ({(missing_coords/total_stations*100):.2f}%)\")\n",
    "    print(f\"  coordinate completeness: {spatial_stats['coordinate_completeness']:.2f}%\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spatial_statistics(spatial_stats):\n",
    "    \"\"\"\n",
    "    Print spatial coverage statistics \n",
    "    \n",
    "    Param:\n",
    "        spatial_stats : \n",
    "            Dic returned by analyse_spatial_coverage()\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b68d2",
   "metadata": {},
   "source": [
    "## 3) Data Quality validations:\n",
    "\n",
    "\n",
    "A critical gap from the laqn report by applying formal statistical tests to validate data quality patterns. While descriptive statistics show 0% (before I notice the flags of the dataset) issue rate, I need statistical evidence that this pattern is real and not due to chance.\n",
    "\n",
    "\n",
    "#### Purpuse:\n",
    " Checking data qualities if it is in the limits of eea, and make sence for general logic.\n",
    "- Outlier detection in pollutant measurements.\n",
    "- Data validity ranges based on WHO/EEA standards.\n",
    "- Measurement consistency across time periods.\n",
    "- Quality flags and suspicious patterns.\n",
    "\n",
    "### methodology\n",
    " applies environmental data quality assessment standards:\n",
    "1. Load aggregated measurement data from all csv files.\n",
    "2. Calculate statistical distributions for each pollutant type.\n",
    "3. Identify outliers using IQR method and domain knowledge.\n",
    "4. Check values against established valid ranges.\n",
    "5. Flag suspicious patterns constant values, extreme spikes.\n",
    "6. Calculate quality scores for each station-pollutant combination.\n",
    "\n",
    "#### air quality measurement standards\n",
    "\n",
    "- Uk air quality objectives, limits and policy.\n",
    "- https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://uk-air.defra.gov.uk/assets/documents/Air_Quality_Objectives_Update_20230403.pdf\n",
    "\n",
    "- DEFRA. (2023). *Air Pollution in the UK 2022*.\n",
    "  - Source: https://uk-air.defra.gov.uk/library/annualreport/\n",
    "  - Air Quality Objectives and limit values\n",
    "  - Compliance assessment methodology\n",
    "\n",
    "- UK Air Information Resource. (2024). *Air Pollution: UK Limits*.\n",
    "  - Source: https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "  - Current UK air quality objectives\n",
    "  - Legal limit values and target dates\n",
    "  - Measurement unit specifications (µg/m³)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
