{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64359774",
   "metadata": {},
   "source": [
    "# DEFRA Dataset Assesment\n",
    "1) I'll be start adding my main paths and modules I will be using in this notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432092e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible python modules i will be using below\n",
    "from curses import meta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "#define base path  without hardcoding\n",
    "base_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"optimised\"\n",
    "#metadata file for pollutant name, location and site names\n",
    "metadata_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" /\"test\"/\"std_london_sites_pollutant.csv\"\n",
    "\n",
    "# output path for saving statistics 1. function\n",
    "stats_output_path = base_dir/ \"dataset_statistics.csv\"\n",
    "\n",
    "# log file from nan replacement process\n",
    "nan_log_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"logs\" / \"NaN_values_record.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f95bb4",
   "metadata": {},
   "source": [
    "## 1) Initial Dataset Assessment: Raw Numbers\n",
    "\n",
    "Before conducting quality checks, I need to establish the baseline characteristics of the DEFRA dataset. This section calculates comprehensive statistics about the data collection effort, including file counts, measurement records, station coverage, and pollutant distribution.\n",
    "\n",
    "### Purpose\n",
    "- Document the scale and scope of data collection.\n",
    "- Establish baseline metrics for comparison with LAQN.\n",
    "- Provide context for subsequent quality analysis.\n",
    "\n",
    "### Methodology\n",
    "The function `get_defra_dataset_statistics()` performs the following:\n",
    "1. Loads standardised metadata to identify unique stations and pollutants.\n",
    "2. Counts files across all three yearly directories (2023, 2024, 2025).\n",
    "3. Calculates total measurement records by reading all CSV files.\n",
    "4. Determines spatial coverage from unique coordinate pairs.\n",
    "5. Documents temporal coverage (35 months: January 2023 to November 2025).\n",
    "\n",
    "### Notes\n",
    "- File counting is fast (scans directory structure only).\n",
    "- Record counting can be slow (reads every CSV file).\n",
    "- Results are saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_defra_dataset_statistics(base_dir, metadata_path, nan_log_path):\n",
    "    \"\"\"\n",
    "    Calculate  statistics at DEFRA dataset.\n",
    "    This function walks through the monthly data directories 2023, 2024, 2025and calculates key metrics needed for reporting.\n",
    "    parameters:\n",
    "        base_dir : Path\n",
    "            base directory containing defra data folders\n",
    "        metadata_path : Path\n",
    "            path to the standardised metadata csv file\n",
    "        nan_log_path : Path\n",
    "            path to the NaN values log file after notice data flags, changed theem to NaN.\n",
    "            \n",
    "    returns:\n",
    "        dict : dictionary containing all calculated statistics\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # read metadata to get station and pollutant info.\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Calculate metadata statistics.\n",
    "    stats['unique_stations'] = metadata['station_name'].nunique()\n",
    "    stats['total_combinations'] = len(metadata)\n",
    "    stats['unique_pollutants'] = metadata['pollutant_std'].nunique()\n",
    "    \n",
    "    # Get pollutant breakdown\n",
    "    pollutant_counts = metadata['pollutant_std'].value_counts()\n",
    "    stats['pollutant_distribution'] = pollutant_counts.to_dict()\n",
    "    \n",
    "    # Count unique coordinates for spatial coverage, i will be use this for laqn dataset asweell.\n",
    "    # Group by lat/lon and count unique locations, instead of station names and will do the validation afterwards\n",
    "    unique_coords = metadata[['latitude', 'longitude']].drop_duplicates()\n",
    "    stats['unique_locations'] = len(unique_coords)\n",
    "    \n",
    "    # Count files in monthly data directories\n",
    "    total_files = 0\n",
    "    files_by_year = {}\n",
    "    \n",
    "    # Loop through each years measurement directory\n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        \n",
    "        if year_dir.exists():\n",
    "            # Count all CSV files in this years directory and subdirec..\n",
    "            year_files = list(year_dir.rglob('*.csv'))\n",
    "            files_by_year[year] = len(year_files)\n",
    "            total_files += len(year_files)\n",
    "            print(f\"  {year}: {len(year_files)} files\")\n",
    "        else:\n",
    "            files_by_year[year] = 0\n",
    "            print(f\"  {year}: Directory not found\")\n",
    "    \n",
    "    stats['total_files'] = total_files\n",
    "    stats['files_by_year'] = files_by_year\n",
    "    \n",
    "    # Calculate total measurement records, this requires reading all csv files and counting rows\n",
    "    total_records = 0\n",
    "    records_by_year = {}\n",
    "    total_missing = 0\n",
    "    missing_by_year = {}\n",
    "    \n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        year_records = 0\n",
    "        # noticed that I didn't use the nan_log_path to count missing values\n",
    "        year_missing = 0\n",
    "        \n",
    "        if year_dir.exists():\n",
    "            # read each csv, count rows\n",
    "            for csv_file in year_dir.rglob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    year_records += len(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Could not read {csv_file.name}: {e}\")\n",
    "            \n",
    "            records_by_year[year] = year_records\n",
    "            missing_by_year[year] = year_missing\n",
    "            total_records += year_records\n",
    "            total_missing += year_missing\n",
    "            print(f\"  {year}: {year_records:,} records, {year_missing:,} missing ({(year_missing/year_records*100):.2f}%)\")\n",
    "        else:\n",
    "            records_by_year[year] = 0\n",
    "    \n",
    "    stats['total_records'] = total_records\n",
    "    stats['records_by_year'] = records_by_year\n",
    "\n",
    "    stats['missing_by_year'] = missing_by_year\n",
    "    stats['overall_completeness'] = ((total_records - total_missing) / total_records * 100) if total_records > 0 else 0\n",
    "    \n",
    "    \n",
    "    # Calculate temporal coverage based on the files collected, understands which months have data\n",
    "    stats['temporal_coverage'] = {\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2025-11-19',  \n",
    "        'total_months': 35  # Jan 2023 to 19.Nov 2025\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5aaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(stats):\n",
    "    \"\"\"\n",
    "    print dataset statistics in a formatted.\n",
    "    \n",
    "    parameters:\n",
    "    stats : dict\n",
    "        dictionary returned by get_defra_dataset_statistics()\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Defra dataset statistics: initial assessment\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"\\nScale and scope:\")\n",
    "    print(f\"Total files collected: {stats['total_files']:,}\")\n",
    "    print(f\"Total measurement records: {stats['total_records']:,}\")\n",
    "    print(f\"Unique monitoring stations: {stats['unique_stations']}\")\n",
    "    print(f\"Total station-pollutant combinations: {stats['total_combinations']}\")\n",
    "    print(f\"Unique pollutant types: {stats['unique_pollutants']}\")\n",
    "    print(f\"Unique geographic locations: {stats['unique_locations']}\")\n",
    "    \n",
    "    print(\"\\nFiles by year:\")\n",
    "    for year, count in stats['files_by_year'].items():\n",
    "        print(f\"{year}: {count:,} files\")\n",
    "    \n",
    "    print(\"\\nRecords by year:\")\n",
    "    for year, count in stats['records_by_year'].items():\n",
    "        print(f\"{year}: {count:,} measurement records\")\n",
    "    \n",
    "    print(\"\\nTemporal coverage:\")\n",
    "    print(f\"start date: {stats['temporal_coverage']['start_date']}\")\n",
    "    print(f\"end date: {stats['temporal_coverage']['end_date']}\")\n",
    "    print(f\"total months: {stats['temporal_coverage']['total_months']}\")\n",
    "    \n",
    "    print(\"\\nPollutant distribution:\")\n",
    "    print(\"Station/Pollutant combinations by type:\")\n",
    "    for pollutant, count in sorted(stats['pollutant_distribution'].items(), \n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_combinations']) * 100\n",
    "        print(f\"  {pollutant}: {count} ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2023: 1431 files\n",
      "  2024: 1193 files\n",
      "  2025: 939 files\n",
      "  2023: 1,000,126 records\n",
      "  2024: 868,320 records\n",
      "  2025: 657,545 records\n",
      "\n",
      "========================================\n",
      "Defra dataset statistics: initial assessment\n",
      "========================================\n",
      "\n",
      "Scale and scope:\n",
      "Total files collected: 3,563\n",
      "Total measurement records: 2,525,991\n",
      "Unique monitoring stations: 18\n",
      "Total station-pollutant combinations: 144\n",
      "Unique pollutant types: 37\n",
      "Unique geographic locations: 20\n",
      "\n",
      "Files by year:\n",
      "2023: 1,431 files\n",
      "2024: 1,193 files\n",
      "2025: 939 files\n",
      "\n",
      "Records by year:\n",
      "2023: 1,000,126 measurement records\n",
      "2024: 868,320 measurement records\n",
      "2025: 657,545 measurement records\n",
      "\n",
      "Temporal coverage:\n",
      "start date: 2023-01-01\n",
      "end date: 2025-11-19\n",
      "total months: 35\n",
      "\n",
      "Pollutant distribution:\n",
      "Station/Pollutant combinations by type:\n",
      "  PM10: 15 (10.4%)\n",
      "  PM2.5: 15 (10.4%)\n",
      "  NO2: 14 (9.7%)\n",
      "  NOx: 14 (9.7%)\n",
      "  NO: 14 (9.7%)\n",
      "  O3: 9 (6.2%)\n",
      "  SO2: 3 (2.1%)\n",
      "  n-Pentane: 2 (1.4%)\n",
      "  m,p-Xylene: 2 (1.4%)\n",
      "  n-Butane: 2 (1.4%)\n",
      "  n-Heptane: 2 (1.4%)\n",
      "  n-Hexane: 2 (1.4%)\n",
      "  n-Octane: 2 (1.4%)\n",
      "  Propene: 2 (1.4%)\n",
      "  o-Xylene: 2 (1.4%)\n",
      "  Propane: 2 (1.4%)\n",
      "  i-Pentane: 2 (1.4%)\n",
      "  Toluene: 2 (1.4%)\n",
      "  trans-2-Butene: 2 (1.4%)\n",
      "  trans-2-Pentene: 2 (1.4%)\n",
      "  Isoprene: 2 (1.4%)\n",
      "  Ethyne: 2 (1.4%)\n",
      "  i-Octane: 2 (1.4%)\n",
      "  i-Hexane: 2 (1.4%)\n",
      "  i-Butane: 2 (1.4%)\n",
      "  Ethylbenzene: 2 (1.4%)\n",
      "  Ethene: 2 (1.4%)\n",
      "  Ethane: 2 (1.4%)\n",
      "  cis-2-Butene: 2 (1.4%)\n",
      "  Benzene: 2 (1.4%)\n",
      "  1-Pentene: 2 (1.4%)\n",
      "  1-Butene: 2 (1.4%)\n",
      "  1,3-Butadiene: 2 (1.4%)\n",
      "  1,3,5-TMB: 2 (1.4%)\n",
      "  1,2,4-TMB: 2 (1.4%)\n",
      "  1,2,3-TMB: 2 (1.4%)\n",
      "  CO: 2 (1.4%)\n",
      "\n",
      "Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/optimised/dataset_statistics.csv\n"
     ]
    }
   ],
   "source": [
    "# run the analysis\n",
    "stats = get_defra_dataset_statistics(base_dir, metadata_dir)\n",
    "print_dataset_statistics(stats)\n",
    "\n",
    "# # Save statistics for later use as csv\n",
    "# pd.DataFrame(list(stats.items()), columns=[\"Metric\", \"Value\"]).to_csv(stats_output_path, index=False)\n",
    "# print(f\"\\nStatistics saved to: {stats_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f816ef",
   "metadata": {},
   "source": [
    "    2023: 1431 files\n",
    "    2024: 1193 files\n",
    "    2025: 939 files\n",
    "    2023: 1,000,126 records\n",
    "    2024: 868,320 records\n",
    "    2025: 657,545 records\n",
    "\n",
    "    ========================================\n",
    "    Defra dataset statistics: initial assessment\n",
    "    ========================================\n",
    "\n",
    "    Scale and scope:\n",
    "    Total files collected: 3,563\n",
    "    Total measurement records: 2,525,991\n",
    "    Unique monitoring stations: 18\n",
    "    Total station-pollutant combinations: 144\n",
    "    Unique pollutant types: 37\n",
    "    Unique geographic locations: 20\n",
    "\n",
    "    Files by year:\n",
    "    2023: 1,431 files\n",
    "    2024: 1,193 files\n",
    "    2025: 939 files\n",
    "\n",
    "    Records by year:\n",
    "    2023: 1,000,126 measurement records\n",
    "    2024: 868,320 measurement records\n",
    "    2025: 657,545 measurement records\n",
    "\n",
    "    Temporal coverage:\n",
    "    start date: 2023-01-01\n",
    "    end date: 2025-11-19\n",
    "    total months: 35\n",
    "\n",
    "    Pollutant distribution:\n",
    "    Station/Pollutant combinations by type:\n",
    "    PM10: 15 (10.4%)\n",
    "    PM2.5: 15 (10.4%)\n",
    "    NO2: 14 (9.7%)\n",
    "    NOx: 14 (9.7%)\n",
    "    NO: 14 (9.7%)\n",
    "    O3: 9 (6.2%)\n",
    "    SO2: 3 (2.1%)\n",
    "    n-Pentane: 2 (1.4%)\n",
    "    m,p-Xylene: 2 (1.4%)\n",
    "    n-Butane: 2 (1.4%)\n",
    "    n-Heptane: 2 (1.4%)\n",
    "    n-Hexane: 2 (1.4%)\n",
    "    n-Octane: 2 (1.4%)\n",
    "    Propene: 2 (1.4%)\n",
    "    o-Xylene: 2 (1.4%)\n",
    "    Propane: 2 (1.4%)\n",
    "    i-Pentane: 2 (1.4%)\n",
    "    Toluene: 2 (1.4%)\n",
    "    trans-2-Butene: 2 (1.4%)\n",
    "    trans-2-Pentene: 2 (1.4%)\n",
    "    Isoprene: 2 (1.4%)\n",
    "    Ethyne: 2 (1.4%)\n",
    "    i-Octane: 2 (1.4%)\n",
    "    i-Hexane: 2 (1.4%)\n",
    "    i-Butane: 2 (1.4%)\n",
    "    Ethylbenzene: 2 (1.4%)\n",
    "    Ethene: 2 (1.4%)\n",
    "    Ethane: 2 (1.4%)\n",
    "    cis-2-Butene: 2 (1.4%)\n",
    "    Benzene: 2 (1.4%)\n",
    "    1-Pentene: 2 (1.4%)\n",
    "    1-Butene: 2 (1.4%)\n",
    "    1,3-Butadiene: 2 (1.4%)\n",
    "    1,3,5-TMB: 2 (1.4%)\n",
    "    1,2,4-TMB: 2 (1.4%)\n",
    "    1,2,3-TMB: 2 (1.4%)\n",
    "    CO: 2 (1.4%)\n",
    "\n",
    "- notes: analyse the stats:  2023: 1431 files, 2024: 1193 files, 2025: 939, making a total of 3,563 files. This is roughly 1k fewer than the laqn dataset which defra's issue  rate around 8%, and laqn's after hardcore cleaning decreased to %17ish. The number of monitoring stations is 18, with 37 unique pollutants and 144 station/pollutant combo. Although the defra dataset is smaller than the laqn dataset in terms of files and station/pollutant combinations promising better accuricy, and numerically six times more pollutant types than the laqn dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97ae28",
   "metadata": {},
   "source": [
    "## 2) Spatial Coverage Analysis\n",
    "\n",
    " analysing spatial distribution patterns before accepting the dataset. I need to understand where defra stations are located, identify any geographic biases, and compare coverage to laqn.\n",
    "\n",
    "### Purpose\n",
    "- Create maps showing station locations across London.\n",
    "- Analyse density by borough to identify coverage gaps\n",
    "- Compare spatial distribution to laqn network\n",
    "- Ensure no geographic areas are overrepresented or underrepresented\n",
    "\n",
    "### Methodology\n",
    "1. Load defra metadata with coordinates\n",
    "2. Create interactive folium map showing all stations\n",
    "3. Calculate station density by borough\n",
    "4. Identify coverage gaps in london\n",
    "5. Compare to laqn spatial distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364fa78",
   "metadata": {},
   "source": [
    "sources: \n",
    "- https://python-visualization.github.io/folium/latest/getting_started.html\n",
    "- https://pandas.pydata.org/docs/user_guide/groupby.html \n",
    "- plotting: https://geopandas.org/en/stable/docs/user_guide/data_structures.html#geoseries\n",
    "    - general: https://geopandas.org/en/stable/getting_started.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5f56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042b68d2",
   "metadata": {},
   "source": [
    "## 3) Statistical Validation\n",
    "\n",
    "A critical gap from the laqn report by applying formal statistical tests to validate data quality patterns. While descriptive statistics show 0% (before I notice the flags of the dataset) issue rate, I need statistical evidence that this pattern is real and not due to chance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
