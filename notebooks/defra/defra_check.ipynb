{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31f24e7",
   "metadata": {},
   "source": [
    "# DEFRA dataset Find Missing Parts\n",
    "\n",
    "- I will identify the missing values and data gaps in the DEFRA dataset and decide how to address them.\n",
    "- Iâ€™ll start by importing the relevant modules and displaying the initial file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc665ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict\n",
    "import importlib.util\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Use absolute path to avoid confusion\n",
    "base_dir = Path(\"/Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels\")\n",
    "processed_dir = base_dir / \"data\" / \"defra\" / \"processed\"\n",
    "metadata_path = base_dir / \"data\" / \"defra\" / \"test\" / \"london_stations_clean.csv\"\n",
    "\n",
    "#new metadata path for parse and checking.\n",
    "site_pollutant = base_dir / \"data\" / \"defra\" / \"test\" / \"london_stations_clean.csv\"\n",
    "pollutant_mapping_path = base_dir / \"src\" / \"data_prep\" / \"pollutant_mapps.py\"\n",
    "\n",
    "# Change output directory to data/defra/missing\n",
    "missing_path = base_dir / \"data\" / \"defra\" / \"missing\"\n",
    "missing_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc11c86",
   "metadata": {},
   "source": [
    "## 1. Optimise for accurate file reading.\n",
    " - metadata_path shows london_stations_clean.csv\n",
    "    - 2 row of the csv file: \n",
    "    - station_id,station_name,pollutant_available,pollutant_air,latitude,longitude,timeseries_id,pollutant\n",
    "    - 785876,Borehamwood Meadow Park,Nitrogen dioxide,air,51.661229,-0.2705499999910774,4565.0,6875 - Borehamwood Meadow Park-Nitrogen dioxide (air)\n",
    "\n",
    "- processed folder files: example: processed_dir path = /Borehamwood_Meadow_Park/NO__2023_01.csv\n",
    "    2 row of the csv file:\n",
    "    - timestamp,value,timeseries_id,station_name,pollutant_name,pollutant_std\n",
    "    - 2023-02-01 00:00:00,0.125,4564,Borehamwood Meadow Park,Nitrogen monoxide,NO\n",
    "\n",
    "metadata_paths and processed_dir file structure not matching. Solution need to add pollutant_std column to metadata_path file\n",
    "\n",
    "- new structure of metadata_path:\n",
    "    - station_id,station_name,pollutant_available,pollutant_std,pollutant_air,latitude,longitude,timeseries_id,pollutant\n",
    "    - 785876,Borehamwood Meadow Park,Nitrogen dioxide,HERE STD VERSION OF POLLUTANT(NO2),air,51.661229,-0.2705499999910774,4565.0,6875 - Borehamwood Meadow Park-Nitrogen dioxide (air)\n",
    "\n",
    "- standartise the pollutant names on pollutant_mapps.csv \n",
    "            -- DEFRA mappings common ones in both datasets first.\n",
    "            'Nitrogen dioxide': 'NO2',\n",
    "            'Nitrogen Dioxide': 'NO2',\n",
    "            'Nitrogen_dioxide': 'NO2',\n",
    "            'Nitric oxide': 'NO',\n",
    "            'Nitrogen_monoxide': 'NO',\n",
    "            'Nitrogen oxides': 'NOx',\n",
    "            'Nitrogen_oxides': 'NOx',\n",
    "            'PM2.5 Particulate': 'PM2.5',\n",
    "            'Particulate_matter_less_than_2.5_micro_m': 'PM2.5',\n",
    "            'PM10 Particulate': 'PM10',\n",
    "            'Particulate_matter_less_than_10_micro_m': 'PM10',\n",
    "            'Sulphur Dioxide': 'SO2',\n",
    "            'Sulphur dioxide': 'SO2',\n",
    "            'Sulphur_dioxide': 'SO2',\n",
    "            'Ozone': 'O3',\n",
    "            'Carbon Monoxide': 'CO',\n",
    "            'Carbon monoxide': 'CO',\n",
    "            'Carbon_monoxide': 'CO',\n",
    "            \n",
    "            -- VOCs used simplified standard codes instead of their chemical names.\n",
    "            'Benzene': 'Benzene',\n",
    "            'Toluene': 'Toluene',\n",
    "            'Ethylbenzene': 'Ethylbenzene',\n",
    "            'Ethyl_benzene': 'Ethylbenzene',\n",
    "            'o-Xylene': 'o-Xylene',\n",
    "            'm,p-Xylene': 'm,p-Xylene',\n",
    "            \n",
    "            -- Trimethylbenzenes.\n",
    "            '1,2,3-Trimethylbenzene': '1,2,3-TMB',\n",
    "            '1,2,4-Trimethylbenzene': '1,2,4-TMB',\n",
    "            '1,3,5-Trimethylbenzene': '1,3,5-TMB',\n",
    "            \n",
    "            -- Alkanes.\n",
    "            'Ethane': 'Ethane',\n",
    "            'Propane': 'Propane',\n",
    "            'n-Butane': 'n-Butane',\n",
    "            'i-Butane': 'i-Butane',\n",
    "            'n-Pentane': 'n-Pentane',\n",
    "            'i-Pentane': 'i-Pentane',\n",
    "            'n-Hexane': 'n-Hexane',\n",
    "            'i-Hexane': 'i-Hexane',\n",
    "            'n-Heptane': 'n-Heptane',\n",
    "            'n-Octane': 'n-Octane',\n",
    "            'i-Octane': 'i-Octane',\n",
    "            \n",
    "            - Alkenes\n",
    "            'Ethene': 'Ethene',\n",
    "            'Propene': 'Propene',\n",
    "            '1-Butene': '1-Butene',\n",
    "            'cis-2-Butene': 'cis-2-Butene',\n",
    "            'trans-2-Butene': 'trans-2-Butene',\n",
    "            '1-Pentene': '1-Pentene',\n",
    "            'trans-2-Pentene': 'trans-2-Pentene',\n",
    "            \n",
    "            - Other VOCs\n",
    "            '1,3-Butadiene': '1,3-Butadiene',\n",
    "            '1.3_Butadiene': '1,3-Butadiene',\n",
    "            'Isoprene': 'Isoprene',\n",
    "            'Ethyne': 'Ethyne',\n",
    "\n",
    "- New metadata file after add pollutant_std col std_london_sites_pollutant.csv and path below\n",
    "    -  site_pollutant = base_dir / \"data\" / \"defra\" / \"test\" / \"std_london_sites_pollutant.csv\"\n",
    "\n",
    "- Last std process for processed folder files:/defra/processed/2023measurements/Borehamwood_Meadow_Park/NO__2023_01.csv\n",
    "    - add latitude,longitude coorditanetion columns.\n",
    "    - timestamp,value,timeseries_id,station_name,pollutant_name,pollutant_std, latitude,longitude\n",
    "    - latitude,longitude columns:\n",
    "        - parse coordination columns from std_london_site_pollutant.csv\n",
    "        - the way to parse it matching pollutant_std and station_name columns. \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f171d83",
   "metadata": {},
   "source": [
    "### 1) Add column pollutant_std to std_london_sites_pollutant.csv file\n",
    "- adding pollutant_std colmn to london_stations_clean.csv file (metadata_path).\n",
    "    - new metadata_path saved as std_london_sites_pollutant.csv the path name changed as:\n",
    "    - site_pollutant = base_dir / \"data\" / \"defra\" / \"test\" / \"london_stations_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pollutant_std column to london_stations_clean.csv using DEFRA mappings\n",
    "# and match it to pollutant_available column\n",
    "\n",
    "defra_mappings = {\n",
    "    'Nitrogen dioxide': 'NO2',\n",
    "    'Nitrogen Dioxide': 'NO2',\n",
    "    'Nitrogen_dioxide': 'NO2',\n",
    "    'Nitric oxide': 'NO',\n",
    "    'Nitrogen_monoxide': 'NO',\n",
    "    'Nitrogen oxides': 'NOx',\n",
    "    'Nitrogen_oxides': 'NOx',\n",
    "    'PM2.5 Particulate': 'PM2.5',\n",
    "    'Particulate_matter_less_than_2.5_micro_m': 'PM2.5',\n",
    "    'PM10 Particulate': 'PM10',\n",
    "    'Particulate_matter_less_than_10_micro_m': 'PM10',\n",
    "    'Sulphur Dioxide': 'SO2',\n",
    "    'Sulphur dioxide': 'SO2',\n",
    "    'Sulphur_dioxide': 'SO2',\n",
    "    'Ozone': 'O3',\n",
    "    'Carbon Monoxide': 'CO',\n",
    "    'Carbon monoxide': 'CO',\n",
    "    'Carbon_monoxide': 'CO',\n",
    "    'Benzene': 'Benzene',\n",
    "    'Toluene': 'Toluene',\n",
    "    'Ethylbenzene': 'Ethylbenzene',\n",
    "    'Ethyl_benzene': 'Ethylbenzene',\n",
    "    'o-Xylene': 'o-Xylene',\n",
    "    'm,p-Xylene': 'm,p-Xylene',\n",
    "    '1,2,3-Trimethylbenzene': '1,2,3-TMB',\n",
    "    '1,2,4-Trimethylbenzene': '1,2,4-TMB',\n",
    "    '1,3,5-Trimethylbenzene': '1,3,5-TMB',\n",
    "    'Ethane': 'Ethane',\n",
    "    'Propane': 'Propane',\n",
    "    'n-Butane': 'n-Butane',\n",
    "    'i-Butane': 'i-Butane',\n",
    "    'n-Pentane': 'n-Pentane',\n",
    "    'i-Pentane': 'i-Pentane',\n",
    "    'n-Hexane': 'n-Hexane',\n",
    "    'i-Hexane': 'i-Hexane',\n",
    "    'n-Heptane': 'n-Heptane',\n",
    "    'n-Octane': 'n-Octane',\n",
    "    'i-Octane': 'i-Octane',\n",
    "    'Ethene': 'Ethene',\n",
    "    'Propene': 'Propene',\n",
    "    '1-Butene': '1-Butene',\n",
    "    'cis-2-Butene': 'cis-2-Butene',\n",
    "    'trans-2-Butene': 'trans-2-Butene',\n",
    "    '1-Pentene': '1-Pentene',\n",
    "    'trans-2-Pentene': 'trans-2-Pentene',\n",
    "    '1,3-Butadiene': '1,3-Butadiene',\n",
    "    '1.3_Butadiene': '1,3-Butadiene',\n",
    "    'Isoprene': 'Isoprene',\n",
    "    'Ethyne': 'Ethyne',\n",
    "}\n",
    "\n",
    "# Load the stations metadata\n",
    "stations_df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Add pollutant_std column by mapping pollutant_available\n",
    "stations_df['pollutant_std'] = stations_df['pollutant_available'].map(defra_mappings)\n",
    "\n",
    "# Save the updated DataFrame (commented out for now)\n",
    "# stations_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "# Display first few rows to check\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_london_sites_pollutant(metadata_path, output_path, defra_mappings):\n",
    "    \"\"\"\n",
    "    Adds a 'pollutant_std' column to the metadata and saves as std_london_sites_pollutant.csv.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    df['pollutant_std'] = df['pollutant_available'].map(defra_mappings)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved standardised metadata to {output_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723d338",
   "metadata": {},
   "source": [
    "## 2. Data Quality test Function:\n",
    "The functions for discover and checks data quality metrics before cleaning, below.\n",
    "- Counts total rows in dataset\n",
    "- Identifies missing values per column (count + percentage)\n",
    "- Counts duplicate rows based on timestamp\n",
    "- Detects negative values in measurements\n",
    "- Checks timestamp format issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c76fb",
   "metadata": {},
   "source": [
    "##### Defra dataset structure:\n",
    "defra/processed/(year)_measurements/station_names(with empty spaces underscore)/(pollutant_std)_(year)_(month) \n",
    " - first row of each csv file:\n",
    "    - timestamp,value,timeseries_id,station_name,pollutant_name,pollutant_std\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##### defra metadata: london_stations_clean.csv \n",
    " - first row: station_id,station_name,pollutant_available,pollutant_air,latitude,longitude,timeseries_id,pollutant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153af90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data quality analysis function for DEFRA dataset\n",
    "def defra_data_quality_analysis(processed_dir, pollutant_mapping_path, metadata_path, output_dir):\n",
    "    \"\"\"\n",
    "    DEFRA Data Quality Analysis:\n",
    "    - Checks empty files, missing columns, duplicates, missing values, types, format errors\n",
    "    - Standardises pollutant names using mapping\n",
    "    - Calculates issue rate for files with >20% missing 'value'\n",
    "    \"\"\"\n",
    "    # Load pollutant mappings from python file as dictionary (assume dict named 'DEFRA_MAPPINGS' in file)\n",
    "    \n",
    "\n",
    "    # Load metadata for reference (not used in checks, but available)\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Find all year directories (e.g. 2023_measurements)\n",
    "    year_dirs = list(Path(processed_dir).glob('*_measurements'))\n",
    "    missing_values_log = []\n",
    "    all_issues = {\n",
    "        'empty_files': [],\n",
    "        'duplicate_timestamps': [],\n",
    "        'high_missing_values': [],\n",
    "        'column_errors': [],\n",
    "        'format_errors': []\n",
    "    }\n",
    "    total_stats = {\n",
    "        'total_files': 0,\n",
    "        'files_processed': 0,\n",
    "        'files_with_high_missing': 0,\n",
    "        'total_rows': 0,\n",
    "        'empty_files': 0\n",
    "    }\n",
    "    # Required columns for DEFRA files\n",
    "    required_columns = ['timestamp', 'value', 'timeseries_id', 'station_name', 'pollutant_name', 'pollutant_std']\n",
    "\n",
    "    # Traverse all year and station directories\n",
    "    for year_dir in year_dirs:\n",
    "        for station_dir in Path(year_dir).iterdir():\n",
    "            if not station_dir.is_dir():\n",
    "                continue\n",
    "            for csv_file in station_dir.glob('*.csv'):\n",
    "                total_stats['total_files'] += 1\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    # Check if file is empty\n",
    "                    if df.empty:\n",
    "                        all_issues['empty_files'].append(str(csv_file))\n",
    "                        total_stats['empty_files'] += 1\n",
    "                        continue\n",
    "                    total_stats['files_processed'] += 1\n",
    "                    total_stats['total_rows'] += len(df)\n",
    "\n",
    "                    # Check for missing required columns\n",
    "                    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_cols:\n",
    "                        all_issues['column_errors'].append({'file': str(csv_file), 'missing_columns': missing_cols})\n",
    "                        continue\n",
    "\n",
    "                    # Standardise pollutant names\n",
    "                    df['pollutant_std'] = df['pollutant_name'].apply(lambda x: standardise_pollutant(x, mapping))\n",
    "\n",
    "                    # Check for duplicate timestamps\n",
    "                    dup_ts = df['timestamp'].duplicated().sum()\n",
    "                    if dup_ts > 0:\n",
    "                        all_issues['duplicate_timestamps'].append({'file': str(csv_file), 'duplicate_count': int(dup_ts)})\n",
    "\n",
    "                    # Check missing values for each column (print summary)\n",
    "                    for col in df.columns:\n",
    "                        missing_count = df[col].isna().sum()\n",
    "                        missing_pct = (missing_count / len(df) * 100) if len(df) > 0 else 0\n",
    "                        logger.info(f\"{csv_file.name}: Missing {col}: {missing_count} ({missing_pct:.2f}%)\")\n",
    "\n",
    "                    # Calculate missing value percentage for 'value' column\n",
    "                    missing_values = df['value'].isna().sum()\n",
    "                    empty_value_percentage = (100 * missing_values / len(df)) if len(df) > 0 else 0\n",
    "                    logger.info(f\"{csv_file.name}: Missing 'value': {missing_values}/{len(df)} ({empty_value_percentage:.2f}%)\")\n",
    "                    if empty_value_percentage > 20:\n",
    "                        total_stats['files_with_high_missing'] += 1\n",
    "                        missing_values_log.append({\n",
    "                            'filename': csv_file.name,\n",
    "                            'path': str(csv_file),\n",
    "                            'station_name': df['station_name'].iloc[0] if 'station_name' in df.columns else '',\n",
    "                            'pollutant_std': df['pollutant_std'].iloc[0] if 'pollutant_std' in df.columns else '',\n",
    "                            'EmptyValuePercentage': round(empty_value_percentage, 2)\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    all_issues['format_errors'].append({'file': str(csv_file), 'error': str(e)})\n",
    "\n",
    "    # Calculate issue rate: percentage of processed files with >20% missing 'value'\n",
    "    if total_stats['files_processed'] > 0:\n",
    "        issue_rate = (total_stats['files_with_high_missing'] / total_stats['files_processed']) * 100\n",
    "    else:\n",
    "        issue_rate = 0.0\n",
    "    print(f\"\\nIssue rate: {issue_rate:.2f}% of files have >20% missing 'value' column.\")\n",
    "\n",
    "    # Save log to CSV (commented out for now)\n",
    "    # if missing_values_log:\n",
    "    #     pd.DataFrame(missing_values_log).to_csv(Path(output_dir) / \"logs_missin_value.csv\", index=False)\n",
    "\n",
    "    return all_issues, total_stats, issue_rate  # Return issue rate for inspection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
