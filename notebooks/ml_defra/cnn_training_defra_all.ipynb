{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f197f750",
   "metadata": {},
   "source": [
    "# CNN model training for all DEFRA stations\n",
    "\n",
    "This notebook trains convolutional neural network (CNN) models to predict air pollution levels across ALL DEFRA UK-AIR monitoring stations. I train CNN models for every station pollutant combination in the DEFRA dataset.\n",
    "\n",
    "The goal is to compare network wide CNN performance against the random forest results from rf_training_defra_all.ipynb.\n",
    "\n",
    "## Why all stations instead of one?\n",
    "\n",
    "The single station approach was useful for proof of concept. The dissertation needs to show how the models perform across the entire DEFRA network. This means training separate models for each station pollutant combination.\n",
    "\n",
    "\n",
    "## Structure of notebook:\n",
    "\n",
    "| Section | What it does                                |\n",
    "| ------- | ------------------------------------------- |\n",
    "| 1       | Setup and imports                           |\n",
    "| 2       | Load prepared data `ml_prep_all`            |\n",
    "| 3       | Understanding data shapes                   |\n",
    "| 4       | Identify all target columns                 |\n",
    "| 5       | Build CNN model function                    |\n",
    "| 6       | Set-up training callbacks                   |\n",
    "| 7       | Train models for all targets with callbacks |\n",
    "| 8       | Load results training output                |\n",
    "| 9       | Investigation of broken models              |\n",
    "| 10      | Baseline evaluation after exclusion         |\n",
    "| 11      | Results summary and save                    |\n",
    "| 12      | Prediction visualisations                   |\n",
    "| 13      | Residual analysis                           |\n",
    "| 14      | Final summary                               |\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40234ea0",
   "metadata": {},
   "source": [
    "## 1) Setup and imports\n",
    "\n",
    "Importing everything needed for CNN training. Tensorflow/keras handles the neural network numpy for arrays matplotlib and seaborn for plotting scikit learn for metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baaa093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "#Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Scikit learn for metrics r2, MSE, MAE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "#Tensorflow and keras for CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e55313",
   "metadata": {},
   "source": [
    "Tensorflow version: 2.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74981b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/ml_prep_all\n",
      "Saving outputs to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/ml/DEFRA_all/cnn_model\n"
     ]
    }
   ],
   "source": [
    "#Paths\n",
    "base_dir = Path.cwd().parent.parent / 'data' / 'defra'\n",
    "data_dir = base_dir / 'ml_prep_all'\n",
    "output_dir = Path.cwd().parent.parent / 'data' / 'ml' / 'DEFRA_all' / 'cnn_model'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Loading data from: {data_dir}')\n",
    "print(f'Saving outputs to: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe05c10",
   "metadata": {},
   "source": [
    "    Loading data from: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/ml_prep_all\n",
    "    Saving outputs to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/ml/DEFRA_all/cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca668bd",
   "metadata": {},
   "source": [
    "### GPU availability\n",
    "\n",
    "Checking if GPU is available. CNN training is faster on GPU but will still work on CPU.\n",
    "\n",
    "Source: Use a GPU: Tensorflow Core (no date) TensorFlow. Available at: https://www.tensorflow.org/guide/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc46095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU training will be slower but still works.\n"
     ]
    }
   ],
   "source": [
    "#Check gpu availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'GPU available: {len(gpus)} device(s)')\n",
    "    for gpu in gpus:\n",
    "        print(f'  - {gpu.name}')\n",
    "else:\n",
    "    print('No GPU found, using CPU training will be slower but still works.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcb374",
   "metadata": {},
   "source": [
    "No GPU found, using CPU training will be slower but still works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137c237",
   "metadata": {},
   "source": [
    "## 2) Load prepared data `ml_prep_all`\n",
    "\n",
    "The data was prepared in `ml_prep_defra_all.ipynb` it created sequences where each sample has 12 hours of history to predict the next hour for all stations.\n",
    "\n",
    "### Why 3D data for CNN?\n",
    "\n",
    "Random forest needs flat 2D data: (samples, features). CNN needs 3D data: (samples, timesteps, features). The 3D shape lets CNN learn patterns across time, not just treat each timestep as an independent feature.\n",
    "\n",
    "\n",
    "| Data shape | Model | Structure |\n",
    "|------------|-------|----------|\n",
    "| 2D | Random forest | each row is a flat list of numbers with no structure |\n",
    "| 3D | CNN | each sample is a grid where rows are hours and columns are features |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7be915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "\n",
      "Shapes:\n",
      "X_train: (17036, 12, 44)\n",
      "X_val: (3641, 12, 44)\n",
      "X_test: (3642, 12, 44)\n",
      "y_train: (17036, 44)\n",
      "y_val: (3641, 44)\n",
      "y_test: (3642, 44)\n"
     ]
    }
   ],
   "source": [
    "#Load the 3d sequences for cnn\n",
    "X_train = np.load(data_dir / 'X_train.npy')\n",
    "X_val = np.load(data_dir / 'X_val.npy')\n",
    "X_test = np.load(data_dir / 'X_test.npy')\n",
    "\n",
    "y_train = np.load(data_dir / 'y_train.npy')\n",
    "y_val = np.load(data_dir / 'y_val.npy')\n",
    "y_test = np.load(data_dir / 'y_test.npy')\n",
    "\n",
    "#Load feature_names and scaler\n",
    "feature_names = joblib.load(data_dir / 'feature_names.joblib')\n",
    "scaler = joblib.load(data_dir / 'scaler.joblib')\n",
    "\n",
    "print('Data loaded successfully.')\n",
    "print(f'\\nShapes:')\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_val: {X_val.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'y_val: {y_val.shape}')\n",
    "print(f'y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225ad45",
   "metadata": {},
   "source": [
    "    Data loaded successfully.\n",
    "\n",
    "    Shapes:\n",
    "    X_train: (17036, 12, 44)\n",
    "    X_val: (3641, 12, 44)\n",
    "    X_test: (3642, 12, 44)\n",
    "    y_train: (17036, 44)\n",
    "    y_val: (3641, 44)\n",
    "    y_test: (3642, 44)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096fba6",
   "metadata": {},
   "source": [
    "## 3) Understanding data shapes\n",
    "\n",
    "X_train shape:\n",
    "\n",
    "| Dimension | What it represents |\n",
    "|-----------|--------------|\n",
    "| First | number of samples (individual training examples) |\n",
    "| Second | timesteps (12 hours of history) |\n",
    "| Third | features (all station pollutant columns + temporal) |\n",
    "\n",
    "y_train shape is (samples, features). The model can predict all features for the next hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ecacd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data dimensions:\n",
      "  Samples: 17,036\n",
      "  Timesteps: 12\n",
      "  Features: 44\n",
      "\n",
      "Feature names (44 total):\n",
      "  First 10: ['Borehamwood_Meadow_Park_NO2', 'Borehamwood_Meadow_Park_PM10', 'Borehamwood_Meadow_Park_PM25', 'Camden_Kerbside_NO2', 'Haringey_Roadside_NO2', 'London_Bexley_NO2', 'London_Bexley_PM10', 'London_Bloomsbury_NO2', 'London_Bloomsbury_O3', 'London_Bloomsbury_PM10']\n",
      "  Last 10: ['London_Teddington_Bushy_Park_PM25', 'London_Westminster_NO2', 'London_Westminster_O3', 'London_Westminster_PM25', 'Southwark_A2_Old_Kent_Road_NO2', 'Tower_Hamlets_Roadside_NO2', 'hour', 'day_of_week', 'month', 'is_weekend']\n"
     ]
    }
   ],
   "source": [
    "#Extract dimensions\n",
    "n_samples, timesteps, n_features = X_train.shape\n",
    "\n",
    "print(f'\\nData dimensions:')\n",
    "print(f'  Samples: {n_samples:,}')\n",
    "print(f'  Timesteps: {timesteps}')\n",
    "print(f'  Features: {n_features}')\n",
    "print(f'\\nFeature names ({len(feature_names)} total):')\n",
    "print(f'  First 10: {feature_names[:10]}')\n",
    "print(f'  Last 10: {feature_names[-10:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450b62c",
   "metadata": {},
   "source": [
    "    Data dimensions:\n",
    "    Samples: 17,036\n",
    "    Timesteps: 12\n",
    "    Features: 44\n",
    "\n",
    "    Feature names (44 total):\n",
    "    First 10: ['Borehamwood_Meadow_Park_NO2', 'Borehamwood_Meadow_Park_PM10', 'Borehamwood_Meadow_Park_PM25', 'Camden_Kerbside_NO2', 'Haringey_Roadside_NO2', 'London_Bexley_NO2', 'London_Bexley_PM10', 'London_Bloomsbury_NO2', 'London_Bloomsbury_O3', 'London_Bloomsbury_PM10']\n",
    "    Last 10: ['London_Teddington_Bushy_Park_PM25', 'London_Westminster_NO2', 'London_Westminster_O3', 'London_Westminster_PM25', 'Southwark_A2_Old_Kent_Road_NO2', 'Tower_Hamlets_Roadside_NO2', 'hour', 'day_of_week', 'month', 'is_weekend']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f1949",
   "metadata": {},
   "source": [
    "## 4) Identify all target columns\n",
    "\n",
    "Need to identify which columns are pollutant predictions (targets) and which are temporal features. Temporal features like hour, day_of_week are inputs only, not things we want to predict.\n",
    "\n",
    "### Pollutant naming convention\n",
    "\n",
    "Each target column follows the pattern: `{StationName}_{PollutantCode}`\n",
    "\n",
    "Example:\n",
    "\n",
    "| Example | Meaning |\n",
    "|---------|--------|\n",
    "| London_Bloomsbury_NO2 | NO2 at London Bloomsbury station |\n",
    "| London_Marylebone_Road_PM25 | PM2.5 at London Marylebone Road station |\n",
    "\n",
    "### The 6 regulatory pollutants:\n",
    "\n",
    "| Pollutant | Code | UK annual limit |\n",
    "|-----------|------|----------------|\n",
    "| Nitrogen Dioxide | NO2 | 40 ug/m3 |\n",
    "| PM2.5 Particulate | PM25 | 20 ug/m3 |\n",
    "| PM10 Particulate | PM10 | 40 ug/m3 |\n",
    "| Ozone | O3 | n/a |\n",
    "| Sulphur Dioxide | SO2 | n/a |\n",
    "| Carbon Monoxide | CO | n/a |\n",
    "\n",
    "Source: Department for Environment, Food and Rural Affairs (2019) UK Air Quality Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "target_identification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 44\n",
      "Temporal features: 4\n",
      "Pollutant targets: 40\n",
      "\n",
      "Breakdown by pollutant:\n",
      "  NO2: 13 stations\n",
      "  PM25: 7 stations\n",
      "  PM10: 7 stations\n",
      "  O3: 8 stations\n",
      "  SO2: 3 stations\n",
      "  CO: 2 stations\n"
     ]
    }
   ],
   "source": [
    "#Identify temporal vs pollutant columns\n",
    "temporal_cols = ['hour', 'day_of_week', 'month', 'is_weekend']\n",
    "\n",
    "#Get pollutant target columns (everything except temporal)\n",
    "target_names = [name for name in feature_names if name not in temporal_cols]\n",
    "target_indices = [i for i, name in enumerate(feature_names) if name not in temporal_cols]\n",
    "\n",
    "#Create target mapping dictionary\n",
    "target_mapping = {name: i for i, name in enumerate(feature_names) if name not in temporal_cols}\n",
    "\n",
    "print(f'Total features: {len(feature_names)}')\n",
    "print(f'Temporal features: {len(temporal_cols)}')\n",
    "print(f'Pollutant targets: {len(target_names)}')\n",
    "\n",
    "#Count by pollutant type\n",
    "pollutant_codes = ['NO2', 'PM25', 'PM10', 'O3', 'SO2', 'CO']\n",
    "print(f'\\nBreakdown by pollutant:')\n",
    "for poll in pollutant_codes:\n",
    "    count = len([n for n in target_names if f'_{poll}' in n])\n",
    "    print(f'  {poll}: {count} stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936826a9",
   "metadata": {},
   "source": [
    "    Total features: 44\n",
    "    Temporal features: 4\n",
    "    Pollutant targets: 40\n",
    "\n",
    "    Breakdown by pollutant:\n",
    "    NO2: 13 stations\n",
    "    PM25: 7 stations\n",
    "    PM10: 7 stations\n",
    "    O3: 8 stations\n",
    "    SO2: 3 stations\n",
    "    CO: 2 stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109ce08",
   "metadata": {},
   "source": [
    "## 5) Build CNN model function\n",
    "\n",
    "Building a function that creates CNN models. Using the best hyperparameters found from the single station tuning:\n",
    "\n",
    "| Parameter | Value | Why |\n",
    "|-----------|-------|-----|\n",
    "| filters_1 | 128 | more capacity to learn patterns |\n",
    "| kernel_1 | 2 | short term patterns matter most |\n",
    "| dropout | 0.1 | less regularisation needed |\n",
    "| filters_2 | 64 | second layer with fewer filters |\n",
    "| kernel_2 | 2 | consistent with first layer |\n",
    "| dense_units | 50 | same as baseline |\n",
    "| learning_rate | 0.001 | adam default works well |\n",
    "\n",
    "These parameters came from keras tuner results in the single station CNN notebook.\n",
    "\n",
    "Source: Geron, A. (2023) Hands on machine learning with scikit learn, Keras and TensorFlow. Ch. 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "689d6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(timesteps, features, filters_1=128, filters_2=64,\n",
    "                    kernel_size=2, dropout_rate=0.1, dense_units=50,\n",
    "                    learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build a 1D CNN for time series prediction.\n",
    "    Based on tuned hyperparameters from single station experiment.\n",
    "\n",
    "    Params:\n",
    "        timesteps: number of historical hours (12)\n",
    "        features: number of input features\n",
    "        filters_1: filters in first conv layer\n",
    "        filters_2: filters in second conv layer\n",
    "        kernel_size: size of convolutional kernel\n",
    "        dropout_rate: dropout rate for regularisation\n",
    "        dense_units: neurons in dense layer\n",
    "        learning_rate: adam learning rate\n",
    "\n",
    "    Returns:\n",
    "        compiled keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        #Input layer\n",
    "        layers.Input(shape=(timesteps, features)),\n",
    "\n",
    "        #First conv layer\n",
    "        layers.Conv1D(\n",
    "            filters=filters_1,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='causal'\n",
    "        ),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        #Second conv layer\n",
    "        layers.Conv1D(\n",
    "            filters=filters_2,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='causal'\n",
    "        ),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        #Flatten and dense for final prediction\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dense_units, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        #Output layer single value prediction\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate, clipnorm=1.0),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793aa747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 66,341 parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,450</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m11,392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m38,450\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,341</span> (259.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,341\u001b[0m (259.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,341</span> (259.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,341\u001b[0m (259.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test model creation\n",
    "test_model = build_cnn_model(timesteps, n_features)\n",
    "print(f'Model created with {test_model.count_params():,} parameters')\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eaa81a",
   "metadata": {},
   "source": [
    "Model created with 66,341 parameters\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
    "</pre>\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,392</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,450</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
    "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    "</pre>\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,341</span> (259.14 KB)\n",
    "</pre>\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_summary_explanation",
   "metadata": {},
   "source": [
    "### Understanding the summary\n",
    "\n",
    "The summary shows each layer, its output shape, and parameter count.\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| param # | number of learnable weights. more parameters = more capacity to learn, but also more risk of overfitting |\n",
    "| output shape | (None, timesteps, filters). None is batch size, determined at runtime |\n",
    "| total params | all weights the model will learn during training |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6cafe",
   "metadata": {},
   "source": [
    "## 6) Set-up training callbacks\n",
    "\n",
    "Callbacks control training behaviour.\n",
    "\n",
    "| Callback | What it does | Why |\n",
    "|----------|--------------|-----|\n",
    "| EarlyStopping | stops when validation loss stops improving | prevents overfitting |\n",
    "| ReduceLROnPlateau | reduces learning rate when stuck | helps find better minimum |\n",
    "\n",
    "Not using ModelCheckpoint for each model because of multiple targets. Saving checkpoints manually every N models instead.\n",
    "\n",
    "Source: Team, K. (no date) Keras Documentation: Callbacks. Available at: https://keras.io/api/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46459cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured:\n",
      "Early stopping (patience=10)\n",
      "Reduce LR on plateau (factor=0.5, patience=5)\n"
     ]
    }
   ],
   "source": [
    "def get_callbacks():\n",
    "    \"\"\"Create callbacks for training.\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.00001,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "\n",
    "print('Callbacks configured:')\n",
    "print('Early stopping (patience=10)')\n",
    "print('Reduce LR on plateau (factor=0.5, patience=5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee3972",
   "metadata": {},
   "source": [
    "    Callbacks configured:\n",
    "    Early stopping (patience=10)\n",
    "    Reduce LR on plateau (factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9a527",
   "metadata": {},
   "source": [
    "## 7) Train models for all targets\n",
    "\n",
    "Training a separate CNN model for each target. This will take a while because:\n",
    "\n",
    "| Aspect | Detail |\n",
    "|--------|--------|\n",
    "| Targets | station pollutant combinations |\n",
    "| Epochs per model | up to 50 (early stopping) |\n",
    "\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "Saving results every 20 models in case something goes wrong. This way if the notebook crashes, I do not lose everything.\n",
    "\n",
    "\n",
    "Source: Geron, A. (2023) Hands on machine learning with scikit learn, Keras and TensorFlow. Ch. 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "741aae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Batch size: 32\n",
      "  Max epochs: 50\n",
      "  Checkpoint every: 20 models\n",
      "  Total targets: 40\n"
     ]
    }
   ],
   "source": [
    "#Training configuration\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 50\n",
    "CHECKPOINT_EVERY = 20\n",
    "\n",
    "print(f'Training configuration:')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Max epochs: {MAX_EPOCHS}')\n",
    "print(f'  Checkpoint every: {CHECKPOINT_EVERY} models')\n",
    "print(f'  Total targets: {len(target_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731dbc92",
   "metadata": {},
   "source": [
    "    Training configuration:\n",
    "    Batch size: 32\n",
    "    Max epochs: 50\n",
    "    Checkpoint every: 20 models\n",
    "    Total targets: 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "training_loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2026-01-02 23:11:30\n",
      "Targets to train: 40\n",
      "Training samples: 17,036\n",
      "Features: 44\n",
      "========================================\n",
      "[  1/40] Borehamwood_Meadow_Park_NO2              | R2=0.839 | Time=41s | ETA=27min\n",
      "[  2/40] Borehamwood_Meadow_Park_PM10             | R2=0.849 | Time=33s | ETA=21min\n",
      "[  3/40] Borehamwood_Meadow_Park_PM25             | R2=0.861 | Time=32s | ETA=20min\n",
      "[  4/40] Camden_Kerbside_NO2                      | R2=0.683 | Time=36s | ETA=22min\n",
      "[  5/40] Haringey_Roadside_NO2                    | R2=0.801 | Time=39s | ETA=23min\n",
      "[  6/40] London_Bexley_NO2                        | R2=0.810 | Time=36s | ETA=20min\n",
      "[  7/40] London_Bexley_PM10                       | R2=0.779 | Time=42s | ETA=23min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m model = build_cnn_model(timesteps, n_features)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#Evaluate on test set\u001b[39;00m\n\u001b[32m     30\u001b[39m y_pred = model.predict(X_test, verbose=\u001b[32m0\u001b[39m).flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1498\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1508\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1509\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1510\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1515\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/data science projects/air-pollution-levels/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "results = []\n",
    "all_models = {}\n",
    "\n",
    "start_time = time.time()\n",
    "print(f'Started at: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'Targets to train: {len(target_names)}')\n",
    "print(f'Training samples: {n_samples:,}')\n",
    "print(f'Features: {n_features}')\n",
    "print('=' * 40)\n",
    "\n",
    "for i, target_name in enumerate(target_names):\n",
    "    target_idx = target_mapping[target_name]\n",
    "    model_start = time.time()\n",
    "\n",
    "    #Build fresh model\n",
    "    model = build_cnn_model(timesteps, n_features)\n",
    "\n",
    "    #Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train[:, target_idx],\n",
    "        validation_data=(X_val, y_val[:, target_idx]),\n",
    "        epochs=MAX_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=get_callbacks(),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    #Evaluate on test set\n",
    "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "    y_actual = y_test[:, target_idx]\n",
    "\n",
    "    test_r2 = r2_score(y_actual, y_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "    test_mae = mean_absolute_error(y_actual, y_pred)\n",
    "\n",
    "    #Extract pollutant from target name\n",
    "    parts = target_name.rsplit('_', 1)\n",
    "    pollutant = parts[1] if len(parts) > 1 else 'unknown'\n",
    "    site = parts[0] if len(parts) > 1 else target_name\n",
    "\n",
    "    results.append({\n",
    "        'target': target_name,\n",
    "        'pollutant': pollutant,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'site': site\n",
    "    })\n",
    "\n",
    "    #Store model\n",
    "    all_models[target_name] = model\n",
    "\n",
    "    #Progress update\n",
    "    elapsed = time.time() - model_start\n",
    "    remaining = len(target_names) - (i + 1)\n",
    "    eta = (elapsed * remaining) / 60\n",
    "    print(f'[{i+1:3d}/{len(target_names)}] {target_name:40s} | R2={test_r2:.3f} | Time={elapsed:.0f}s | ETA={eta:.0f}min')\n",
    "\n",
    "    #Checkpoint\n",
    "    if (i + 1) % CHECKPOINT_EVERY == 0:\n",
    "        checkpoint_df = pd.DataFrame(results)\n",
    "        checkpoint_df.to_csv(output_dir / f'checkpoint_{i+1}.csv', index=False)\n",
    "        print(f'   [Checkpoint saved at {i+1} models]')\n",
    "\n",
    "    #Memory cleanup\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print('=' * 40)\n",
    "print('Training complete!')\n",
    "print(f'Total time: {total_time:.1f} minutes ({total_time/60:.2f} hours)')\n",
    "print(f'Average per model: {total_time*60/len(target_names):.1f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5700ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and results after training completes\n",
    "joblib.dump(all_models, output_dir / 'cnn_all_models.joblib')\n",
    "print(f'Saved {len(all_models)} models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_section",
   "metadata": {},
   "source": [
    "## 8) Load results training output\n",
    "\n",
    "Loading the results from the training output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_from_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load results from training output if training just completed, results should be in the 'results' variable\n",
    "results_file = output_dir / 'cnn_results_local.csv'\n",
    "\n",
    "if 'results' in dir() and len(results) > 0:\n",
    "    #Use results from current training session\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f'Using results from current training session')\n",
    "    print(f'Loaded {len(results_df)} results')\n",
    "    \n",
    "elif results_file.exists():\n",
    "    #Load from saved csv\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    print(f'Loaded results from: {results_file}')\n",
    "    print(f'Loaded {len(results_df)} results')\n",
    "    \n",
    "else:\n",
    "    print('No results found!')\n",
    "    print('Either run the training loop or check the output directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_results_df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify results dataframe structure\n",
    "print('Results dataframe')\n",
    "print('=' * 40)\n",
    "\n",
    "#Check if required columns exist, add if missing\n",
    "if 'site' not in results_df.columns:\n",
    "    results_df['site'] = results_df['target'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "    \n",
    "if 'pollutant' not in results_df.columns:\n",
    "    results_df['pollutant'] = results_df['target'].apply(lambda x: x.rsplit('_', 1)[1])\n",
    "\n",
    "print(f'Shape: {results_df.shape}')\n",
    "print(f'\\nColumns: {list(results_df.columns)}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "print(results_df.head().to_string(index=False))\n",
    "\n",
    "#Save results\n",
    "results_df.to_csv(output_dir / 'cnn_results_local.csv', index=False)\n",
    "print(f'\\nResults saved to: {output_dir / \"cnn_results_local.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken_models_section",
   "metadata": {},
   "source": [
    "## 9) Investigation of broken models\n",
    "\n",
    "Some models may produce extremely negative R2 values, indicating numerical issues. Before continuing with results analysis, I need to investigate and document these failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identify_broken",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify broken models\n",
    "print('Identifying broken models')\n",
    "print('=' * 40)\n",
    "\n",
    "#Broken threshold: R2 < -10 is clearly numerical failure\n",
    "broken_threshold = -10\n",
    "\n",
    "broken_models = results_df[results_df['test_r2'] < broken_threshold].copy()\n",
    "valid_models = results_df[results_df['test_r2'] >= broken_threshold].copy()\n",
    "\n",
    "print(f'\\nTotal models:  {len(results_df)}')\n",
    "print(f'Valid models:  {len(valid_models)}')\n",
    "print(f'Broken models: {len(broken_models)}')\n",
    "\n",
    "if len(broken_models) > 0:\n",
    "    print('\\n' + '-' * 40)\n",
    "    print('Broken models:')\n",
    "    print('-' * 40)\n",
    "    print(broken_models[['target', 'pollutant', 'test_r2']].to_string(index=False))\n",
    "else:\n",
    "    print('\\nNo broken models found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "investigate_broken",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate broken models by checking test set variance\n",
    "if len(broken_models) > 0:\n",
    "    print('Detailed investigation of broken models:')\n",
    "    print('=' * 40)\n",
    "\n",
    "    broken_targets = broken_models['target'].values\n",
    "\n",
    "    for target in broken_targets:\n",
    "        print(f'\\n>>> {target}')\n",
    "        print('-' * 40)\n",
    "\n",
    "        target_idx = target_mapping[target]\n",
    "\n",
    "        #Check target data statistics\n",
    "        y_train_target = y_train[:, target_idx]\n",
    "        y_val_target = y_val[:, target_idx]\n",
    "        y_test_target = y_test[:, target_idx]\n",
    "\n",
    "        print(f'Training   - min: {y_train_target.min():.6f}, max: {y_train_target.max():.6f}, '\n",
    "              f'std: {y_train_target.std():.6f}')\n",
    "        print(f'Validation - min: {y_val_target.min():.6f}, max: {y_val_target.max():.6f}, '\n",
    "              f'std: {y_val_target.std():.6f}')\n",
    "        print(f'Test       - min: {y_test_target.min():.6f}, max: {y_test_target.max():.6f}, '\n",
    "              f'std: {y_test_target.std():.6f}')\n",
    "\n",
    "        #Check for constant or near constant values\n",
    "        if y_test_target.std() < 0.001:\n",
    "            print('>>> Very low variance in test set (std < 0.001)')\n",
    "else:\n",
    "    print('No broken models to investigate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "root_cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root cause analysis\n",
    "if len(broken_models) > 0:\n",
    "    print('Root cause analysis of broken models:')\n",
    "    print('=' * 40)\n",
    "\n",
    "    broken_targets = broken_models['target'].values\n",
    "\n",
    "    #Check broken models site\n",
    "    broken_sites = [t.rsplit('_', 1)[0] for t in broken_targets]\n",
    "    print(f'\\nBroken model sites: {broken_sites}')\n",
    "\n",
    "    #Count and addup if the sites have multiple failures\n",
    "    site_counts = Counter(broken_sites)\n",
    "    multi_broken = {site: count for site, count in site_counts.items() if count > 1}\n",
    "\n",
    "    if multi_broken:\n",
    "        print(f'\\nSites with multiple broken models: {multi_broken}')\n",
    "        print('This suggests data quality issues at these monitoring stations.')\n",
    "else:\n",
    "    print('No broken models to analyse.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken_findings",
   "metadata": {},
   "source": [
    "### Findings: why these models broke\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_eval_section",
   "metadata": {},
   "source": [
    "## 10) Baseline evaluation after exclusion\n",
    "\n",
    "Evaluating the valid models, excluding any broken models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline evaluation excluding broken models\n",
    "print('CNN baseline evaluation excluding broken models:')\n",
    "print('=' * 40)\n",
    "\n",
    "print(f'\\nValid models: {len(valid_models)} out of {len(results_df)}')\n",
    "print(f'Broken models excluded: {len(broken_models)}')\n",
    "\n",
    "print('Test set performance with valid models only:')\n",
    "print('-' * 40)\n",
    "\n",
    "print(f'\\nMean R2:   {valid_models[\"test_r2\"].mean():.4f}')\n",
    "print(f'Median R2: {valid_models[\"test_r2\"].median():.4f}')\n",
    "print(f'Std R2:    {valid_models[\"test_r2\"].std():.4f}')\n",
    "print(f'Min R2:    {valid_models[\"test_r2\"].min():.4f}')\n",
    "print(f'Max R2:    {valid_models[\"test_r2\"].max():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pollutant_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance by pollutant type\n",
    "print('\\nPerformance by pollutant type')\n",
    "print('=' * 40)\n",
    "\n",
    "pollutant_summary = valid_models.groupby('pollutant').agg({\n",
    "    'test_r2': ['mean', 'std', 'min', 'max', 'count']\n",
    "}).round(4)\n",
    "\n",
    "pollutant_summary.columns = ['r2_mean', 'r2_std', 'r2_min', 'r2_max', 'n_models']\n",
    "pollutant_summary = pollutant_summary.sort_values('r2_mean', ascending=False)\n",
    "\n",
    "print(pollutant_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top_bottom_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top and bottom performing models\n",
    "print('\\nTop 10 best performing targets (by R2)')\n",
    "print('-' * 50)\n",
    "top_10 = valid_models.nlargest(10, 'test_r2')[['target', 'pollutant', 'test_r2']]\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "print('\\nBottom 10 worst performing targets (by R2)')\n",
    "print('-' * 50)\n",
    "bottom_10 = valid_models.nsmallest(10, 'test_r2')[['target', 'pollutant', 'test_r2']]\n",
    "print(bottom_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_section",
   "metadata": {},
   "source": [
    "## 11) Results summary and save\n",
    "\n",
    "Saving all results to csv files for later analysis and comparison with random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results\n",
    "results_df.to_csv(output_dir / 'cnn_all_results.csv', index=False)\n",
    "valid_models.to_csv(output_dir / 'cnn_valid_results.csv', index=False)\n",
    "if len(broken_models) > 0:\n",
    "    broken_models.to_csv(output_dir / 'cnn_broken_models.csv', index=False)\n",
    "pollutant_summary.to_csv(output_dir / 'cnn_pollutant_summary.csv')\n",
    "\n",
    "print('Results saved:')\n",
    "print(f'  - cnn_all_results.csv ({len(results_df)} models)')\n",
    "print(f'  - cnn_valid_results.csv ({len(valid_models)} models)')\n",
    "if len(broken_models) > 0:\n",
    "    print(f'  - cnn_broken_models.csv ({len(broken_models)} models)')\n",
    "print(f'  - cnn_pollutant_summary.csv')\n",
    "print(f'\\nOutput directory: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_section",
   "metadata": {},
   "source": [
    "## 12) Prediction visualisations\n",
    "\n",
    "Plotting actual vs predicted values helps identify systematic errors or patterns the models miss.\n",
    "\n",
    "**Scatter plot interpretation:**\n",
    "\n",
    "| Pattern | Meaning |\n",
    "|---------|--------|\n",
    "| Points close to diagonal | good predictions |\n",
    "| Spread around line | prediction variance |\n",
    "| Curve away at high values | model underestimates pollution spikes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r2_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 distribution histogram and boxplot by pollutant\n",
    "#Exclude models with R2 < 0 for better visualisation\n",
    "valid_for_plot = valid_models[valid_models['test_r2'] >= 0].copy()\n",
    "print(f'Models for visualisation: {len(valid_for_plot)} (excluding {len(valid_models) - len(valid_for_plot)} with R2 < 0)')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "#Histogram\n",
    "axes[0].hist(valid_for_plot['test_r2'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(valid_for_plot['test_r2'].mean(), color='red', linestyle='--', \n",
    "                label=f'mean = {valid_for_plot[\"test_r2\"].mean():.3f}')\n",
    "axes[0].set_xlabel('Test R2')\n",
    "axes[0].set_ylabel('Number of models')\n",
    "axes[0].set_title('Distribution of test R2 valid models')\n",
    "axes[0].legend()\n",
    "\n",
    "#Boxplot by pollutant\n",
    "pollutant_order = ['O3', 'NO2', 'PM25', 'PM10', 'CO', 'SO2']\n",
    "box_data = [valid_for_plot[valid_for_plot['pollutant'] == p]['test_r2'].values \n",
    "            for p in pollutant_order if p in valid_for_plot['pollutant'].values]\n",
    "box_labels = [p for p in pollutant_order if p in valid_for_plot['pollutant'].values]\n",
    "\n",
    "axes[1].boxplot(box_data, tick_labels=box_labels)\n",
    "axes[1].set_xlabel('Pollutant')\n",
    "axes[1].set_ylabel('Test R2')\n",
    "axes[1].set_title('Test R2 by pollutant type:')\n",
    "axes[1].axhline(0.8, color='green', linestyle='--', alpha=0.5, label='R2 = 0.8')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'cnn_r2_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Saved: cnn_r2_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r2_interpretation",
   "metadata": {},
   "source": [
    "### Interpretation: R2 distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_series_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time series plots for best model per pollutant\n",
    "print('Time series plots for best model per pollutant')\n",
    "print('=' * 40)\n",
    "\n",
    "colors = {\n",
    "    'NO2': '#1f77b4',\n",
    "    'PM10': '#2ca02c',\n",
    "    'PM25': '#9467bd',\n",
    "    'O3': '#ff7f0e',\n",
    "    'SO2': '#d62728',\n",
    "    'CO': '#8c564b'\n",
    "}\n",
    "\n",
    "n_samples_plot = 200\n",
    "\n",
    "for pollutant in ['O3', 'NO2', 'PM25', 'PM10', 'CO', 'SO2']:\n",
    "    #Find best model for this pollutant\n",
    "    poll_results = valid_models[valid_models['pollutant'] == pollutant]\n",
    "\n",
    "    if len(poll_results) == 0:\n",
    "        print(f'{pollutant}: no valid models')\n",
    "        continue\n",
    "\n",
    "    best_target = poll_results.loc[poll_results['test_r2'].idxmax(), 'target']\n",
    "    best_r2 = poll_results['test_r2'].max()\n",
    "\n",
    "    target_idx = target_mapping[best_target]\n",
    "    model = all_models[best_target]\n",
    "\n",
    "    #Get predictions\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    y_actual_test = y_test[:, target_idx]\n",
    "\n",
    "    #Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    ax.plot(range(n_samples_plot), y_actual_test[:n_samples_plot],\n",
    "            label='Actual', color=colors[pollutant], linewidth=1.5)\n",
    "    ax.plot(range(n_samples_plot), y_pred_test[:n_samples_plot],\n",
    "            label='Predicted', color='black', linewidth=1.5, linestyle='--')\n",
    "\n",
    "    ax.set_xlabel('Sample index')\n",
    "    ax.set_ylabel(f'{best_target} (normalised)')\n",
    "    ax.set_title(f'{pollutant}: test set predictions ({best_target}, R2 = {best_r2:.3f})')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f'cnn_time_series_{pollutant}_{best_target}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'{pollutant}: {best_target} (R2 = {best_r2:.4f})')\n",
    "    print('-' * 40)\n",
    "\n",
    "print('All time series plots saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time_series_interpretation",
   "metadata": {},
   "source": [
    "### Interpretation: time series predictions by pollutant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residual_section",
   "metadata": {},
   "source": [
    "## 13) Residual analysis\n",
    "\n",
    "Residuals are the difference between actual and predicted values. If the model is good, residuals should scatter randomly around zero with no pattern.\n",
    "\n",
    "residual = actual - predicted\n",
    "\n",
    "Source: Effect of transforming the targets in regression model (no date) scikit. Available at: https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual analysis methodology reference code\n",
    "print('Residual analysis methodology:')\n",
    "print('=' * 40)\n",
    "\n",
    "#Get best model for residual analysis\n",
    "best_idx = valid_models['test_r2'].idxmax()\n",
    "best_target = valid_models.loc[best_idx, 'target']\n",
    "target_idx = target_mapping[best_target]\n",
    "model = all_models[best_target]\n",
    "\n",
    "print(f'Analysing residuals for best model: {best_target}')\n",
    "\n",
    "#Get predictions\n",
    "y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "y_actual = y_test[:, target_idx]\n",
    "\n",
    "#Calculate residuals\n",
    "residuals = y_actual - y_pred\n",
    "\n",
    "#Plot residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "#Residuals vs predicted\n",
    "axes[0].scatter(y_pred, residuals, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].set_title('Residuals vs predicted')\n",
    "\n",
    "#Residual histogram\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[1].set_xlabel('Residual')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'cnn_residual_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Saved: cnn_residual_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33006efe",
   "metadata": {},
   "source": [
    "Expected patterns:\n",
    "- Good model: residuals randomly scattered around zero\n",
    "- Underfitting: systematic patterns in residuals\n",
    "- Heteroscedasticity: residual spread increases with predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residual_interpretation",
   "metadata": {},
   "source": [
    "### Interpretation: residual analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_summary_section",
   "metadata": {},
   "source": [
    "## 14) Final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final summary\n",
    "print('CNN model training summary all DEFRA targets')\n",
    "print('=' * 40)\n",
    "\n",
    "print(f'\\nDataset:')\n",
    "print(f'  Training samples:   {X_train.shape[0]:,}')\n",
    "print(f'  Validation samples: {X_val.shape[0]:,}')\n",
    "print(f'  Test samples:       {X_test.shape[0]:,}')\n",
    "print(f'  Features:           {X_train.shape[2]:,}')\n",
    "print(f'  Timesteps:          {X_train.shape[1]}')\n",
    "\n",
    "print(f'\\nModels:')\n",
    "print(f'  Total trained:      {len(results_df)}')\n",
    "print(f'  Valid models:       {len(valid_models)}')\n",
    "print(f'  Broken models:      {len(broken_models)} (excluded due to data quality issues)')\n",
    "\n",
    "print(f'\\nHyperparameters used:')\n",
    "print(f'  filters_1:     128')\n",
    "print(f'  filters_2:     64')\n",
    "print(f'  kernel_size:   2')\n",
    "print(f'  dropout:       0.1')\n",
    "print(f'  dense_units:   50')\n",
    "print(f'  learning_rate: 0.001')\n",
    "\n",
    "print(f'\\nTest set performance for only valid models:')\n",
    "print(f'  Mean R2:   {valid_models[\"test_r2\"].mean():.4f} (+/- {valid_models[\"test_r2\"].std():.4f})')\n",
    "print(f'  Median R2: {valid_models[\"test_r2\"].median():.4f}')\n",
    "\n",
    "#Best performed pollutant\n",
    "best_poll = pollutant_summary['r2_mean'].idxmax()\n",
    "best_poll_r2 = pollutant_summary.loc[best_poll, 'r2_mean']\n",
    "print(f'\\nBest performing pollutant: {best_poll} (mean R2 = {best_poll_r2:.4f})')\n",
    "\n",
    "#Best performed individual model\n",
    "best_idx = valid_models['test_r2'].idxmax()\n",
    "best_target = valid_models.loc[best_idx, 'target']\n",
    "best_r2 = valid_models.loc[best_idx, 'test_r2']\n",
    "print(f'Best individual model: {best_target} (R2 = {best_r2:.4f})')\n",
    "\n",
    "print(f'\\nOutputs saved to: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "### Key findings\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
