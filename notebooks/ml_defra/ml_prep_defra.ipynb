{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16f68a0",
   "metadata": {},
   "source": [
    "# ML DEFRA Data Preparation for Air Quality Prediction\n",
    "\n",
    "This notebook prepares DEFRA data for machine learning models.\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1. Loads cleaned DEFRA data from the measurements folder.\n",
    "\n",
    "   ```bash\n",
    "   ├── 2023measurements/           # Year folders\n",
    "   │   ├── London Bloomsbury/      # Station folders\n",
    "   │   │   ├── NO2__2023_01.csv   # Pollutant files\n",
    "   │   │   ├── PM10__2023_01.csv\n",
    "   │   │   └── ...\n",
    "   │   ├── London Eltham/\n",
    "   │   └── ...\n",
    "   ├── 2024measurements/\n",
    "   └── 2025measurements/\n",
    "   ```\n",
    "\n",
    "2. Combines all measurements into a single dataset.\n",
    "3. Creates temporal features (hour, day, month).\n",
    "4. Creates sequences for ML training.\n",
    "\n",
    "## Key difference from LAQN\n",
    "\n",
    "| Aspect | LAQN | DEFRA |\n",
    "|--------|------|-------|\n",
    "| File structure | SiteCode_Species_Date.csv | Station/Pollutant__YYYY_MM.csv |\n",
    "| Date column | @MeasurementDateGMT | date (or Date) |\n",
    "| Value column | @Value | varies by pollutant name |\n",
    "| Missing flags | NaN | -99 (maintenance), -1 (invalid) |\n",
    "\n",
    "## Output path:\n",
    "\n",
    "Data will be saved to: `data/defra/ml_prep/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f93c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports same as LAQN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Save section\n",
    "import joblib\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a86fb",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "\n",
    "- Usual drill, adding paths under this cell for organisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d3ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra\n",
      "Output path: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/defra/ml_prep\n"
     ]
    }
   ],
   "source": [
    "# DEFRA prep file path\n",
    "base_dir = Path.cwd().parent.parent / \"data\" / \"defra\" \n",
    "project_root = Path.cwd() / \"defra_ml_prep.ipynb\"\n",
    "\n",
    "# Defra's optimased measurements data path\n",
    "optimased_path = base_dir / \"optimased\" # Contains 2023measurements, 2024measurements, etc.\n",
    "\n",
    "# Output paths\n",
    "output_path = base_dir / \"ml_prep\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Visualisation output path\n",
    "visualisation_path = output_path / \"visualisation\"\n",
    "visualisation_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ad20e",
   "metadata": {},
   "source": [
    "## 1) Load DEFRA data\n",
    "\n",
    "**CHANGE FROM LAQN:** \n",
    "- LAQN has flat monthly folders with all files\n",
    "- DEFRA has nested structure: year > station > pollutant files\n",
    "\n",
    "DEFRA file naming: `{POLLUTANT}__{YYYY_MM}.csv`\n",
    "\n",
    "DEFRA columns typically include:\n",
    "- `date` or `Date` - timestamp\n",
    "- Pollutant name as column (e.g., `Nitrogen dioxide`, `PM10 particulate matter`)\n",
    "- Values use -99 for maintenance, -1 for invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_defra_data(optimased_path):\n",
    "    \"\"\"\n",
    "    Function to load the optimased data files from the defra dataset.\n",
    "\n",
    "            optimased_path: path for data/defra/optimased directory.\n",
    "\n",
    "    \"\"\"\n",
    "    optimased_path = Path(optimased_path)\n",
    "    all_files = []\n",
    "    file_count = 0\n",
    "    \n",
    "    # Get all year measurement folders\n",
    "    year_folders = sorted([f for f in optimased_path.glob('*measurements') if f.is_dir()])\n",
    "    \n",
    "    print(f\"Found {len(year_folders)} year folders\")\n",
    "    \n",
    "    for year_dir in year_folders:\n",
    "        year = year_dir.name.replace('measurements', '')\n",
    "        print(f\"\\nProcessing {year}...\")\n",
    "        \n",
    "        # Iterate through station folders\n",
    "        for station_dir in sorted(year_dir.iterdir()):\n",
    "            if not station_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            station_name = station_dir.name\n",
    "            \n",
    "            # Process each CSV file in station folder\n",
    "            for csv_file in station_dir.glob('*.csv'):\n",
    "                try:\n",
    "                    # Parse filename: POLLUTANT__YYYY_MM.csv\n",
    "                    parts = csv_file.stem.split('__')\n",
    "                    if len(parts) != 2:\n",
    "                        print(f\"  Skipping {csv_file.name}: unexpected format\")\n",
    "                        continue\n",
    "                    \n",
    "                    pollutant_name = parts[0]\n",
    "                    \n",
    "                    # Read the CSV\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    # DEFRA files have pollutant_std name as column\n",
    "                    # Find the value column not 'date' or 'Date'\n",
    "                    date_cols = ['date', 'Date']\n",
    "                    value_col = None\n",
    "                    date_col = None\n",
    "                    \n",
    "                    for col in df.columns:\n",
    "                        if col.lower() == 'date':\n",
    "                            date_col = col\n",
    "                        elif col not in date_cols:\n",
    "                            value_col = col  # Assume non-date column is value\n",
    "                    \n",
    "                    if date_col is None or value_col is None:\n",
    "                        print(f\"  Skipping {csv_file.name}: missing date or value column\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Standardise column names to match LAQN format\n",
    "                    df_standard = pd.DataFrame({\n",
    "                        '@MeasurementDateGMT': df[date_col],\n",
    "                        '@Value': df[value_col],\n",
    "                        'SpeciesCode': pollutant_name,  # Will standardise later\n",
    "                        'SiteCode': station_name.replace(' ', '_'),  # Create site code\n",
    "                        'SiteName': station_name,\n",
    "                        'Source': 'DEFRA'\n",
    "                    })\n",
    "                    \n",
    "                    all_files.append(df_standard)\n",
    "                    file_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error reading {csv_file.name}: {e}\")\n",
    "        \n",
    "        print(f\"  Loaded from {year_dir.name}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if not all_files:\n",
    "        raise ValueError(f\"No CSV files found in {optimased_path}\")\n",
    "    \n",
    "    combined_df = pd.concat(all_files, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"Total files loaded: {file_count}\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    print(f\"Columns: {list(combined_df.columns)}\")\n",
    "    \n",
    "    return combined_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
