{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Random Forest Training - DEFRA All Stations (6 Pollutants)\n",
    "\n",
    "This notebook trains Random Forest models for **all station-pollutant combinations** in the DEFRA dataset, limited to the 6 regulatory pollutants that match LAQN for direct comparison.\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1. Load prepared data from `ml_prep_all` output.\n",
    "2. Train separate RF models for each station-pollutant target.\n",
    "3. Use checkpoint saving for overnight training safety.\n",
    "4. Evaluate using RMSE, MAE, R² for each target.\n",
    "5. Aggregate results by pollutant type.\n",
    "6. Save all models and results.\n",
    "\n",
    "## Why train separate models?\n",
    "\n",
    "From earlier experiments, `MultiOutputRegressor` with 100+ targets causes memory issues on 8GB RAM machines. Training separate models:\n",
    "\n",
    "| Approach | Memory | Time | Flexibility |\n",
    "| --- | --- | --- | --- |\n",
    "| MultiOutputRegressor | High (all at once) | Faster total | Less control |\n",
    "| Separate models | Low (one at a time) | Slower total | Can resume from checkpoint |\n",
    "\n",
    "The separate approach is safer for overnight training and allows resuming if something goes wrong.\n",
    "\n",
    "## Expected targets\n",
    "\n",
    "DEFRA 6 pollutants across ~18 stations:\n",
    "\n",
    "| Pollutant | Expected stations |\n",
    "| --- | --- |\n",
    "| NO2 | ~15 |\n",
    "| PM10 | ~8 |\n",
    "| PM2.5 | ~6 |\n",
    "| O3 | ~5 |\n",
    "| SO2 | ~2 |\n",
    "| CO | ~1 |\n",
    "\n",
    "Total: approximately 35-50 station-pollutant combinations (varies based on data availability after filtering).\n",
    "\n",
    "## Output path\n",
    "\n",
    "Results saved to: `data/defra/rf_model_all/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mandatory libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for progress tracking\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paths_md",
   "metadata": {},
   "source": [
    "## File paths\n",
    "\n",
    "Loading from the ml_prep_all output folder where the 6-pollutant prepared arrays are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths setup\n",
    "base_dir = Path.cwd().parent.parent / \"data\" / \"defra\"\n",
    "ml_prep_dir = base_dir / \"ml_prep_all\"\n",
    "\n",
    "# output folder\n",
    "rf_output_dir = base_dir / \"rf_model_all\"\n",
    "rf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# checkpoint folder\n",
    "checkpoint_dir = rf_output_dir / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Loading data from: {ml_prep_dir}\")\n",
    "print(f\"Saving results to: {rf_output_dir}\")\n",
    "print(f\"Checkpoints: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_md",
   "metadata": {},
   "source": [
    "## 1) Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "X_train_rf = np.load(ml_prep_dir / \"X_train_rf.npy\")\n",
    "X_val_rf = np.load(ml_prep_dir / \"X_val_rf.npy\")\n",
    "X_test_rf = np.load(ml_prep_dir / \"X_test_rf.npy\")\n",
    "\n",
    "y_train = np.load(ml_prep_dir / \"y_train.npy\")\n",
    "y_val = np.load(ml_prep_dir / \"y_val.npy\")\n",
    "y_test = np.load(ml_prep_dir / \"y_test.npy\")\n",
    "\n",
    "feature_names = joblib.load(ml_prep_dir / \"feature_names.joblib\")\n",
    "rf_feature_names = joblib.load(ml_prep_dir / \"rf_feature_names.joblib\")\n",
    "scaler = joblib.load(ml_prep_dir / \"scaler.joblib\")\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  X_train_rf: {X_train_rf.shape}\")\n",
    "print(f\"  X_val_rf: {X_val_rf.shape}\")\n",
    "print(f\"  X_test_rf: {X_test_rf.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "targets_md",
   "metadata": {},
   "source": [
    "## 2) Identify pollutant targets\n",
    "\n",
    "The y array contains all features including temporal columns. We need to separate the actual pollutant targets from temporal features (hour, day_of_week, month, is_weekend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identify_targets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate pollutant targets from temporal columns\n",
    "temporal_cols = ['hour', 'day_of_week', 'month', 'is_weekend']\n",
    "\n",
    "pollutant_indices = []\n",
    "pollutant_names = []\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    if name not in temporal_cols:\n",
    "        pollutant_indices.append(i)\n",
    "        pollutant_names.append(name)\n",
    "\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Temporal features: {len(temporal_cols)}\")\n",
    "print(f\"Pollutant targets: {len(pollutant_names)}\")\n",
    "\n",
    "# count by pollutant type\n",
    "print(f\"\\nTargets by pollutant type:\")\n",
    "for poll in ['NO2', 'PM25', 'PM10', 'O3', 'SO2', 'CO']:\n",
    "    count = len([n for n in pollutant_names if poll in n])\n",
    "    print(f\"  {poll}: {count} stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list_targets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all targets\n",
    "print(\"All pollutant targets:\")\n",
    "for i, name in enumerate(pollutant_names):\n",
    "    print(f\"  {i:2d}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params_md",
   "metadata": {},
   "source": [
    "## 3) Define training parameters\n",
    "\n",
    "Using memory-safe parameters optimised for 8GB RAM Mac. These were tuned in earlier single-station experiments.\n",
    "\n",
    "### Why these parameters?\n",
    "\n",
    "| Parameter | Value | Reasoning |\n",
    "| --- | --- | --- |\n",
    "| n_estimators | 100 | Balance between accuracy and memory |\n",
    "| max_depth | 15 | Limits tree size to prevent memory overflow |\n",
    "| min_samples_leaf | 2 | From hyperparameter tuning |\n",
    "| min_samples_split | 5 | From hyperparameter tuning |\n",
    "| n_jobs | 1 | Single core to prevent memory spikes |\n",
    "\n",
    "Using `n_jobs=1` instead of `-1` (all cores) because parallel tree building multiplies memory usage. For overnight training, stability matters more than speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory-safe RF parameters\n",
    "RF_PARAMS = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 15,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 5,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': 1  # single core for memory safety\n",
    "}\n",
    "\n",
    "# checkpoint frequency\n",
    "CHECKPOINT_EVERY = 10  # save progress every 10 models\n",
    "\n",
    "print(\"RF Parameters:\")\n",
    "for param, value in RF_PARAMS.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nCheckpoint every {CHECKPOINT_EVERY} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_md",
   "metadata": {},
   "source": [
    "## 4) Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate model using RMSE, MAE, R².\n",
    "    Returns dict with metrics.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'y_pred': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_md",
   "metadata": {},
   "source": [
    "## 5) Train models for all targets\n",
    "\n",
    "This is the main training loop. It trains one RF model per target and saves checkpoints periodically.\n",
    "\n",
    "### Checkpoint strategy\n",
    "\n",
    "Every 10 models, the current results are saved to a checkpoint file. If training is interrupted, you can resume from the last checkpoint.\n",
    "\n",
    "### Memory management\n",
    "\n",
    "After training each model, garbage collection runs to free memory. This prevents gradual memory buildup that could crash overnight training.\n",
    "\n",
    "### Estimated time\n",
    "\n",
    "With ~40 targets and ~2-3 minutes per model: approximately 1.5-2 hours total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing checkpoint\n",
    "checkpoint_file = checkpoint_dir / \"training_checkpoint.joblib\"\n",
    "results_so_far = []\n",
    "start_idx = 0\n",
    "\n",
    "if checkpoint_file.exists():\n",
    "    checkpoint_data = joblib.load(checkpoint_file)\n",
    "    results_so_far = checkpoint_data['results']\n",
    "    start_idx = checkpoint_data['next_idx']\n",
    "    print(f\"Resuming from checkpoint: {start_idx}/{len(pollutant_names)} models completed\")\n",
    "else:\n",
    "    print(\"Starting fresh training\")\n",
    "\n",
    "print(f\"\\nTargets to train: {len(pollutant_names) - start_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST TRAINING - DEFRA ALL STATIONS (6 POLLUTANTS)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Targets: {len(pollutant_names)}\")\n",
    "print(f\"Training samples: {X_train_rf.shape[0]:,}\")\n",
    "print(f\"Features: {X_train_rf.shape[1]:,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results = results_so_far.copy()\n",
    "total_start = time.time()\n",
    "\n",
    "for idx in range(start_idx, len(pollutant_names)):\n",
    "    target_name = pollutant_names[idx]\n",
    "    target_idx = pollutant_indices[idx]\n",
    "    \n",
    "    # extract single target\n",
    "    y_train_single = y_train[:, target_idx]\n",
    "    y_val_single = y_val[:, target_idx]\n",
    "    y_test_single = y_test[:, target_idx]\n",
    "    \n",
    "    # train model\n",
    "    print(f\"\\n[{idx+1}/{len(pollutant_names)}] Training: {target_name}\")\n",
    "    model_start = time.time()\n",
    "    \n",
    "    rf = RandomForestRegressor(**RF_PARAMS)\n",
    "    rf.fit(X_train_rf, y_train_single)\n",
    "    \n",
    "    train_time = time.time() - model_start\n",
    "    \n",
    "    # evaluate\n",
    "    train_metrics = evaluate_model(rf, X_train_rf, y_train_single)\n",
    "    val_metrics = evaluate_model(rf, X_val_rf, y_val_single)\n",
    "    test_metrics = evaluate_model(rf, X_test_rf, y_test_single)\n",
    "    \n",
    "    print(f\"  Time: {train_time:.1f}s | Test R²: {test_metrics['r2']:.4f} | RMSE: {test_metrics['rmse']:.4f}\")\n",
    "    \n",
    "    # store results\n",
    "    result = {\n",
    "        'target': target_name,\n",
    "        'target_idx': target_idx,\n",
    "        'train_time': train_time,\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_r2': test_metrics['r2']\n",
    "    }\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # save model\n",
    "    model_file = rf_output_dir / f\"rf_model_{target_name}.joblib\"\n",
    "    joblib.dump(rf, model_file)\n",
    "    \n",
    "    # save predictions\n",
    "    predictions = {\n",
    "        'y_train_actual': y_train_single,\n",
    "        'y_train_pred': train_metrics['y_pred'],\n",
    "        'y_val_actual': y_val_single,\n",
    "        'y_val_pred': val_metrics['y_pred'],\n",
    "        'y_test_actual': y_test_single,\n",
    "        'y_test_pred': test_metrics['y_pred']\n",
    "    }\n",
    "    joblib.dump(predictions, rf_output_dir / f\"predictions_{target_name}.joblib\")\n",
    "    \n",
    "    # checkpoint\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0:\n",
    "        checkpoint_data = {\n",
    "            'results': all_results,\n",
    "            'next_idx': idx + 1,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        joblib.dump(checkpoint_data, checkpoint_file)\n",
    "        print(f\"  [Checkpoint saved: {idx+1} models complete]\")\n",
    "    \n",
    "    # memory cleanup\n",
    "    del rf, train_metrics, val_metrics, test_metrics\n",
    "    gc.collect()\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_md",
   "metadata": {},
   "source": [
    "## 6) Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results dataframe\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# add pollutant type column\n",
    "def get_pollutant_type(name):\n",
    "    for poll in ['NO2', 'PM25', 'PM10', 'O3', 'SO2', 'CO']:\n",
    "        if poll in name:\n",
    "            return poll\n",
    "    return 'Other'\n",
    "\n",
    "results_df['pollutant'] = results_df['target'].apply(get_pollutant_type)\n",
    "\n",
    "# save results\n",
    "results_df.to_csv(rf_output_dir / 'all_results.csv', index=False)\n",
    "print(f\"Results saved to: {rf_output_dir / 'all_results.csv'}\")\n",
    "\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall performance\n",
    "print(\"Overall Performance Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal targets trained: {len(results_df)}\")\n",
    "print(f\"Total training time: {results_df['train_time'].sum()/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nTest Set Metrics (mean across all targets):\")\n",
    "print(f\"  RMSE: {results_df['test_rmse'].mean():.4f} (± {results_df['test_rmse'].std():.4f})\")\n",
    "print(f\"  MAE:  {results_df['test_mae'].mean():.4f} (± {results_df['test_mae'].std():.4f})\")\n",
    "print(f\"  R²:   {results_df['test_r2'].mean():.4f} (± {results_df['test_r2'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "by_pollutant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance by pollutant type\n",
    "print(\"Performance by Pollutant Type\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pollutant_summary = results_df.groupby('pollutant').agg({\n",
    "    'test_rmse': ['mean', 'std', 'count'],\n",
    "    'test_mae': 'mean',\n",
    "    'test_r2': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "pollutant_summary.columns = ['RMSE_mean', 'RMSE_std', 'n_stations', 'MAE_mean', 'R2_mean']\n",
    "pollutant_summary = pollutant_summary.sort_values('R2_mean', ascending=False)\n",
    "\n",
    "print(pollutant_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best and worst performing targets\n",
    "print(\"\\nTop 5 Best Performing Targets (by R²):\")\n",
    "print(\"-\" * 50)\n",
    "best = results_df.nlargest(5, 'test_r2')[['target', 'test_r2', 'test_rmse', 'test_mae']]\n",
    "print(best.to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 5 Worst Performing Targets (by R²):\")\n",
    "print(\"-\" * 50)\n",
    "worst = results_df.nsmallest(5, 'test_r2')[['target', 'test_r2', 'test_rmse', 'test_mae']]\n",
    "print(worst.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_md",
   "metadata": {},
   "source": [
    "## 7) Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vis_r2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² distribution by pollutant\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "pollutants = ['NO2', 'PM10', 'PM25', 'O3', 'SO2', 'CO']\n",
    "colors = ['steelblue', 'coral', 'seagreen', 'gold', 'purple', 'brown']\n",
    "\n",
    "data_to_plot = []\n",
    "labels = []\n",
    "for poll in pollutants:\n",
    "    poll_data = results_df[results_df['pollutant'] == poll]['test_r2']\n",
    "    if len(poll_data) > 0:\n",
    "        data_to_plot.append(poll_data.values)\n",
    "        labels.append(f\"{poll}\\n(n={len(poll_data)})\")\n",
    "\n",
    "bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors[:len(data_to_plot)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax.set_ylabel('Test R²')\n",
    "ax.set_title('DEFRA Random Forest Performance by Pollutant Type')\n",
    "ax.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='R²=0.8 threshold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(rf_output_dir / 'r2_by_pollutant.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vis_scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot: actual vs predicted for best model\n",
    "best_target = results_df.loc[results_df['test_r2'].idxmax(), 'target']\n",
    "best_preds = joblib.load(rf_output_dir / f\"predictions_{best_target}.joblib\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.scatter(best_preds['y_test_actual'], best_preds['y_test_pred'], \n",
    "           alpha=0.3, s=10, c='steelblue')\n",
    "\n",
    "# perfect prediction line\n",
    "lims = [0, 1]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.8, label='Perfect prediction')\n",
    "\n",
    "ax.set_xlabel('Actual (normalised)')\n",
    "ax.set_ylabel('Predicted (normalised)')\n",
    "ax.set_title(f'Best Model: {best_target}')\n",
    "ax.legend()\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(rf_output_dir / 'best_model_scatter.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_md",
   "metadata": {},
   "source": [
    "## 8) Training summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DEFRA RANDOM FOREST TRAINING SUMMARY (6 POLLUTANTS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training samples: {X_train_rf.shape[0]:,}\")\n",
    "print(f\"  Validation samples: {X_val_rf.shape[0]:,}\")\n",
    "print(f\"  Test samples: {X_test_rf.shape[0]:,}\")\n",
    "print(f\"  Features: {X_train_rf.shape[1]:,}\")\n",
    "\n",
    "print(f\"\\nModels trained: {len(results_df)}\")\n",
    "print(f\"Total training time: {results_df['train_time'].sum()/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nOverall Test Performance:\")\n",
    "print(f\"  Mean R²:   {results_df['test_r2'].mean():.4f}\")\n",
    "print(f\"  Mean RMSE: {results_df['test_rmse'].mean():.4f}\")\n",
    "print(f\"  Mean MAE:  {results_df['test_mae'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nBest model: {best_target}\")\n",
    "print(f\"  R²: {results_df['test_r2'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nFiles saved to: {rf_output_dir}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_md",
   "metadata": {},
   "source": [
    "## 9) Cleanup checkpoint (optional)\n",
    "\n",
    "Run this cell after training completes successfully to remove the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove checkpoint after successful completion\n",
    "if checkpoint_file.exists():\n",
    "    checkpoint_file.unlink()\n",
    "    print(\"Checkpoint file removed.\")\n",
    "else:\n",
    "    print(\"No checkpoint file to remove.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_md",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. Compare these results with LAQN all-stations RF to see dataset differences.\n",
    "2. Run CNN training on the same targets for model comparison.\n",
    "3. Analyse which stations/pollutants perform best and why.\n",
    "4. Create comprehensive results tables for dissertation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_defra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
