{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f197f750",
   "metadata": {},
   "source": [
    "# CNN model training\n",
    "\n",
    "## LAQN air quality prediction\n",
    "\n",
    "## what this notebook does?\n",
    "\n",
    "this notebook trains a convolutional neural network (CNN) to predict nitrogen dioxide (NO2) levels using the same LAQN data that was used for random forest. the goal is to compare CNN performance against the random forest baseline (test R² = 0.814)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40234ea0",
   "metadata": {},
   "source": [
    "## 1. setup and imports\n",
    "\n",
    "First, as usual import everything. \n",
    "tensorflow/keras is the deep learning library. \n",
    "numpy handles arrays. \n",
    "matplotlib and seaborn make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5baaa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# scikit-learn for metrics r^2, MSE, MAE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# tensorflow and keras for neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import platform\n",
    "\n",
    "# adding tensorflow keras models, layers optimizer adam  for cnn model section 4\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca50154",
   "metadata": {},
   "source": [
    "        operating system: Darwin\n",
    "        processor: arm\n",
    "        tensorflow version: 2.16.2\n",
    "        built with CUDA: False\n",
    "\n",
    "        available devices:\n",
    "        - PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74981b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/ml_prep\n",
      "saving outputs to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model\n"
     ]
    }
   ],
   "source": [
    "# set paths update this to match your folder structure using cwd\n",
    "base_dir = Path.cwd().parent.parent / 'data' / 'laqn'\n",
    "data_dir = base_dir / 'ml_prep' # where ml_prep saved the arrays\n",
    "output_dir = base_dir / 'cnn_model' # where we save CNN outputs \n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'loading data from: {data_dir}')\n",
    "print(f'saving outputs to: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca668bd",
   "metadata": {},
   "source": [
    "\n",
    "### GPU availability\n",
    "\n",
    "- TensorFlow code, and tf.keras models will transparently run on a single GPU with no code changes required.\n",
    "> Note: Use tf.config.list_physical_devices('GPU') to confirm that TensorFlow is using the GPU.\n",
    "- The simplest way to run on multiple GPUs, on one or many machines, is using Distribution Strategies.\n",
    "\n",
    "Source: *Use a GPU :  Tensorflow Core* (no date) *TensorFlow*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc46095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU found, using CPU (training will be slower but still works)\n"
     ]
    }
   ],
   "source": [
    "# checks gpu availability taken from documentation.\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'GPU available: {len(gpus)} device(s)')\n",
    "    for gpu in gpus:\n",
    "        print(f'  - {gpu.name}')\n",
    "else:\n",
    "    print('no GPU found, using CPU (training will be slower but still works)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137c237",
   "metadata": {},
   "source": [
    "## 2. load prepared data\n",
    "\n",
    "the data was prepared in ml_prep notebook. it created sequences where each sample has 12 hours of history to predict the next hour. this is the same data random forest used, just in 3D shape instead of flattened.\n",
    "\n",
    "### why 3D data for CNN?\n",
    "\n",
    "random forest needs flat 2D data: (samples, features). CNN needs 3D data: (samples, timesteps, features). the 3D shape lets CNN learn patterns across time, not just treat each timestep as an independent feature.\n",
    "\n",
    "think of it like this:\n",
    "- 2D (random forest): each row is a list of 468 numbers with no structure\n",
    "- 3D (CNN): each sample is a 12×39 grid where rows are hours and columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7be915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded successfully\n",
      "\n",
      "shapes:\n",
      "X_train:(9946, 12, 39)\n",
      "X_val:(2131, 12, 39)\n",
      "X_test:(2132, 12, 39)\n",
      "y_train:(9946, 39)\n",
      "y_val:(2131, 39)\n",
      "y_test:(2132, 39)\n"
     ]
    }
   ],
   "source": [
    "# load the 3d sequences for cnn\n",
    "X_train = np.load(data_dir / 'X_train.npy')\n",
    "X_val = np.load(data_dir / 'X_val.npy')\n",
    "X_test = np.load(data_dir / 'X_test.npy')\n",
    "\n",
    "y_train = np.load(data_dir / 'y_train.npy')\n",
    "y_val = np.load(data_dir / 'y_val.npy')\n",
    "y_test = np.load(data_dir / 'y_test.npy')\n",
    "\n",
    "#load feature_names and scaler\n",
    "feature_names = joblib.load(data_dir / 'feature_names.joblib')\n",
    "scaler = joblib.load(data_dir / 'scaler.joblib')\n",
    "\n",
    "print('data loaded successfully')\n",
    "print(f'\\nshapes:')\n",
    "print(f'X_train:{X_train.shape}')\n",
    "print(f'X_val:{X_val.shape}')\n",
    "print(f'X_test:{X_test.shape}')\n",
    "print(f'y_train:{y_train.shape}')\n",
    "print(f'y_val:{y_val.shape}')\n",
    "print(f'y_test:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44896cc7",
   "metadata": {},
   "source": [
    "    data loaded successfully\n",
    "\n",
    "    shapes:\n",
    "    X_train:(9946, 12, 39)\n",
    "    X_val:(2131, 12, 39)\n",
    "    X_test:(2132, 12, 39)\n",
    "    y_train:(9946, 39)\n",
    "    y_val:(2131, 39)\n",
    "    y_test:(2132, 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096fba6",
   "metadata": {},
   "source": [
    "### understanding the shapes\n",
    "\n",
    "X_train shape is (9946, 12, 39). this means:\n",
    "\n",
    "| dimension | value | what it represents |\n",
    "|-----------|-------|-------------------|\n",
    "| samples | 9,946 | individual training examples |\n",
    "| timesteps | 12 | hours of history per sample |\n",
    "| features | 39 | 24 NO2 + 8 PM10 + 3 O3 + 4 temporal |\n",
    "\n",
    "y_train shape is (9946, 39). the model predicts all 39 features for the next hour.\n",
    "\n",
    "for fair comparison with random forest, I focus on EN5_NO2 (first column) as the single target. this is the same station used in the RF training report. EN5 had the highest data coverage (99.6%) which makes it the most reliable target for evaluation.\n",
    "\n",
    "the 3D shape is the key difference from random forest:\n",
    "- random forest got flattened 2D: (9946, 468) where 468 = 12 × 39\n",
    "- CNN keeps the 3D structure: (9946, 12, 39)\n",
    "\n",
    "why does this matter? CNN can learn that hour 1 connects to hour 2 connects to hour 3. random forest just sees 468 separate numbers with no time relationship. this is why CNN might capture temporal patterns better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ecacd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target feature: EN5_NO2\n",
      "y_train_single shape: (9946,)\n",
      "y_val_single shape: (2131,)\n",
      "y_test_single shape: (2132,)\n"
     ]
    }
   ],
   "source": [
    "# select single target as RF EN5_NO2\n",
    "target_idx = 0\n",
    "target_name = feature_names[target_idx]\n",
    "\n",
    "y_train_single = y_train[:, target_idx]\n",
    "y_val_single = y_val[:, target_idx]\n",
    "y_test_single = y_test[:, target_idx]\n",
    "\n",
    "print(f'target feature: {target_name}')\n",
    "print(f'y_train_single shape: {y_train_single.shape}')\n",
    "print(f'y_val_single shape: {y_val_single.shape}')\n",
    "print(f'y_test_single shape: {y_test_single.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f1949",
   "metadata": {},
   "source": [
    "    target feature: EN5_NO2\n",
    "    y_train_single shape: (9946,)\n",
    "    y_val_single shape: (2131,)\n",
    "    y_test_single shape: (2132,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109ce08",
   "metadata": {},
   "source": [
    "## 3. understanding CNN for time series\n",
    "\n",
    "before building the model, let me explain what a CNN actually does. this helps understand why certain choices are made.\n",
    "\n",
    "### what is a convolutional neural network?\n",
    "CNN  designed for image recognition. It slides a small \"filter\" (also called kernel) across the input to detect patterns. The patterns can be for images, this finds edges, shapes, textures. For time series, it finds temporal patterns. (Gilik, A., Ogrenci, A.S. and Ozmen, A. (2021a) ‘Air quality prediction using CNN+LSTM-based hybrid deep learning architecture’)\n",
    "\n",
    "### why I decided to go with CNN for timeseries air quality?\n",
    "Gilik, A., Ogrenci, A.S AND Ozmen, A(2021) on their Air Quality Prediction Using CNN LSTM-based hybrid deep learning architecture found that CNN can capture local temporal dependencies in pollution data. pollution at hour t depends heavily on hours t-1, t-2, t-3. CNN's sliding filter naturally captures this. I will be add the graph of this to my dissertation.\n",
    "According to this finding it is makes more sense CNN's local pattern detection should capture this same relationship but can also learn multi-hour patterns that RF might miss.\n",
    "\n",
    "source: \n",
    "\n",
    "Gilik, A., Ogrenci, A.S. and Ozmen, A. (2021a) ‘Air quality prediction using CNN+LSTM-based hybrid deep learning architecture’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750e17e",
   "metadata": {},
   "source": [
    "## 4. build baseline CNN model\n",
    "\n",
    "Starting a simple architecture. \n",
    "The logic is: simple first, add complexity only if neeeds. \n",
    "I will be follow Hands-on machine learning... Géron's book.\n",
    "\n",
    "### architecture choices explained\n",
    "\n",
    "| layer | what it does | why we use it |\n",
    "|-------|--------------|---------------|\n",
    "| Conv1D | extracts temporal patterns | learns what combinations of hours predict next hour |\n",
    "| MaxPooling1D | reduces sequence length | keeps important patterns, reduces computation |\n",
    "| Flatten | converts 2D to 1D | prepares for Dense layer |\n",
    "| Dense | combines patterns | learns how to weight different patterns |\n",
    "| Dropout | randomly turns off neurons | prevents overfitting |\n",
    "\n",
    "### hyperparameters\n",
    "\n",
    "- filters: how many different patterns to learn (like having multiple detectors)\n",
    "- kernel_size: how many timesteps each filter looks at. Conv1D(14, kernel_size=1)\n",
    "- pool_size: how much to compress after convolution\n",
    "- dropout rate: fraction of neurons to turn off during training\n",
    "\n",
    "source: https://www.geeksforgeeks.org/deep-learning/adam-optimizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "689d6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(timesteps, features):\n",
    "    \"\"\"\n",
    "    Build a 1D CNN for time series prediction.\n",
    "    Based on Géron (2023, ch. 15) approach.\n",
    "    \n",
    "    parames:\n",
    "        timesteps: number of historical hours 12 hours\n",
    "        features: number of input features 39 \n",
    "    \n",
    "    returns:\n",
    "        compiled keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # input layer explicit input shape\n",
    "        layers.Input(shape=(timesteps, features)),\n",
    "        \n",
    "        # first conv layer with stride=2 to downsample. Geron (2023)\"the convolutional layer may help detect longer patterns\"\n",
    "        layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            activation='relu',\n",
    "            padding='causal'  \n",
    "        ),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        \n",
    "        # second conv layer\n",
    "        layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=4,\n",
    "            activation='relu',\n",
    "            padding='causal'\n",
    "        ),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # flatten and dense for final prediction\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # output layer single value for EN5_NO2 prediction Conv1D(filters=1, kernel_size=1) is equivalent to Dense(1) \n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # used Adam optimiser here it's an efficient, robust, algorithm that combines momentum and adaptive learning rates.https://www.geeksforgeeks.org/deep-learning/adam-optimizer/\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "793aa747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building CNN with:\n",
      "  timesteps: 12\n",
      "  features: 39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m5,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m9,650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,853</span> (73.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,853\u001b[0m (73.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,853</span> (73.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,853\u001b[0m (73.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get dimensions from data\n",
    "timesteps = X_train.shape[1]  # 12\n",
    "n_features = X_train.shape[2]  # 39\n",
    "\n",
    "print(f'building CNN with:')\n",
    "print(f'  timesteps: {timesteps}')\n",
    "print(f'  features: {n_features}')\n",
    "\n",
    "# build the model\n",
    "model = cnn_model(timesteps, n_features)\n",
    "\n",
    "# show architecture summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b5001",
   "metadata": {},
   "source": [
    "    building CNN with:\n",
    "    timesteps: 12\n",
    "    features: 39  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df7c8a",
   "metadata": {},
   "source": [
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a2e82",
   "metadata": {},
   "source": [
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ conv1d (Conv1D)                 │ (None, 6, 32)          │         5,024 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout (Dropout)               │ (None, 6, 32)          │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv1d_1 (Conv1D)               │ (None, 6, 32)          │         4,128 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout_1 (Dropout)             │ (None, 6, 32)          │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ flatten (Flatten)               │ (None, 192)            │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense (Dense)                   │ (None, 50)             │         9,650 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout_2 (Dropout)             │ (None, 50)             │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense_1 (Dense)                 │ (None, 1)              │            51 │"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d59ef",
   "metadata": {},
   "source": [
    "    Total params: 18,853 (73.64 KB)\n",
    "    Trainable params: 18,853 (73.64 KB)\n",
    "    Non-trainable params: 0 (0.00 B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
