{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f197f750",
   "metadata": {},
   "source": [
    "# CNN model training\n",
    "\n",
    "## LAQN air quality prediction\n",
    "\n",
    "## what this notebook does?\n",
    "\n",
    "this notebook trains a convolutional neural network (CNN) to predict nitrogen dioxide (NO2) levels using the same LAQN data that was used for random forest. the goal is to compare CNN performance against the random forest baseline (test R² = 0.814)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40234ea0",
   "metadata": {},
   "source": [
    "## 1. setup and imports\n",
    "\n",
    "First, as usual import everything. \n",
    "tensorflow/keras is the deep learning library. \n",
    "numpy handles arrays. \n",
    "matplotlib and seaborn make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5baaa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# scikit-learn for metrics r^2, MSE, MAE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# tensorflow and keras for neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import platform\n",
    "\n",
    "# adding tensorflow keras models, layers optimizer adam  for cnn model section 4\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#callbacks early stopping modules section 5\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca50154",
   "metadata": {},
   "source": [
    "        operating system: Darwin\n",
    "        processor: arm\n",
    "        tensorflow version: 2.16.2\n",
    "        built with CUDA: False\n",
    "\n",
    "        available devices:\n",
    "        - PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74981b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/ml_prep\n",
      "saving outputs to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model\n"
     ]
    }
   ],
   "source": [
    "# set paths update this to match your folder structure using cwd\n",
    "base_dir = Path.cwd().parent.parent / 'data' / 'laqn'\n",
    "data_dir = base_dir / 'ml_prep' # where ml_prep saved the arrays\n",
    "output_dir = base_dir / 'cnn_model' # where we save CNN outputs \n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'loading data from: {data_dir}')\n",
    "print(f'saving outputs to: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca668bd",
   "metadata": {},
   "source": [
    "\n",
    "### GPU availability\n",
    "\n",
    "- TensorFlow code, and tf.keras models will transparently run on a single GPU with no code changes required.\n",
    "> Note: Use tf.config.list_physical_devices('GPU') to confirm that TensorFlow is using the GPU.\n",
    "- The simplest way to run on multiple GPUs, on one or many machines, is using Distribution Strategies.\n",
    "\n",
    "Source: *Use a GPU :  Tensorflow Core* (no date) *TensorFlow*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc46095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU found, using CPU (training will be slower but still works)\n"
     ]
    }
   ],
   "source": [
    "# checks gpu availability taken from documentation.\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'GPU available: {len(gpus)} device(s)')\n",
    "    for gpu in gpus:\n",
    "        print(f'  - {gpu.name}')\n",
    "else:\n",
    "    print('no GPU found, using CPU (training will be slower but still works)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137c237",
   "metadata": {},
   "source": [
    "## 2. load prepared data\n",
    "\n",
    "the data was prepared in ml_prep notebook. it created sequences where each sample has 12 hours of history to predict the next hour. this is the same data random forest used, just in 3D shape instead of flattened.\n",
    "\n",
    "### why 3D data for CNN?\n",
    "\n",
    "random forest needs flat 2D data: (samples, features). CNN needs 3D data: (samples, timesteps, features). the 3D shape lets CNN learn patterns across time, not just treat each timestep as an independent feature.\n",
    "\n",
    "think of it like this:\n",
    "- 2D (random forest): each row is a list of 468 numbers with no structure\n",
    "- 3D (CNN): each sample is a 12×39 grid where rows are hours and columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7be915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded successfully\n",
      "\n",
      "shapes:\n",
      "X_train:(9946, 12, 39)\n",
      "X_val:(2131, 12, 39)\n",
      "X_test:(2132, 12, 39)\n",
      "y_train:(9946, 39)\n",
      "y_val:(2131, 39)\n",
      "y_test:(2132, 39)\n"
     ]
    }
   ],
   "source": [
    "# load the 3d sequences for cnn\n",
    "X_train = np.load(data_dir / 'X_train.npy')\n",
    "X_val = np.load(data_dir / 'X_val.npy')\n",
    "X_test = np.load(data_dir / 'X_test.npy')\n",
    "\n",
    "y_train = np.load(data_dir / 'y_train.npy')\n",
    "y_val = np.load(data_dir / 'y_val.npy')\n",
    "y_test = np.load(data_dir / 'y_test.npy')\n",
    "\n",
    "#load feature_names and scaler\n",
    "feature_names = joblib.load(data_dir / 'feature_names.joblib')\n",
    "scaler = joblib.load(data_dir / 'scaler.joblib')\n",
    "\n",
    "print('data loaded successfully')\n",
    "print(f'\\nshapes:')\n",
    "print(f'X_train:{X_train.shape}')\n",
    "print(f'X_val:{X_val.shape}')\n",
    "print(f'X_test:{X_test.shape}')\n",
    "print(f'y_train:{y_train.shape}')\n",
    "print(f'y_val:{y_val.shape}')\n",
    "print(f'y_test:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44896cc7",
   "metadata": {},
   "source": [
    "    data loaded successfully\n",
    "\n",
    "    shapes:\n",
    "    X_train:(9946, 12, 39)\n",
    "    X_val:(2131, 12, 39)\n",
    "    X_test:(2132, 12, 39)\n",
    "    y_train:(9946, 39)\n",
    "    y_val:(2131, 39)\n",
    "    y_test:(2132, 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096fba6",
   "metadata": {},
   "source": [
    "### understanding the shapes\n",
    "\n",
    "X_train shape is (9946, 12, 39). this means:\n",
    "\n",
    "| dimension | value | what it represents |\n",
    "|-----------|-------|-------------------|\n",
    "| samples | 9,946 | individual training examples |\n",
    "| timesteps | 12 | hours of history per sample |\n",
    "| features | 39 | 24 NO2 + 8 PM10 + 3 O3 + 4 temporal |\n",
    "\n",
    "y_train shape is (9946, 39). the model predicts all 39 features for the next hour.\n",
    "\n",
    "for fair comparison with random forest, I focus on EN5_NO2 (first column) as the single target. this is the same station used in the RF training report. EN5 had the highest data coverage (99.6%) which makes it the most reliable target for evaluation.\n",
    "\n",
    "the 3D shape is the key difference from random forest:\n",
    "- random forest got flattened 2D: (9946, 468) where 468 = 12 × 39\n",
    "- CNN keeps the 3D structure: (9946, 12, 39)\n",
    "\n",
    "why does this matter? CNN can learn that hour 1 connects to hour 2 connects to hour 3. random forest just sees 468 separate numbers with no time relationship. this is why CNN might capture temporal patterns better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ecacd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target feature: EN5_NO2\n",
      "y_train_single shape: (9946,)\n",
      "y_val_single shape: (2131,)\n",
      "y_test_single shape: (2132,)\n"
     ]
    }
   ],
   "source": [
    "# select single target as RF EN5_NO2\n",
    "target_idx = 0\n",
    "target_name = feature_names[target_idx]\n",
    "\n",
    "y_train_single = y_train[:, target_idx]\n",
    "y_val_single = y_val[:, target_idx]\n",
    "y_test_single = y_test[:, target_idx]\n",
    "\n",
    "print(f'target feature: {target_name}')\n",
    "print(f'y_train_single shape: {y_train_single.shape}')\n",
    "print(f'y_val_single shape: {y_val_single.shape}')\n",
    "print(f'y_test_single shape: {y_test_single.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f1949",
   "metadata": {},
   "source": [
    "    target feature: EN5_NO2\n",
    "    y_train_single shape: (9946,)\n",
    "    y_val_single shape: (2131,)\n",
    "    y_test_single shape: (2132,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109ce08",
   "metadata": {},
   "source": [
    "## 3. understanding CNN for time series\n",
    "\n",
    "before building the model, let me explain what a CNN actually does. this helps understand why certain choices are made.\n",
    "\n",
    "### what is a convolutional neural network?\n",
    "CNN  designed for image recognition. It slides a small \"filter\" (also called kernel) across the input to detect patterns. The patterns can be for images, this finds edges, shapes, textures. For time series, it finds temporal patterns. (Gilik, A., Ogrenci, A.S. and Ozmen, A. (2021a) ‘Air quality prediction using CNN+LSTM-based hybrid deep learning architecture’)\n",
    "\n",
    "### why I decided to go with CNN for timeseries air quality?\n",
    "Gilik, A., Ogrenci, A.S AND Ozmen, A(2021) on their Air Quality Prediction Using CNN LSTM-based hybrid deep learning architecture found that CNN can capture local temporal dependencies in pollution data. pollution at hour t depends heavily on hours t-1, t-2, t-3. CNN's sliding filter naturally captures this. I will be add the graph of this to my dissertation.\n",
    "According to this finding it is makes more sense CNN's local pattern detection should capture this same relationship but can also learn multi-hour patterns that RF might miss.\n",
    "\n",
    "source: \n",
    "\n",
    "Gilik, A., Ogrenci, A.S. and Ozmen, A. (2021a) ‘Air quality prediction using CNN+LSTM-based hybrid deep learning architecture’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750e17e",
   "metadata": {},
   "source": [
    "## 4. build baseline CNN model\n",
    "\n",
    "Starting a simple architecture. \n",
    "The logic is: simple first, add complexity only if neeeds. \n",
    "I will be follow Hands-on machine learning... Géron's book.\n",
    "\n",
    "### architecture choices explained\n",
    "\n",
    "| layer | what it does | why we use it |\n",
    "|-------|--------------|---------------|\n",
    "| Conv1D | extracts temporal patterns | learns what combinations of hours predict next hour |\n",
    "| MaxPooling1D | reduces sequence length | keeps important patterns, reduces computation |\n",
    "| Flatten | converts 2D to 1D | prepares for Dense layer |\n",
    "| Dense | combines patterns | learns how to weight different patterns |\n",
    "| Dropout | randomly turns off neurons | prevents overfitting |\n",
    "\n",
    "### hyperparameters\n",
    "\n",
    "- filters: how many different patterns to learn (like having multiple detectors)\n",
    "- kernel_size: how many timesteps each filter looks at. Conv1D(14, kernel_size=1)\n",
    "- pool_size: how much to compress after convolution\n",
    "- dropout rate: fraction of neurons to turn off during training\n",
    "\n",
    "source: https://www.geeksforgeeks.org/deep-learning/adam-optimizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "689d6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(timesteps, features):\n",
    "    \"\"\"\n",
    "    Build a 1D CNN for time series prediction.\n",
    "    Based on Géron (2023, ch. 15) approach.\n",
    "    \n",
    "    parames:\n",
    "        timesteps: number of historical hours 12 hours\n",
    "        features: number of input features 39 \n",
    "    \n",
    "    returns:\n",
    "        compiled keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # input layer explicit input shape\n",
    "        layers.Input(shape=(timesteps, features)),\n",
    "        \n",
    "        # first conv layer with stride=2 to downsample. Geron (2023)\"the convolutional layer may help detect longer patterns\"\n",
    "        layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            activation='relu',\n",
    "            padding='causal'  \n",
    "        ),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        \n",
    "        # second conv layer\n",
    "        layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=4,\n",
    "            activation='relu',\n",
    "            padding='causal'\n",
    "        ),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # flatten and dense for final prediction\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # output layer single value for EN5_NO2 prediction Conv1D(filters=1, kernel_size=1) is equivalent to Dense(1) \n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # used Adam optimiser here it's an efficient, robust, algorithm that combines momentum and adaptive learning rates.https://www.geeksforgeeks.org/deep-learning/adam-optimizer/\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "793aa747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building CNN with:\n",
      "  timesteps: 12\n",
      "  features: 39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m5,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m9,650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,853</span> (73.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,853\u001b[0m (73.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,853</span> (73.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,853\u001b[0m (73.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get dimensions from data\n",
    "timesteps = X_train.shape[1]  # 12\n",
    "n_features = X_train.shape[2]  # 39\n",
    "\n",
    "print(f'building CNN with:')\n",
    "print(f'  timesteps: {timesteps}')\n",
    "print(f'  features: {n_features}')\n",
    "\n",
    "# build the model\n",
    "model = cnn_model(timesteps, n_features)\n",
    "\n",
    "# show architecture summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b5001",
   "metadata": {},
   "source": [
    "    building CNN with:\n",
    "    timesteps: 12\n",
    "    features: 39  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df7c8a",
   "metadata": {},
   "source": [
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a2e82",
   "metadata": {},
   "source": [
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ conv1d (Conv1D)                 │ (None, 6, 32)          │         5,024 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout (Dropout)               │ (None, 6, 32)          │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv1d_1 (Conv1D)               │ (None, 6, 32)          │         4,128 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout_1 (Dropout)             │ (None, 6, 32)          │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ flatten (Flatten)               │ (None, 192)            │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense (Dense)                   │ (None, 50)             │         9,650 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout_2 (Dropout)             │ (None, 50)             │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense_1 (Dense)                 │ (None, 1)              │            51 │"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d59ef",
   "metadata": {},
   "source": [
    "    Total params: 18,853 (73.64 KB)\n",
    "    Trainable params: 18,853 (73.64 KB)\n",
    "    Non-trainable params: 0 (0.00 B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12239c",
   "metadata": {},
   "source": [
    "### understanding the summary\n",
    "\n",
    "the summary shows each layer, its output shape, and parameter count.\n",
    "\n",
    "- **param #**: number of learnable weights. more parameters = more capacity to learn, but also more risk of overfitting\n",
    "- **output shape**: (None, timesteps, filters). None is batch size, determined at runtime\n",
    "- **total params**: all weights the model will learn during training\n",
    "\n",
    "the model should have roughly 10,000-50,000 parameters for this task. too few = underfitting, too many = overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497cc84",
   "metadata": {},
   "source": [
    "## 5. set up training callbacks\n",
    "\n",
    "callbacks are functions that run during training. they help prevent overfitting and save the best model.\n",
    "\n",
    "> Another way is to use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and if you set restore_best_weights=True it will roll back to the best model at the end of training. You can combine both callbacks to save checkpoints of your model in case your computer crashes, and interrupt training early when there is no more progress, to avoid wasting time and resources and to reduce overfitting:\n",
    "Geron,2023 hands on machine learning\n",
    "\n",
    "```\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "history = model.fit([...], callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "```\n",
    "\n",
    "### early stopping\n",
    "\n",
    "stops training when validation loss stops improving. from keras documentation: \"stop training when a monitored metric has stopped improving.\"\n",
    "\n",
    "source: https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "> keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", #at hands on machine learning book géron uses val_mae which is mix of mse and mae so val_mae gives him consistent metric.\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=0,\n",
    ")\n",
    "\n",
    "### model checkpoint\n",
    "\n",
    "saves the model whenever validation loss improves. this way we keep the best version even if training continues and overfits.\n",
    "\n",
    "### reduce learning rate on plateau\n",
    "\n",
    "if validation loss stops improving, reduce learning rate. smaller steps help find the minimum when close to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c46459cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callbacks configured:\n",
      "  - early stopping (patience=15)\n",
      "  - model checkpoint (saves best)\n",
      "  - reduce LR on plateau (factor=0.5, patience=5)\n"
     ]
    }
   ],
   "source": [
    "# define callbacks\n",
    "callbacks = [\n",
    "    # stop if no improvement for 15 iterations (epochs)\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', # monitor validation loss because it is directly related to generalisation as suggest on keras doc\n",
    "        patience=15,     # gives model time to escape local minima\n",
    "\n",
    "        restore_best_weights=True,     # restore_best_weights loads the best model at the end\n",
    "\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # save best model to file\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(output_dir / 'best_cnn_model.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5, # reduce learning rata if its stucks factor=0.5 means halve the learning rate\n",
    "        patience=5, #means wait 5 epochs before reducing\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print('callbacks configured:')\n",
    "print('  - early stopping (patience=15)')\n",
    "print('  - model checkpoint (saves best)')\n",
    "print('  - reduce LR on plateau (factor=0.5, patience=5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9a527",
   "metadata": {},
   "source": [
    "    callbacks configured:\n",
    "    - early stopping (patience=15)\n",
    "    - model checkpoint (saves best)\n",
    "    - reduce LR on plateau (factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6cafe",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Model Training\n",
    "\n",
    "Fitting the model to training data. Keras uses mini-batch gradient descent:\n",
    "\n",
    "```\n",
    "#keras/models/model_training/fit method\n",
    "Model.fit(\n",
    "    x=None,\n",
    "    y=None,\n",
    "    batch_size=None,\n",
    "    epochs=1,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=None,\n",
    "    validation_split=0.0,\n",
    "    validation_data=None,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=None,\n",
    "    validation_freq=1,\n",
    ")\n",
    "```\n",
    "\n",
    "1. take a batch of samples = 32 is a suggested default.\n",
    "2. compute predictions \n",
    "3. compute loss (how wrong predictions are)\n",
    "4. update weights to reduce loss\n",
    "5. repeat for all batches = 1 epoch epoch will be set as 100 \n",
    "6. repeat for many epochs\n",
    "\n",
    "### hyperparameters for training\n",
    "\n",
    "| parameter | value | explanation |\n",
    "|-----------|-------|-------------|\n",
    "| batch_size | 32 | samples per gradient update |\n",
    "| epochs | 100 | maximum passes through data |\n",
    "\n",
    "> batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your input data x is a keras.utils.PyDataset, tf.data.Dataset, torch.utils.data.DataLoader or Python generator function since they generate batches.\n",
    "\n",
    "> epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.\n",
    "\n",
    "Setting epochs to 100 as maximum. early stopping callback will stop training before this if validation loss stops improving.\n",
    "\n",
    "Sourcxe: Team, K. (no date b) Keras Documentation: Model training apis, Keras. Available at: https://keras.io/api/models/model_training_apis/#fit-method (Accessed: 25 December 2025). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "741aae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Max epochs: 100\n",
      "Training samples: 9946\n",
      "Validation samples: 2131\n",
      "Epoch 1/100\n",
      "\u001b[1m294/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.0193 - mae: 0.1011\n",
      "Epoch 1: val_loss improved from None to 0.01201, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\n",
      "Epoch 1: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0138 - mae: 0.0868 - val_loss: 0.0120 - val_mae: 0.0841 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m282/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0099 - mae: 0.0742\n",
      "Epoch 2: val_loss improved from 0.01201 to 0.00976, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\n",
      "Epoch 2: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0095 - mae: 0.0722 - val_loss: 0.0098 - val_mae: 0.0748 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m267/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0080 - mae: 0.0660\n",
      "Epoch 3: val_loss did not improve from 0.00976\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0081 - mae: 0.0654 - val_loss: 0.0102 - val_mae: 0.0750 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m270/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 0.0079 - mae: 0.0655\n",
      "Epoch 4: val_loss did not improve from 0.00976\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0076 - mae: 0.0638 - val_loss: 0.0102 - val_mae: 0.0739 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m302/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0073 - mae: 0.0614\n",
      "Epoch 5: val_loss did not improve from 0.00976\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - mae: 0.0617 - val_loss: 0.0167 - val_mae: 0.0957 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m283/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.0073 - mae: 0.0625\n",
      "Epoch 6: val_loss did not improve from 0.00976\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0068 - mae: 0.0599 - val_loss: 0.0102 - val_mae: 0.0738 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m279/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.0066 - mae: 0.0584\n",
      "Epoch 7: val_loss improved from 0.00976 to 0.00933, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\n",
      "Epoch 7: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0065 - mae: 0.0584 - val_loss: 0.0093 - val_mae: 0.0713 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m305/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0063 - mae: 0.0569  \n",
      "Epoch 8: val_loss improved from 0.00933 to 0.00924, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\n",
      "Epoch 8: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0064 - mae: 0.0575 - val_loss: 0.0092 - val_mae: 0.0693 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m286/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0061 - mae: 0.0569\n",
      "Epoch 9: val_loss did not improve from 0.00924\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0064 - mae: 0.0577 - val_loss: 0.0110 - val_mae: 0.0761 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m272/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 0.0059 - mae: 0.0563\n",
      "Epoch 10: val_loss did not improve from 0.00924\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0060 - mae: 0.0562 - val_loss: 0.0116 - val_mae: 0.0783 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m269/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0062 - mae: 0.0578\n",
      "Epoch 11: val_loss improved from 0.00924 to 0.00783, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\n",
      "Epoch 11: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0060 - mae: 0.0562 - val_loss: 0.0078 - val_mae: 0.0636 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m270/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 0.0063 - mae: 0.0567\n",
      "Epoch 12: val_loss did not improve from 0.00783\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0060 - mae: 0.0558 - val_loss: 0.0105 - val_mae: 0.0744 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m279/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0057 - mae: 0.0546\n",
      "Epoch 13: val_loss did not improve from 0.00783\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0059 - mae: 0.0553 - val_loss: 0.0086 - val_mae: 0.0684 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m262/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 0.0057 - mae: 0.0549\n",
      "Epoch 14: val_loss did not improve from 0.00783\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0058 - mae: 0.0552 - val_loss: 0.0102 - val_mae: 0.0733 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m263/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 0.0058 - mae: 0.0548\n",
      "Epoch 15: val_loss improved from 0.00783 to 0.00770, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\n",
      "Epoch 15: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0057 - mae: 0.0546 - val_loss: 0.0077 - val_mae: 0.0634 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m269/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0056 - mae: 0.0543\n",
      "Epoch 16: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0058 - mae: 0.0549 - val_loss: 0.0080 - val_mae: 0.0648 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m264/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0053 - mae: 0.0529\n",
      "Epoch 17: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0055 - mae: 0.0537 - val_loss: 0.0077 - val_mae: 0.0648 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 0.0054 - mae: 0.0534\n",
      "Epoch 18: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0054 - mae: 0.0536 - val_loss: 0.0085 - val_mae: 0.0676 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m271/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 0.0054 - mae: 0.0537\n",
      "Epoch 19: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0055 - mae: 0.0537 - val_loss: 0.0092 - val_mae: 0.0701 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 0.0057 - mae: 0.0548\n",
      "Epoch 20: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0055 - mae: 0.0541 - val_loss: 0.0091 - val_mae: 0.0686 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m268/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0053 - mae: 0.0527\n",
      "Epoch 21: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0051 - mae: 0.0521 - val_loss: 0.0079 - val_mae: 0.0648 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.0049 - mae: 0.0508\n",
      "Epoch 22: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0050 - mae: 0.0509 - val_loss: 0.0083 - val_mae: 0.0661 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m276/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 0.0050 - mae: 0.0514\n",
      "Epoch 23: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0051 - mae: 0.0519 - val_loss: 0.0091 - val_mae: 0.0687 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 0.0050 - mae: 0.0510\n",
      "Epoch 24: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0050 - mae: 0.0513 - val_loss: 0.0088 - val_mae: 0.0676 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m261/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 0.0047 - mae: 0.0501\n",
      "Epoch 25: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0049 - mae: 0.0511 - val_loss: 0.0082 - val_mae: 0.0643 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m268/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 0.0049 - mae: 0.0509\n",
      "Epoch 26: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0048 - mae: 0.0504 - val_loss: 0.0088 - val_mae: 0.0677 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m303/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.0051 - mae: 0.0508\n",
      "Epoch 27: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0498 - val_loss: 0.0084 - val_mae: 0.0659 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m271/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.0047 - mae: 0.0499\n",
      "Epoch 28: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0497 - val_loss: 0.0080 - val_mae: 0.0648 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m307/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.0046 - mae: 0.0491\n",
      "Epoch 29: val_loss did not improve from 0.00770\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0490 - val_loss: 0.0081 - val_mae: 0.0655 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m305/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0489\n",
      "Epoch 30: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0496 - val_loss: 0.0095 - val_mae: 0.0702 - learning_rate: 2.5000e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "training completed\n",
      "total epochs run: 30\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 32  # keras documentation model_fitting recommends 32 as good default\n",
    "epochs = 100  # max epochs, early stopping will likely stop before this.\n",
    "\n",
    "\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Max epochs: {epochs}')\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Validation samples: {len(X_val)}')\n",
    "\n",
    "# train the model model.fit() is the main training loop in keras\n",
    "history = model.fit(\n",
    "    X_train, y_train_single,  # training data and targets\n",
    "    validation_data=(X_val, y_val_single),  # validation set for monitoring\n",
    "    epochs=epochs,  # maximum training rounds\n",
    "    batch_size=batch_size,  # samples per gradient update\n",
    "    callbacks=callbacks,  # early stopping, checkpoint, LR reduction\n",
    "    verbose=1  # show progress bar\n",
    ")\n",
    "\n",
    "print(f'\\ntraining completed')\n",
    "print(f'total epochs run: {len(history.history[\"loss\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79f40f",
   "metadata": {},
   "source": [
    "    Batch size: 32\n",
    "    Max epochs: 100\n",
    "    Training samples: 9946\n",
    "    Validation samples: 2131\n",
    "    Epoch 1/100\n",
    "    \u001b[1m294/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.0193 - mae: 0.1011\n",
    "    Epoch 1: val_loss improved from None to 0.01201, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "\n",
    "    Epoch 1: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0138 - mae: 0.0868 - val_loss: 0.0120 - val_mae: 0.0841 - learning_rate: 0.0010\n",
    "    Epoch 2/100\n",
    "    \u001b[1m282/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0099 - mae: 0.0742\n",
    "    Epoch 2: val_loss improved from 0.01201 to 0.00976, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "\n",
    "    Epoch 2: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0095 - mae: 0.0722 - val_loss: 0.0098 - val_mae: 0.0748 - learning_rate: 0.0010\n",
    "    Epoch 3/100\n",
    "    \u001b[1m267/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0080 - mae: 0.0660\n",
    "    Epoch 3: val_loss did not improve from 0.00976\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0081 - mae: 0.0654 - val_loss: 0.0102 - val_mae: 0.0750 - learning_rate: 0.0010\n",
    "    Epoch 4/100\n",
    "    \u001b[1m270/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 0.0079 - mae: 0.0655\n",
    "    Epoch 4: val_loss did not improve from 0.00976\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0076 - mae: 0.0638 - val_loss: 0.0102 - val_mae: 0.0739 - learning_rate: 0.0010\n",
    "    Epoch 5/100\n",
    "    \u001b[1m302/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0073 - mae: 0.0614\n",
    "    Epoch 5: val_loss did not improve from 0.00976\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - mae: 0.0617 - val_loss: 0.0167 - val_mae: 0.0957 - learning_rate: 0.0010\n",
    "    Epoch 6/100\n",
    "    \u001b[1m283/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.0073 - mae: 0.0625\n",
    "    Epoch 6: val_loss did not improve from 0.00976\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0068 - mae: 0.0599 - val_loss: 0.0102 - val_mae: 0.0738 - learning_rate: 0.0010\n",
    "    Epoch 7/100\n",
    "    \u001b[1m279/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.0066 - mae: 0.0584\n",
    "    Epoch 7: val_loss improved from 0.00976 to 0.00933, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "\n",
    "    Epoch 7: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0065 - mae: 0.0584 - val_loss: 0.0093 - val_mae: 0.0713 - learning_rate: 0.0010\n",
    "    Epoch 8/100\n",
    "    \u001b[1m305/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0063 - mae: 0.0569  \n",
    "    Epoch 8: val_loss improved from 0.00933 to 0.00924, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "\n",
    "    Epoch 8: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0064 - mae: 0.0575 - val_loss: 0.0092 - val_mae: 0.0693 - learning_rate: 0.0010\n",
    "    Epoch 9/100\n",
    "    \u001b[1m286/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0061 - mae: 0.0569\n",
    "    Epoch 9: val_loss did not improve from 0.00924\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0064 - mae: 0.0577 - val_loss: 0.0110 - val_mae: 0.0761 - learning_rate: 0.0010\n",
    "    Epoch 10/100\n",
    "    \u001b[1m272/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 0.0059 - mae: 0.0563\n",
    "    Epoch 10: val_loss did not improve from 0.00924\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0060 - mae: 0.0562 - val_loss: 0.0116 - val_mae: 0.0783 - learning_rate: 0.0010\n",
    "    Epoch 11/100\n",
    "    \u001b[1m269/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0062 - mae: 0.0578\n",
    "    Epoch 11: val_loss improved from 0.00924 to 0.00783, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "\n",
    "    Epoch 11: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0060 - mae: 0.0562 - val_loss: 0.0078 - val_mae: 0.0636 - learning_rate: 0.0010\n",
    "    Epoch 12/100\n",
    "    \u001b[1m270/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 0.0063 - mae: 0.0567\n",
    "    Epoch 12: val_loss did not improve from 0.00783\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0060 - mae: 0.0558 - val_loss: 0.0105 - val_mae: 0.0744 - learning_rate: 0.0010\n",
    "    Epoch 13/100\n",
    "    \u001b[1m279/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0057 - mae: 0.0546\n",
    "    Epoch 13: val_loss did not improve from 0.00783\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0059 - mae: 0.0553 - val_loss: 0.0086 - val_mae: 0.0684 - learning_rate: 0.0010\n",
    "    Epoch 14/100\n",
    "    \u001b[1m262/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 0.0057 - mae: 0.0549\n",
    "    Epoch 14: val_loss did not improve from 0.00783\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0058 - mae: 0.0552 - val_loss: 0.0102 - val_mae: 0.0733 - learning_rate: 0.0010\n",
    "    Epoch 15/100\n",
    "    \u001b[1m263/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 0.0058 - mae: 0.0548\n",
    "    Epoch 15: val_loss improved from 0.00783 to 0.00770, saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "\n",
    "    Epoch 15: finished saving model to /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/cnn_model/best_cnn_model.keras\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0057 - mae: 0.0546 - val_loss: 0.0077 - val_mae: 0.0634 - learning_rate: 0.0010\n",
    "    Epoch 16/100\n",
    "    \u001b[1m269/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0056 - mae: 0.0543\n",
    "    Epoch 16: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0058 - mae: 0.0549 - val_loss: 0.0080 - val_mae: 0.0648 - learning_rate: 0.0010\n",
    "    Epoch 17/100\n",
    "    \u001b[1m264/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0053 - mae: 0.0529\n",
    "    Epoch 17: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0055 - mae: 0.0537 - val_loss: 0.0077 - val_mae: 0.0648 - learning_rate: 0.0010\n",
    "    Epoch 18/100\n",
    "    \u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 0.0054 - mae: 0.0534\n",
    "    Epoch 18: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0054 - mae: 0.0536 - val_loss: 0.0085 - val_mae: 0.0676 - learning_rate: 0.0010\n",
    "    Epoch 19/100\n",
    "    \u001b[1m271/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 0.0054 - mae: 0.0537\n",
    "    Epoch 19: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0055 - mae: 0.0537 - val_loss: 0.0092 - val_mae: 0.0701 - learning_rate: 0.0010\n",
    "    Epoch 20/100\n",
    "    \u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 0.0057 - mae: 0.0548\n",
    "    Epoch 20: val_loss did not improve from 0.00770\n",
    "\n",
    "    Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0055 - mae: 0.0541 - val_loss: 0.0091 - val_mae: 0.0686 - learning_rate: 0.0010\n",
    "    Epoch 21/100\n",
    "    \u001b[1m268/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0053 - mae: 0.0527\n",
    "    Epoch 21: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0051 - mae: 0.0521 - val_loss: 0.0079 - val_mae: 0.0648 - learning_rate: 5.0000e-04\n",
    "    Epoch 22/100\n",
    "    \u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.0049 - mae: 0.0508\n",
    "    Epoch 22: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0050 - mae: 0.0509 - val_loss: 0.0083 - val_mae: 0.0661 - learning_rate: 5.0000e-04\n",
    "    Epoch 23/100\n",
    "    \u001b[1m276/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 0.0050 - mae: 0.0514\n",
    "    Epoch 23: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0051 - mae: 0.0519 - val_loss: 0.0091 - val_mae: 0.0687 - learning_rate: 5.0000e-04\n",
    "    Epoch 24/100\n",
    "    \u001b[1m306/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 0.0050 - mae: 0.0510\n",
    "    Epoch 24: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0050 - mae: 0.0513 - val_loss: 0.0088 - val_mae: 0.0676 - learning_rate: 5.0000e-04\n",
    "    Epoch 25/100\n",
    "    \u001b[1m261/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 0.0047 - mae: 0.0501\n",
    "    Epoch 25: val_loss did not improve from 0.00770\n",
    "\n",
    "    Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0049 - mae: 0.0511 - val_loss: 0.0082 - val_mae: 0.0643 - learning_rate: 5.0000e-04\n",
    "    Epoch 26/100\n",
    "    \u001b[1m268/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 0.0049 - mae: 0.0509\n",
    "    Epoch 26: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0048 - mae: 0.0504 - val_loss: 0.0088 - val_mae: 0.0677 - learning_rate: 2.5000e-04\n",
    "    Epoch 27/100\n",
    "    \u001b[1m303/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.0051 - mae: 0.0508\n",
    "    Epoch 27: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0498 - val_loss: 0.0084 - val_mae: 0.0659 - learning_rate: 2.5000e-04\n",
    "    Epoch 28/100\n",
    "    \u001b[1m271/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.0047 - mae: 0.0499\n",
    "    Epoch 28: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0497 - val_loss: 0.0080 - val_mae: 0.0648 - learning_rate: 2.5000e-04\n",
    "    Epoch 29/100\n",
    "    \u001b[1m307/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.0046 - mae: 0.0491\n",
    "    Epoch 29: val_loss did not improve from 0.00770\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0490 - val_loss: 0.0081 - val_mae: 0.0655 - learning_rate: 2.5000e-04\n",
    "    Epoch 30/100\n",
    "    \u001b[1m305/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0489\n",
    "    Epoch 30: val_loss did not improve from 0.00770\n",
    "\n",
    "    Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
    "    \u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0496 - val_loss: 0.0095 - val_mae: 0.0702 - learning_rate: 2.5000e-04\n",
    "    Epoch 30: early stopping\n",
    "    Restoring model weights from the end of the best epoch: 15.\n",
    "\n",
    "    training completed\n",
    "    total epochs run: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bdf8b",
   "metadata": {},
   "source": [
    "### training results\n",
    "\n",
    "training completed at epoch 30. early stopping triggered after 15 epochs of no improvement. best model was epoch 15, weights restored from there.\n",
    "\n",
    "| metric | start (epoch 1) | best (epoch 15) | change |\n",
    "|--------|-----------------|-----------------|--------|\n",
    "| val_loss | 0.0120 | 0.0077 | -36% |\n",
    "| val_mae | 0.0841 | 0.0634 | -25% |\n",
    "\n",
    "learning rate reduced three times during training:\n",
    "\n",
    "| epoch | learning rate | reason |\n",
    "|-------|---------------|--------|\n",
    "| 1-20 | 0.001 | initial |\n",
    "| 20-25 | 0.0005 | no improvement for 5 epochs |\n",
    "| 25-30 | 0.00025 | still no improvement |\n",
    "| 30 | 0.000125 | still stuck, early stopping triggered |\n",
    "\n",
    "**what val_loss = 0.0077 means:**\n",
    "\n",
    "this is MSE on normalised data (0-1 scale). RMSE = sqrt(0.0077) = 0.088. so predictions are roughly 8.8% off on average.\n",
    "\n",
    "**overfitting check:**\n",
    "\n",
    "training and validation loss stayed close throughout training. no sign of serious overfitting. the model generalised well to validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19d28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
