{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb60f4c",
   "metadata": {},
   "source": [
    "# Random Forest Modelling Notebook for LAQN\n",
    "\n",
    "- Starting ML, very excited.\n",
    "- This notebook is for Random Forest training using 2D flattened data.\n",
    "- Inputs will be taken from: `data/laqn/ml_prep` folder.\n",
    "- I will be using Géron's *Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow* 3rd edition as primary source to understand the X_training and y sets better and clear implementation structures mirroring from the book.\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1. Load prepared data from ml_prep output.\n",
    "2. Understand the X and y structure (following Géron Chapter 2).\n",
    "3. Train a baseline Random Forest model.\n",
    "4. Evaluate using RMSE, MAE, R² (Géron Chapter 2 evaluation approach).\n",
    "5. Fine-tune hyperparameters with GridSearchCV (Géron Chapter 2).\n",
    "6. Analyse feature importance.\n",
    "7. Save the trained model.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Random Forest?\n",
    "\n",
    "From Géron (2023, Chapter 7), Random Forest is an ensemble of Decision Trees trained on different random subsets of the training data. Each tree votes on the prediction, and the final output is the average (for regression) or majority vote (for classification).\n",
    "\n",
    "Key advantages for air quality prediction:\n",
    "- Handles nonlinear relationships without feature scaling.\n",
    "- Provides feature importance for interpretability.\n",
    "- Robust against overfitting when properly tuned.\n",
    "- Works well with tabular data like our flattened time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9ec12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mandatory libraries for random forest training\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# scikit-learn for random forest and evaluation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#before grid search, I decided to use kfol n_splits =5\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modules for ebaluation metrics - scikit-learn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "#gridsearch for 6 sections\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82194a",
   "metadata": {},
   "source": [
    "### File paths\n",
    "\n",
    "Loading from the ml_prep output folder where all prepared arrays are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eb4bf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/ml_prep\n",
      "Saving results to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/rf_model\n"
     ]
    }
   ],
   "source": [
    "#Paths setup matching ml_prep output \n",
    "base_dir = Path.cwd().parent.parent / \"data\" / \"laqn\"\n",
    "ml_prep_dir = base_dir / \"ml_prep\"\n",
    "\n",
    "#Output folder for this notebook\n",
    "rf_output_dir = base_dir / \"rf_model\"\n",
    "rf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Loading data from: {ml_prep_dir}\")\n",
    "print(f\"Saving results to: {rf_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a8431",
   "metadata": {},
   "source": [
    "## 1) Load prepared data\n",
    "\n",
    "The ml_prep notebook created:\n",
    "- `X_train_rf.npy`: Flattened training features (9,946 samples × 468 features)\n",
    "- `X_val_rf.npy`: Flattened validation features (2,131 samples × 468 features)\n",
    "- `X_test_rf.npy`: Flattened test features (2,132 samples × 468 features)\n",
    "- `y_train.npy`, `y_val.npy`, `y_test.npy`: Target values\n",
    "- `rf_feature_names.joblib`: Feature names for interpretability\n",
    "- `scaler.joblib`: MinMaxScaler to reverse normalisation\n",
    "\n",
    "The flattening was necessary because Random Forest expects 2D input (samples, features), but the original sequences were 3D (samples, timesteps, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4765b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "# load all prepared data\n",
    "print(\"Loading data\")\n",
    "\n",
    "X_train = np.load(ml_prep_dir / \"X_train_rf.npy\")\n",
    "X_val = np.load(ml_prep_dir / \"X_val_rf.npy\")\n",
    "X_test = np.load(ml_prep_dir / \"X_test_rf.npy\")\n",
    "\n",
    "y_train = np.load(ml_prep_dir / \"y_train.npy\")\n",
    "y_val = np.load(ml_prep_dir / \"y_val.npy\")\n",
    "y_test = np.load(ml_prep_dir / \"y_test.npy\")\n",
    "\n",
    "rf_feature_names = joblib.load(ml_prep_dir / \"rf_feature_names.joblib\")\n",
    "feature_names = joblib.load(ml_prep_dir / \"feature_names.joblib\")\n",
    "scaler = joblib.load(ml_prep_dir / \"scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762e3181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9946, 468)\n",
      "X_val shape: (2131, 468)\n",
      "X_test shape: (2132, 468)\n",
      "y_train shape: (9946, 39)\n",
      "y_val shape: (2131, 39)\n",
      "y_test shape: (2132, 39)\n",
      "Number of RF features: 468\n",
      "Number of target features: 39\n"
     ]
    }
   ],
   "source": [
    "#check loaded data shapes\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Number of RF features: {len(rf_feature_names)}\")\n",
    "print(f\"Number of target features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b4d05",
   "metadata": {},
   "source": [
    "    X_train shape: (9946, 468)\n",
    "    X_val shape: (2131, 468)\n",
    "    X_test shape: (2132, 468)\n",
    "    y_train shape: (9946, 39)\n",
    "    y_val shape: (2131, 39)\n",
    "    y_test shape: (2132, 39)\n",
    "    Number of RF features: 468\n",
    "    Number of target features: 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74be7b",
   "metadata": {},
   "source": [
    "## 2) Select target pollutant\n",
    "\n",
    "The y array has 39 outputs (one for each feature). For better evaluation, I will train a single-output model first.\n",
    "\n",
    "### Why single-output?\n",
    "\n",
    "Starting with one target keeps things simple:\n",
    "- Easier to interpret evaluation metrics (RMSE, R² for one pollutant).\n",
    "- Easier to understand feature importance (what predicts NO2 specifically).\n",
    "- Can train separate models for PM10 and O3 later and compare.\n",
    "\n",
    "### Which target to select?\n",
    "\n",
    "In ml_prep notebook section 6.A, I selected 35 columns sorted by data coverage. The first column has the highest coverage, making it the most reliable target for initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9172182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available targets:\n",
      "   0: EN5_NO2\n",
      "   1: WMD_NO2\n",
      "   2: BT5_NO2\n",
      "   3: HP1_PM10\n",
      "   4: EN1_NO2\n",
      "   5: ME9_NO2\n",
      "   6: BT6_NO2\n",
      "   7: BT8_PM10\n",
      "   8: HV1_NO2\n",
      "   9: BT4_PM10\n",
      "  10: KC1_NO2\n",
      "  11: BT8_NO2\n",
      "  12: EI1_NO2\n",
      "  13: HP1_NO2\n",
      "  14: BX2_PM10\n",
      "  15: GN0_NO2\n",
      "  16: WM6_NO2\n",
      "  17: IS6_NO2\n",
      "  18: RI1_NO2\n",
      "  19: HP1_O3\n",
      "  20: BT6_PM10\n",
      "  21: SK5_NO2\n",
      "  22: BX1_O3\n",
      "  23: GR9_NO2\n",
      "  24: EN4_NO2\n",
      "  25: GN6_NO2\n",
      "  26: GR7_PM10\n",
      "  27: KC1_O3\n",
      "  28: GR7_NO2\n",
      "  29: GN3_PM10\n",
      "  30: LB4_NO2\n",
      "  31: GN4_PM10\n",
      "  32: GN4_NO2\n",
      "  33: EA8_NO2\n",
      "  34: EA6_NO2\n",
      "  35: hour\n",
      "  36: day_of_week\n",
      "  37: month\n",
      "  38: is_weekend\n"
     ]
    }
   ],
   "source": [
    "# list targets\n",
    "print(\"Available targets:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i:2d}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fa0a7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0\n",
      "y_train_single: (9946,)\n",
      "Range: [0.0079, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "# select EN5_NO2\n",
    "target_idx= 0\n",
    "target_name = feature_names[target_idx]\n",
    "\n",
    "y_train_single = y_train[:, target_idx]\n",
    "y_val_single = y_val[:, target_idx]\n",
    "y_test_single = y_test[:, target_idx]\n",
    "\n",
    "print(f\"Target: {target_idx}\")\n",
    "print(f\"y_train_single: {y_train_single.shape}\")\n",
    "print(f\"Range: [{y_train_single.min():.4f}, {y_train_single.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702ac86",
   "metadata": {},
   "source": [
    "    Target: 0\n",
    "    y_train_single: (9946,)\n",
    "    Range: [0.0079, 1.0000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23994f0d",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train baseline model (Géron Chapter 7)\n",
    "\n",
    "From Géron (2023, Chapter 7 - Ensemble Learning and Random Forests):\n",
    "\n",
    "> \"A random forest is an ensemble of decision trees, generally trained via the bagging method (or sometimes pasting), typically with `max_samples` set to the size of the training set. Instead of building a `BaggingClassifier` and passing it a `DecisionTreeClassifier`, you can use the `RandomForestClassifier` class, which is more convenient and optimized for decision trees (similarly, there is a `RandomForestRegressor` class for regression tasks).\"\n",
    "\n",
    "Since I am predicting continuous pollution values (regression), I use `RandomForestRegressor`.\n",
    "\n",
    "### Key parameters:\n",
    "\n",
    "| Parameter | Default | What it does |\n",
    "| --- | --- | --- |\n",
    "| n_estimators | 100 | Number of trees in the forest |\n",
    "| max_leaf_nodes | None | Maximum leaf nodes per tree |\n",
    "| n_jobs | -1 | CPU cores to use (-1 = all available) |\n",
    "| random_state | 42 | Seed for reproducibility |\n",
    "\n",
    "Géron's example uses `n_estimators=500` and `max_leaf_nodes=16`, but I start with defaults to establish a baseline before tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15a52368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline Random Forest\n",
      "----------------------------------------\n",
      "\n",
      "Training complete in 97.59 seconds\n",
      "Number of trees: 100\n",
      "Max leaf nodes: None\n",
      "Max depth: None\n"
     ]
    }
   ],
   "source": [
    "# train baseline Random Forest\n",
    "# Using RandomForestRegressor for regression task predicting continuous values following Géron's structure from Chapter 7\n",
    "\n",
    "print(\"Training baseline Random Forest\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# baseline with default parameters\n",
    "rf_baseline = RandomForestRegressor(\n",
    "    n_estimators=100,      # default, Géron's example uses 500\n",
    "    random_state=42,       # for reproducibility\n",
    "    n_jobs=-1              # use all CPU cores\n",
    ")\n",
    "\n",
    "rf_baseline.fit(X_train, y_train_single)\n",
    "\n",
    "baseline_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTraining complete in {baseline_time:.2f} seconds\")\n",
    "print(f\"Number of trees: {rf_baseline.n_estimators}\")\n",
    "print(f\"Max leaf nodes: {rf_baseline.max_leaf_nodes}\")\n",
    "print(f\"Max depth: {rf_baseline.max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d200e",
   "metadata": {},
   "source": [
    "    Training baseline Random Forest\n",
    "    ----------------------------------------\n",
    "\n",
    "    Training complete in 87.43 seconds\n",
    "    Number of trees: 100\n",
    "    Max leaf nodes: None\n",
    "    Max depth: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340e37e",
   "metadata": {},
   "source": [
    "## 4) Evaluate baseline model\n",
    "\n",
    "To evaluate the baseline model, I use three metrics from scikit-learn's `sklearn.metrics` module.\n",
    "\n",
    "### RMSE (Root Mean Square Error)\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "Penalises large errors more heavily. Lower is better.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)-\n",
    "#changed to - newer scikit-learn not supports squared= False\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "```\n",
    "\n",
    "Source: Stack Overflow (2013) *Is there a library function for root mean square error (RMSE) in python?* Available at: https://stackoverflow.com/questions/17197492 (Accessed: 23 December 2025).\n",
    "\n",
    "### MAE (Mean Absolute Error)\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "Average absolute difference between actual and predicted. More interpretable than RMSE. Lower is better.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "Source: scikit-learn (no date) *sklearn.metrics.mean_absolute_error*. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\n",
    "\n",
    "### R² (Coefficient of Determination)\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n",
    "\n",
    "Proportion of variance explained by the model. Range 0 to 1, higher is better. A score of 1.0 means perfect predictions.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "Source: scikit-learn (no date) *sklearn.metrics.r2_score*. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "369d6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y_true, name):\n",
    "    \"\"\"\n",
    "    Evaluate model using RMSE, MAE, and R².\n",
    "    \n",
    "    Params:\n",
    "    model : trained sklearn model\n",
    "    X : feature matrix n_samples, n_features\n",
    "    y_true : actual values n_samples,\n",
    "    name : string displayin \n",
    "    \n",
    "    Returns: rmse, mae, r2, and predict\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # RMSE using as np.sqrt for rmse\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    # MAE avg absolute difference\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # R^2 proportion of variance explained\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  RMSE = {rmse:.6f}\")\n",
    "    print(f\"  MAE  = {mae:.6f}\")\n",
    "    print(f\"  R^2   = {r2:.6f}\")\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2, 'y_pred': y_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "726f60da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Evaluation\n",
      "========================================\n",
      "Training:\n",
      "  RMSE = 0.019309\n",
      "  MAE  = 0.012969\n",
      "  R^2   = 0.979111\n",
      "\n",
      "Validation:\n",
      "  RMSE = 0.057030\n",
      "  MAE  = 0.038897\n",
      "  R^2   = 0.860611\n",
      "\n",
      "Test:\n",
      "  RMSE = 0.050930\n",
      "  MAE  = 0.034585\n",
      "  R^2   = 0.810181\n"
     ]
    }
   ],
   "source": [
    "# Print to evaluate baseline on three sets\n",
    "print(\"Baseline Model Evaluation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "base_train = evaluate(rf_baseline, X_train, y_train_single, \"Training\")\n",
    "print()\n",
    "base_val = evaluate(rf_baseline, X_val, y_val_single, \"Validation\")\n",
    "print()\n",
    "base_test = evaluate(rf_baseline, X_test, y_test_single, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b0fda",
   "metadata": {},
   "source": [
    "        Baseline Model Evaluation\n",
    "        ========================================\n",
    "        Training:\n",
    "        RMSE = 0.019309\n",
    "        MAE  = 0.012969\n",
    "        R^2   = 0.979111\n",
    "\n",
    "        Validation:\n",
    "        RMSE = 0.057030\n",
    "        MAE  = 0.038897\n",
    "        R^2   = 0.860611\n",
    "\n",
    "        Test:\n",
    "        RMSE = 0.050930\n",
    "        MAE  = 0.034585\n",
    "        R^2   = 0.810181"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bdadd",
   "metadata": {},
   "source": [
    "The gap between training R² (0.979) and validation R^2 (0.861) is **0.118** that shows that overfitting. The model memorised the training data rather than learning general patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ae826",
   "metadata": {},
   "source": [
    "### Checking for overfitting\n",
    "\n",
    "The training R^2 is much higher than validation R², the model is overfitting. This happens when the model memorises training data instead of learning general patterns.\n",
    "\n",
    "Signs of overfitting:\n",
    "\n",
    "- Training R^2 close to 1.0, validation R^2 much lower.\n",
    "- Large gap between training and validation RMSE.\n",
    "\n",
    "Since = the model overfits , I'll tune hyperparameters to reduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c165d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting Check\n",
      "========================================\n",
      "Training R^2:   0.9791\n",
      "Validation R^2: 0.8606\n",
      "Gap:           0.1185\n",
      "\n",
      "Mild overfitting = tuning may help.\n"
     ]
    }
   ],
   "source": [
    "# overfitting check\n",
    "r2_gap = base_train['r2'] - base_val['r2']\n",
    "print(\"Overfitting Check\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Training R^2:   {base_train['r2']:.4f}\")\n",
    "print(f\"Validation R^2: {base_val['r2']:.4f}\")\n",
    "print(f\"Gap:           {r2_gap:.4f}\")\n",
    "\n",
    "if r2_gap > 0.15:\n",
    "    print(\"\\n The Model EN5_NO2 overfits.\")\n",
    "elif r2_gap > 0.05:\n",
    "    print(\"\\nMild overfitting = tuning may help.\")\n",
    "else:\n",
    "    print(\"\\nNo significant overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd256b",
   "metadata": {},
   "source": [
    "## 5) Cross-validation \n",
    "\n",
    "### What is K-fold cross-validation?\n",
    "\n",
    "From scikit-learn documentation: > \"K-Fold cross-validator provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\"\n",
    "\n",
    "### sklearn.model_selection.KFold\n",
    "\n",
    "```python\n",
    "class sklearn.model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- `n_splits`: int, default=5. Number of folds. Must be at least 2. in version 0.22.\n",
    "- `shuffle`: bool, default=False. Whether to shuffle the data before splitting into batches.\n",
    "- `random_state`: int or None, default=None. When shuffle is True, controls randomness of each fold.\n",
    "\n",
    "**Example from documentation:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=2)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "```\n",
    "\n",
    "Source: *Kfold* (no date) *scikit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eae35a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 fold Cross-Validation:\n",
      "----------------------------------------\n",
      "Fold 1: RMSE = 0.062451\n",
      "Fold 2: RMSE = 0.054345\n",
      "Fold 3: RMSE = 0.051401\n",
      "Fold 4: RMSE = 0.053195\n",
      "Fold 5: RMSE = 0.049685\n",
      "\n",
      "CV RMSE: 0.054215 +/- 0.004412\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits=5)\n",
    "cv_rmse_scores = []\n",
    "\n",
    "print(\"Running 5 fold Cross-Validation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    # split data\n",
    "    X_fold_train = X_train[train_idx]\n",
    "    X_fold_val = X_train[val_idx]\n",
    "    y_fold_train = y_train_single[train_idx]\n",
    "    y_fold_val = y_train_single[val_idx]\n",
    "    \n",
    "    # train and evaluate\n",
    "    rf_baseline.fit(X_fold_train, y_fold_train)\n",
    "    y_pred = rf_baseline.predict(X_fold_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))\n",
    "    cv_rmse_scores.append(rmse)\n",
    "    print(f\"Fold {i+1}: RMSE = {rmse:.6f}\")\n",
    "\n",
    "print(f\"\\nCV RMSE: {np.mean(cv_rmse_scores):.6f} +/- {np.std(cv_rmse_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46fb8a6",
   "metadata": {},
   "source": [
    "## 6) Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "### Why GridSearchCV?\n",
    "\n",
    "From the baseline evaluation:\n",
    "\n",
    "- Training R^2: 0.9791\n",
    "- Validation R^2: 0.8606\n",
    "- Gap: 0.1185 (mild overfitting)\n",
    "\n",
    "The model is too complex - it memorises training data. I need to find parameters that reduce this gap.\n",
    "\n",
    "### sklearn.model_selection.GridSearchCV\n",
    "\n",
    "From scikit-learn documentation:\n",
    "\n",
    "> \"Exhaustive search over specified parameter values for an estimator. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\"\n",
    "\n",
    "```python\n",
    "class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, \n",
    "    n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', \n",
    "    error_score=nan, return_train_score=False)\n",
    "```\n",
    "\n",
    "**Parameters I will use:**\n",
    "\n",
    "- `estimator`: RandomForestRegressor\n",
    "- `param_grid`: dict with parameter names as keys and lists of settings to try as values\n",
    "- `scoring`: strategy to evaluate performance of the cross-validated model\n",
    "- `cv`: int, to specify the number of folds. Default is 5-fold cross validation\n",
    "- `n_jobs`: -1 means using all processors\n",
    "- `verbose`: controls the verbosity, higher = more messages\n",
    "\n",
    "**Key attributes after fitting:**\n",
    "\n",
    "- `best_estimator_`: estimator which gave highest score on the leftout data\n",
    "- `best_score_`: mean cross-validated score of the best_estimator\n",
    "- `best_params_`: parameter setting that gave the best results\n",
    "\n",
    "### Key hyperparameters to tune for Random Forest:\n",
    "\n",
    "| Parameter         | Description                   | Effect                                  |\n",
    "| ----------------- | ----------------------------- | --------------------------------------- |\n",
    "| n_estimators      | Number of trees               | More trees = better accuracy but slower |\n",
    "| max_depth         | Maximum tree depth            | Limits complexity, prevents overfitting |\n",
    "| min_samples_split | Minimum samples to split node | Higher = simpler trees                  |\n",
    "| min_samples_leaf  | Minimum samples in leaf       | Higher = smoother predictions           |\n",
    "\n",
    "Source:*GRIDSEARCHCV* (no date) *scikit*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06d3c07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid:\n",
      "  n_estimators: [100, 200]\n",
      "  max_depth: [10, 20, None]\n",
      "  min_samples_split: [2, 5]\n",
      "  min_samples_leaf: [1, 2]\n",
      "\n",
      "Total combinations to test: 24\n",
      "With 3-fold CV: 72 model fits\n"
     ]
    }
   ],
   "source": [
    "# defined param_grid. dict with parameter names as keys and lists of settings to try as values\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# calculate total combinations\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(\"Parameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal combinations to test: {total_combinations}\")\n",
    "print(f\"With 3-fold CV: {total_combinations * 3} model fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7552c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV\n",
      "----------------------------------------\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 4.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 4.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 5.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 4.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 5.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 3.9min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 4.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 4.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 3.9min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 3.9min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 8.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 8.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 8.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 7.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 7.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 7.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 3.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 3.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 3.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 7.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 7.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 7.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 6.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 4.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 4.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 4.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 7.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 7.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 4.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 4.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 4.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 9.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 9.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 9.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 5.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 5.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 5.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 9.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=10.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=10.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 5.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 5.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 9.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 4.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 9.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 9.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 7.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 6.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 5.3min\n",
      "\n",
      "GridSearchCV complete in 3231.71 seconds (53.86 minutes)\n"
     ]
    }
   ],
   "source": [
    "# run GridSearchCV \n",
    "print(\"Running GridSearchCV\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rf_grid = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_grid,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,           # 3-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=2,      # display computation time and score\n",
    "    n_jobs=-1       # use all processors\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train_single)\n",
    "\n",
    "tuning_time = time.time() - start_time\n",
    "print(f\"\\nGridSearchCV complete in {tuning_time:.2f} seconds ({tuning_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295743ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params_ = param setting that gave the best results?\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(\"-\" * 40)\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# best_score_: mean cross-validated score of the best_estimator\n",
    "best_cv_rmse = np.sqrt(-grid_search.best_score_)\n",
    "print(f\"\\nBest CV RMSE: {best_cv_rmse:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
