{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d96e",
   "metadata": {},
   "source": [
    "# LAQN Updated Active sites/species File Function:\n",
    "- laqn_remove notebook got slower so i will move the update function here.\n",
    "- Start with paths and modules to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74ea5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# paths beloww\n",
    "base_dir = Path(\"/Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels\")\n",
    "# the metadata file for nan @value \n",
    "nanValue_path = base_dir / \"data\" / \"laqn\" / \"missing\" / \"logs_nan_value.csv\"\n",
    "\n",
    "# Month abbreviation list for reference/use in functions\n",
    "month_list = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "# analyse_affected_sites_2023 path below.\n",
    "check_nonActive_path = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies.csv\"\n",
    "output_notActive_siteSpecies_2023 = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies_2023.csv\"\n",
    "\n",
    "# checks the removed site-species combinations against active list and removes them\n",
    "existing_nonactive_path = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_site_species.csv\"\n",
    "\n",
    "# analyse_affected_sites_2024 path below.\n",
    "output_notActive_siteSpecies_2024 = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies_2024.csv\"\n",
    "\n",
    "# analyse_affected_sites_2025 path below.\n",
    "output_notActive_siteSpecies_2025 = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies_2025.csv\"\n",
    "\n",
    "#calculating new issue rate, so I will be count all files in optimased folder and than recalculate the issue rate taking \n",
    "# out the notAcvtive_2024/2023 files from output_notActive_siteSpecies_2023, output_notActive_siteSpecies_2024 csv's\n",
    "optimased_root = base_dir / \"data\" / \"laqn\" / \"optimased\"\n",
    "log_file = base_dir / \"data\" / \"laqn\" / \"missing\" / \"logs_missin_value.csv\"\n",
    "\n",
    "# Collect all CSV files in the optimased_root directory\n",
    "all_csv_files = list(Path(optimased_root).glob(\"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56e9c7",
   "metadata": {},
   "source": [
    "## 1) analyse_affected_sites_2023 function to identify site/species with all months missing in 2023\n",
    "    what does function:\n",
    "\n",
    "- scans the missing value log for site/species combinations that have 100% missing values for all 12 months in 2023\n",
    "- if an existing non-active site/species list is provided, only new combinations not already in that list are included\n",
    "- prints the results and saves them as a CSV file for further review\n",
    "\n",
    "    paths:\n",
    "\n",
    "- nanValue_path: base_dir / data / laqn / missing / logs_nan_value.csv\n",
    "- output_notActive_siteSpecies_2023: base_dir / data / laqn / missing / notActive_siteSpecies_2023.csv\n",
    "- existing_nonactive_path (optional): base_dir / data / laqn / missing / notActive_site_species.csv\n",
    "- new csv file save as: notActive_siteSpecies_2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a9ca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_affected_sites_year(\n",
    "    nanValue_path,  # value_100filtered_missing.csv\n",
    "    output_notActive_site_species,  # recommended output path\n",
    "    check_nonActive_path=None,  # notActive_site_species.csv\n",
    "    year=2025,\n",
    "    optimased_dir=None  # Optional: base dir for constructing file paths\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Checks value_100filtered_missing.csv for site/species combos with 100% missing values for all months in the given year.\n",
    "    2. Compares to notActive_site_species.csv and finds new combos not already listed.\n",
    "    3. Adds actual filenames (and optionally full paths) for each combo from logs_nan_value.csv.\n",
    "    4. Prints and saves these new combos to output_notActive_site_species.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_month_number(month_str):\n",
    "        month_map = {abbr: i+1 for i, abbr in enumerate(month_list)}\n",
    "        m = re.match(r\"\\d{4}_(\\w{3})\", str(month_str).lower())\n",
    "        if m:\n",
    "            return month_map.get(m.group(1), None)\n",
    "        return None\n",
    "\n",
    "    # Load and filter for year, 100% missing\n",
    "    df = pd.read_csv(nanValue_path, encoding='utf-8')\n",
    "    df['month_number'] = df['month'].apply(extract_month_number)\n",
    "    df = df[df['month_number'].notna()].copy()\n",
    "    df_year = df[df['year'] == year]\n",
    "    summary_year = df_year.groupby(['siteCode', 'SpeciesCode'])['month_number'].nunique().reset_index()\n",
    "    affected_year = summary_year[summary_year['month_number'] == 12].copy()\n",
    "\n",
    "    print(f\"\\nTotal site/species combos with 100% missing in {year}: {len(affected_year)}\")\n",
    "    print(affected_year)\n",
    "\n",
    "    # Merge to get actual filenames and (optionally) full paths from nanValue_path\n",
    "    merged = pd.merge(\n",
    "        affected_year,\n",
    "        df_year[['siteCode', 'SpeciesCode', 'filename', 'path']].drop_duplicates(),\n",
    "        on=['siteCode', 'SpeciesCode'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Add expected filename prefix for reference\n",
    "    def make_filename(row):\n",
    "        return f\"{str(row['siteCode']).lower()}_{str(row['SpeciesCode']).lower()}_\"\n",
    "    merged['expected_filename_prefix'] = merged.apply(make_filename, axis=1)\n",
    "    if optimased_dir is not None:\n",
    "        merged['expected_path_prefix'] = merged['expected_filename_prefix'].apply(\n",
    "            lambda x: str(Path(optimased_dir) / x)\n",
    "        )\n",
    "\n",
    "    #  Compare to notActive_site_species.csv \n",
    "    if check_nonActive_path is not None:\n",
    "        try:\n",
    "            existing = pd.read_csv(check_nonActive_path, encoding='utf-8')\n",
    "            existing_set = set(zip(existing['siteCode'], existing['SpeciesCode']))\n",
    "            affected_year_set = set(zip(merged['siteCode'], merged['SpeciesCode']))\n",
    "            new_combos = affected_year_set - existing_set\n",
    "            new_affected_year = merged[\n",
    "                merged.apply(lambda row: (row['siteCode'], row['SpeciesCode']) in new_combos, axis=1)\n",
    "            ]\n",
    "            print(f\"\\nNew site/species combos NOT in notActive_site_species.csv: {len(new_affected_year)}\")\n",
    "            print(new_affected_year)\n",
    "            # Save to output\n",
    "            new_affected_year.to_csv(output_notActive_site_species, index=False, encoding='utf-8')\n",
    "            print(f\"\\nSaved new combos to: {output_notActive_site_species}\")\n",
    "            return new_affected_year\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not filter by notActive_site_species.csv: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nNo notActive_site_species.csv provided for comparison.\")\n",
    "    merged.to_csv(output_notActive_site_species, index=False, encoding='utf-8')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb3bced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total site/species combos with 100% missing in 2023: 41\n",
      "    siteCode SpeciesCode  month_number\n",
      "40       EN5       PM2.5            12\n",
      "61       GT1         NO2            12\n",
      "62       GT1        PM10            12\n",
      "63       GT1       PM2.5            12\n",
      "65       HG1       PM2.5            12\n",
      "80       IS2       PM2.5            12\n",
      "83       IS6       PM2.5            12\n",
      "86       KF1        PM25            12\n",
      "98       ME2         NO2            12\n",
      "99       ME2        PM10            12\n",
      "100      ME2       PM2.5            12\n",
      "101      MEA         NO2            12\n",
      "102      MEA       PM2.5            12\n",
      "103      MEB        PM10            12\n",
      "104      MEB       PM2.5            12\n",
      "105      MR8        PM25            12\n",
      "110      RHI         NO2            12\n",
      "111      RHI        PM10            12\n",
      "112      RHI       PM2.5            12\n",
      "117      RI2       PM2.5            12\n",
      "121      TH2       PM2.5            12\n",
      "124      TH5         NO2            12\n",
      "125      TH5        PM10            12\n",
      "126      TH5       PM2.5            12\n",
      "127      TH6         NO2            12\n",
      "128      TH6          O3            12\n",
      "129      TH6        PM10            12\n",
      "130      TH7         NO2            12\n",
      "131      TH7       PM2.5            12\n",
      "138      WA7       PM2.5            12\n",
      "139      WA9         NO2            12\n",
      "141      WA9       PM2.5            12\n",
      "144      WAA       PM2.5            12\n",
      "146      WAB       PM2.5            12\n",
      "147      WAC         NO2            12\n",
      "148      WAC       PM2.5            12\n",
      "152      WM5       PM2.5            12\n",
      "153      WM6       PM2.5            12\n",
      "154      WME         NO2            12\n",
      "155      WME          O3            12\n",
      "156      WME       PM2.5            12\n",
      "\n",
      "New site/species combos NOT in notActive_site_species.csv: 492\n",
      "    siteCode SpeciesCode  month_number                             filename  \\\n",
      "0        EN5       PM2.5            12  EN5_PM2.5_2023-04-01_2023-04-30.csv   \n",
      "1        EN5       PM2.5            12  EN5_PM2.5_2023-08-01_2023-08-31.csv   \n",
      "2        EN5       PM2.5            12  EN5_PM2.5_2023-12-01_2023-12-31.csv   \n",
      "3        EN5       PM2.5            12  EN5_PM2.5_2023-02-01_2023-02-28.csv   \n",
      "4        EN5       PM2.5            12  EN5_PM2.5_2023-01-01_2023-01-31.csv   \n",
      "..       ...         ...           ...                                  ...   \n",
      "487      WME       PM2.5            12  WME_PM2.5_2023-03-01_2023-03-31.csv   \n",
      "488      WME       PM2.5            12  WME_PM2.5_2023-05-01_2023-05-31.csv   \n",
      "489      WME       PM2.5            12  WME_PM2.5_2023-11-01_2023-11-30.csv   \n",
      "490      WME       PM2.5            12  WME_PM2.5_2023-10-01_2023-10-31.csv   \n",
      "491      WME       PM2.5            12  WME_PM2.5_2023-09-01_2023-09-30.csv   \n",
      "\n",
      "                                                  path  \\\n",
      "0    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "1    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "2    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "3    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "4    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "..                                                 ...   \n",
      "487  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "488  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "489  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "490  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "491  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "\n",
      "    expected_filename_prefix  \n",
      "0                 en5_pm2.5_  \n",
      "1                 en5_pm2.5_  \n",
      "2                 en5_pm2.5_  \n",
      "3                 en5_pm2.5_  \n",
      "4                 en5_pm2.5_  \n",
      "..                       ...  \n",
      "487               wme_pm2.5_  \n",
      "488               wme_pm2.5_  \n",
      "489               wme_pm2.5_  \n",
      "490               wme_pm2.5_  \n",
      "491               wme_pm2.5_  \n",
      "\n",
      "[492 rows x 6 columns]\n",
      "\n",
      "Saved new combos to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/missing/notActive_siteSpecies_2023.csv\n",
      "\n",
      "Total site/species combos with 100% missing in 2024: 63\n",
      "    siteCode SpeciesCode  month_number\n",
      "4        BL0         NO2            12\n",
      "5        BL0          O3            12\n",
      "6        BL0        PM10            12\n",
      "7        BL0       PM2.5            12\n",
      "8        BL0         SO2            12\n",
      "..       ...         ...           ...\n",
      "147      WM0         NO2            12\n",
      "148      WM0       PM2.5            12\n",
      "153      WME         NO2            12\n",
      "154      WME          O3            12\n",
      "155      WME       PM2.5            12\n",
      "\n",
      "[63 rows x 3 columns]\n",
      "\n",
      "New site/species combos NOT in notActive_site_species.csv: 756\n",
      "    siteCode SpeciesCode  month_number                             filename  \\\n",
      "0        BL0         NO2            12    BL0_NO2_2024-04-01_2024-04-30.csv   \n",
      "1        BL0         NO2            12    BL0_NO2_2024-08-01_2024-08-31.csv   \n",
      "2        BL0         NO2            12    BL0_NO2_2024-12-01_2024-12-31.csv   \n",
      "3        BL0         NO2            12    BL0_NO2_2024-02-01_2024-02-29.csv   \n",
      "4        BL0         NO2            12    BL0_NO2_2024-01-01_2024-01-31.csv   \n",
      "..       ...         ...           ...                                  ...   \n",
      "751      WME       PM2.5            12  WME_PM2.5_2024-03-01_2024-03-31.csv   \n",
      "752      WME       PM2.5            12  WME_PM2.5_2024-05-01_2024-05-31.csv   \n",
      "753      WME       PM2.5            12  WME_PM2.5_2024-11-01_2024-11-30.csv   \n",
      "754      WME       PM2.5            12  WME_PM2.5_2024-10-01_2024-10-31.csv   \n",
      "755      WME       PM2.5            12  WME_PM2.5_2024-09-01_2024-09-30.csv   \n",
      "\n",
      "                                                  path  \\\n",
      "0    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "1    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "2    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "3    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "4    /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "..                                                 ...   \n",
      "751  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "752  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "753  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "754  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "755  /Users/burdzhuchaglayan/Desktop/data science p...   \n",
      "\n",
      "    expected_filename_prefix  \n",
      "0                   bl0_no2_  \n",
      "1                   bl0_no2_  \n",
      "2                   bl0_no2_  \n",
      "3                   bl0_no2_  \n",
      "4                   bl0_no2_  \n",
      "..                       ...  \n",
      "751               wme_pm2.5_  \n",
      "752               wme_pm2.5_  \n",
      "753               wme_pm2.5_  \n",
      "754               wme_pm2.5_  \n",
      "755               wme_pm2.5_  \n",
      "\n",
      "[756 rows x 6 columns]\n",
      "\n",
      "Saved new combos to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/missing/notActive_siteSpecies_2024.csv\n",
      "\n",
      "Total site/species combos with 100% missing in 2025: 0\n",
      "Empty DataFrame\n",
      "Columns: [siteCode, SpeciesCode, month_number]\n",
      "Index: []\n",
      "\n",
      "New site/species combos NOT in notActive_site_species.csv: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Saved new combos to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/missing/notActive_siteSpecies_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the function for 2023\n",
    "affected_2023 = analyse_affected_sites_year(\n",
    "    nanValue_path,\n",
    "    output_notActive_siteSpecies_2023,\n",
    "    check_nonActive_path=existing_nonactive_path,\n",
    "    year=2023\n",
    ")\n",
    "\n",
    "# Run the function for 2024\n",
    "affected_2024 = analyse_affected_sites_year(\n",
    "    nanValue_path,\n",
    "    output_notActive_siteSpecies_2024,\n",
    "    check_nonActive_path=existing_nonactive_path,\n",
    "    year=2024\n",
    ")\n",
    "\n",
    "# Run the function for 2025\n",
    "affected_2025 = analyse_affected_sites_year(\n",
    "    nanValue_path,\n",
    "    output_notActive_siteSpecies_2025,\n",
    "    check_nonActive_path=existing_nonactive_path,\n",
    "    year=2025\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bf7e8",
   "metadata": {},
   "source": [
    "## 2) calculate_adjusted_issue_rate\n",
    "    This function calculates the adjusted issue rate of files with more than 20% missing @Value in the optimased directory, after excluding files associated with site/species combinations listed in the notActive_siteSpecies_2023.csv and notActive_siteSpecies_2024.csv files.\n",
    "\n",
    "- How it works:\n",
    "\n",
    "    - Reads the log file (logs_missin_value.csv) to count the number of files with >20% missing @Value.\n",
    "    - Recursively counts all CSV files in the optimased directory.\n",
    "    - Excludes from the total any files that match site/species pairs found in the not-active lists for 2023 and 2024.\n",
    "    - Calculates the issue rate as: issue rate = ( Number of files with >20% missing @Value / total number of files) Ã— 100\n",
    "\n",
    "\n",
    "- Purpose:\n",
    "\n",
    "    - To provide a more accurate data quality metric by removing files that are already flagged as not active for 2023 and 2024, so the issue rate reflects only the remaining, relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "008961d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_issue_rate(optimased_root, log_file, notactive_files, all_csv_files):\n",
    "    \"\"\"\n",
    "    Calculate the adjusted issue rate of files with >20% missing values,\n",
    "    excluding files associated with not-active site/species combinations.\n",
    "    Now excludes files by matching the 'filename' column in notActive_siteSpecies_... CSVs (using full paths, no lowercasing).\n",
    "    \"\"\"\n",
    "    # Read log file (files with >20% missing @Value)\n",
    "    df_log = pd.read_csv(log_file)\n",
    "    # Use full path as filename (no lowercasing)\n",
    "    df_log['filename'] = df_log['filename'].apply(lambda x: str(x).strip())\n",
    "    all_csv_files = set(Path(f).name for f in all_csv_files)\n",
    "\n",
    "    # Read notActive_siteSpecies_2023/2024 and collect filenames to exclude (by filename column if present)\n",
    "    exclude_filenames = set()\n",
    "    for naf in notactive_files:\n",
    "        if Path(naf).exists():\n",
    "            df_exclude = pd.read_csv(naf)\n",
    "            if 'filename' in df_exclude.columns:\n",
    "                # Use the filename column directly (full path, no lowercasing)\n",
    "                exclude_filenames.update(df_exclude['filename'].dropna().astype(str).str.strip())\n",
    "            else:\n",
    "                # Fallback to old pattern-based exclusion\n",
    "                for _, row in df_exclude.iterrows():\n",
    "                    site = str(row['siteCode'])\n",
    "                    species = str(row['SpeciesCode'])\n",
    "                    pattern = f\"{site}_{species}_\"\n",
    "                    matches = [fname for fname in all_csv_files if Path(fname).name.startswith(pattern)]\n",
    "                    exclude_filenames.update(matches)\n",
    "\n",
    "    # Remove excluded files from total\n",
    "    adjusted_total_files = len([f for f in all_csv_files if str(f) not in exclude_filenames])\n",
    "\n",
    "    # Filter log to only include files not in exclude_filenames\n",
    "    df_log_filtered = df_log[~df_log['filename'].isin(exclude_filenames)]\n",
    "    files_with_high_missing = df_log_filtered['filename'].nunique()\n",
    "\n",
    "    print(\"Total CSV files in optimased:\", len(all_csv_files))\n",
    "    print(\"Total files after exclusion:\", adjusted_total_files)\n",
    "    print(\"Files with >20% missing after exclusion:\", files_with_high_missing)\n",
    "    print(\"Sample exclude_filenames:\", list(exclude_filenames)[:5])\n",
    "    print(\"Sample log filenames:\", df_log['filename'].head())\n",
    "    print(\"Sample all_csv_filenames:\", list(all_csv_files)[:5])\n",
    "\n",
    "    if adjusted_total_files == 0:\n",
    "        return 0.0\n",
    "    issue_rate = (files_with_high_missing / adjusted_total_files) * 100\n",
    "    return issue_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc1dd25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files in optimased: 0\n",
      "Total files after exclusion: 0\n",
      "Files with >20% missing after exclusion: 2936\n",
      "Sample exclude_filenames: ['TH2_PM2.5_2023-03-01_2023-03-31.csv', 'TH6_O3_2023-02-01_2023-02-28.csv', 'GR4_NO2_2024-05-01_2024-05-31.csv', 'BL0_PM10_2024-12-01_2024-12-31.csv', 'TH6_O3_2024-08-01_2024-08-31.csv']\n",
      "Sample log filenames: 0       BL0_CO_2023-04-01_2023-04-30.csv\n",
      "1       BT4_O3_2023-04-01_2023-04-30.csv\n",
      "2      BT4_SO2_2023-04-01_2023-04-30.csv\n",
      "3    BT5_PM2.5_2023-04-01_2023-04-30.csv\n",
      "4    BT6_PM2.5_2023-04-01_2023-04-30.csv\n",
      "Name: filename, dtype: object\n",
      "Sample all_csv_filenames: []\n",
      "Adjusted issue rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "notactive_files = [output_notActive_siteSpecies_2023, output_notActive_siteSpecies_2024]\n",
    "adjusted_issue_rate = calculate_adjusted_issue_rate(\n",
    "    optimased_root, log_file, notactive_files, all_csv_files\n",
    ")\n",
    "print(\"Adjusted issue rate: {:.2f}%\".format(adjusted_issue_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9150c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_issue_rate_excluding_notactive(optimased_dir, notactive_files):\n",
    "    \"\"\"\n",
    "    Checks all files in optimased_dir for >20% missing @Value, and calculates the issue rate\n",
    "    after excluding files listed in notActive_siteSpecies_2023/2024 (by filename).\n",
    "    Returns: (issue_rate, total_files_checked, files_with_high_missing, missing_values_log)\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "\n",
    "    # Gather all CSV files recursively\n",
    "    all_csv_files = list(Path(optimased_dir).rglob(\"*.csv\"))\n",
    "    all_csv_filenames = set(f.name for f in all_csv_files)\n",
    "\n",
    "    # Collect filenames to exclude from notactive_files\n",
    "    exclude_filenames = set()\n",
    "    for naf in notactive_files:\n",
    "        if Path(naf).exists():\n",
    "            df_exclude = pd.read_csv(naf)\n",
    "            if 'filename' in df_exclude.columns:\n",
    "                exclude_filenames.update(df_exclude['filename'].dropna().astype(str).str.strip())\n",
    "            else:\n",
    "                for _, row in df_exclude.iterrows():\n",
    "                    site = str(row['siteCode'])\n",
    "                    species = str(row['SpeciesCode'])\n",
    "                    pattern = f\"{site}_{species}_\"\n",
    "                    matches = [fname for fname in all_csv_filenames if fname.startswith(pattern)]\n",
    "                    exclude_filenames.update(matches)\n",
    "\n",
    "    # Filter files to check (exclude notactive)\n",
    "    files_to_check = [f for f in all_csv_files if f.name not in exclude_filenames]\n",
    "    total_files_checked = len(files_to_check)\n",
    "    files_with_high_missing = 0\n",
    "    missing_values_log = []\n",
    "\n",
    "    for filepath in files_to_check:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            if '@Value' in df.columns and len(df) > 0:\n",
    "                missing_values = df['@Value'].isna().sum()\n",
    "                empty_value_percentage = (100 * missing_values / len(df))\n",
    "                if empty_value_percentage > 20:\n",
    "                    files_with_high_missing += 1\n",
    "                    missing_values_log.append({\n",
    "                        'filename': filepath.name,\n",
    "                        'path': str(filepath),\n",
    "                        'EmptyValuePercentage': round(empty_value_percentage, 2)\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "\n",
    "    issue_rate = (files_with_high_missing / total_files_checked * 100) if total_files_checked > 0 else 0.0\n",
    "    print(f\"Total files checked: {total_files_checked}\")\n",
    "    print(f\"Files with >20% missing @Value: {files_with_high_missing}\")\n",
    "    print(f\"Issue rate (excluding not-active): {issue_rate:.2f}%\")\n",
    "    print(f\"Files excluded (not-active): {len(exclude_filenames)}\")\n",
    "    return issue_rate, total_files_checked, files_with_high_missing, missing_values_log\n",
    "\n",
    "# Example usage:\n",
    "notactive_files = [output_notActive_siteSpecies_2023, output_notActive_siteSpecies_2024]\n",
    "issue_rate, total_files_checked, files_with_high_missing, missing_values_log = detailed_issue_rate_excluding_notactive(\n",
    "    optimased_root, notactive_files\n",
    ")\n",
    "print(f\"\\nFinal issue rate (excluding not-active): {issue_rate:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
