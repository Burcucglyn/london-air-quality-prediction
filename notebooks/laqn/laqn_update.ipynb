{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d96e",
   "metadata": {},
   "source": [
    "# LAQN Updated Active sites/species File Function:\n",
    "- laqn_remove notebook got slower so i will move the update function here.\n",
    "- Start with paths and modules to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74ea5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# paths beloww\n",
    "base_dir = Path(\"/Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels\")\n",
    "# the metadata file for nan @value \n",
    "nanValue_path = base_dir / \"data\" / \"laqn\" / \"missing\" / \"logs_nan_value.csv\"\n",
    "\n",
    "# Month abbreviation list for reference/use in functions\n",
    "month_list = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "# analyse_affected_sites_2023 path below.\n",
    "check_nonActive_path = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies.csv\"\n",
    "output_notActive_siteSpecies_2023 = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies_2023.csv\"\n",
    "\n",
    "# checks the removed site-species combinations against active list and removes them\n",
    "existing_nonactive_path = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_site_species.csv\"\n",
    "\n",
    "# analyse_affected_sites_2024 path below.\n",
    "output_notActive_siteSpecies_2024 = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies_2024.csv\"\n",
    "\n",
    "# analyse_affected_sites_2025 path below.\n",
    "output_notActive_siteSpecies_2025 = base_dir / \"data\" / \"laqn\" / \"missing\" / \"notActive_siteSpecies_2025.csv\"\n",
    "\n",
    "#calculating new issue rate, so I will be count all files in optimased folder and than recalculate the issue rate taking \n",
    "# out the notAcvtive_2024/2023 files from output_notActive_siteSpecies_2023, output_notActive_siteSpecies_2024 csv's\n",
    "optimased_root = base_dir / \"data\" / \"laqn\" / \"optimased\"\n",
    "log_file = base_dir / \"data\" / \"laqn\" / \"missing\" / \"logs_missin_value.csv\"\n",
    "\n",
    "# Collect all CSV files in the optimased_root directory\n",
    "all_csv_files = list(Path(optimased_root).glob(\"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56e9c7",
   "metadata": {},
   "source": [
    "## 1) analyse_affected_sites_2023 function to identify site/species with all months missing in 2023\n",
    "    what does function:\n",
    "\n",
    "- scans the missing value log for site/species combinations that have 100% missing values for all 12 months in 2023\n",
    "- if an existing non-active site/species list is provided, only new combinations not already in that list are included\n",
    "- prints the results and saves them as a CSV file for further review\n",
    "\n",
    "    paths:\n",
    "\n",
    "- nanValue_path: base_dir / data / laqn / missing / logs_nan_value.csv\n",
    "- output_notActive_siteSpecies_2023: base_dir / data / laqn / missing / notActive_siteSpecies_2023.csv\n",
    "- existing_nonactive_path (optional): base_dir / data / laqn / missing / notActive_site_species.csv\n",
    "- new csv file save as: notActive_siteSpecies_2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_affected_sites_year(\n",
    "    nanValue_path,  # value_100filtered_missing.csv\n",
    "    output_notActive_site_species,  # recommended output path\n",
    "    check_nonActive_path=None,  # notActive_site_species.csv\n",
    "    year=2025,\n",
    "    optimased_dir=None  # Optional: base dir for constructing file paths\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Checks value_100filtered_missing.csv for site/species combos with 100% missing values for all months in the given year.\n",
    "    2. Compares to notActive_site_species.csv and finds new combos not already listed.\n",
    "    3. Adds actual filenames (and optionally full paths) for each combo from logs_nan_value.csv.\n",
    "    4. Prints and saves these new combos to output_notActive_site_species.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_month_number(month_str):\n",
    "        month_map = {abbr: i+1 for i, abbr in enumerate(month_list)}\n",
    "        m = re.match(r\"\\d{4}_(\\w{3})\", str(month_str).lower())\n",
    "        if m:\n",
    "            return month_map.get(m.group(1), None)\n",
    "        return None\n",
    "\n",
    "    # Load and filter for year, 100% missing\n",
    "    df = pd.read_csv(nanValue_path, encoding='utf-8')\n",
    "    df['month_number'] = df['month'].apply(extract_month_number)\n",
    "    df = df[df['month_number'].notna()].copy()\n",
    "    df_year = df[df['year'] == year]\n",
    "    summary_year = df_year.groupby(['siteCode', 'SpeciesCode'])['month_number'].nunique().reset_index()\n",
    "    affected_year = summary_year[summary_year['month_number'] == 12].copy()\n",
    "\n",
    "    print(f\"\\nTotal site/species combos with 100% missing in {year}: {len(affected_year)}\")\n",
    "    print(affected_year)\n",
    "\n",
    "    # Merge to get actual filenames and (optionally) full paths from nanValue_path\n",
    "    merged = pd.merge(\n",
    "        affected_year,\n",
    "        df_year[['siteCode', 'SpeciesCode', 'filename', 'path']].drop_duplicates(),\n",
    "        on=['siteCode', 'SpeciesCode'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Add expected filename prefix for reference\n",
    "    def make_filename(row):\n",
    "        return f\"{str(row['siteCode']).lower()}_{str(row['SpeciesCode']).lower()}_\"\n",
    "    merged['expected_filename_prefix'] = merged.apply(make_filename, axis=1)\n",
    "    if optimased_dir is not None:\n",
    "        merged['expected_path_prefix'] = merged['expected_filename_prefix'].apply(\n",
    "            lambda x: str(Path(optimased_dir) / x)\n",
    "        )\n",
    "\n",
    "    #  Compare to notActive_site_species.csv \n",
    "    if check_nonActive_path is not None:\n",
    "        try:\n",
    "            existing = pd.read_csv(check_nonActive_path, encoding='utf-8')\n",
    "            existing_set = set(zip(existing['siteCode'], existing['SpeciesCode']))\n",
    "            affected_year_set = set(zip(merged['siteCode'], merged['SpeciesCode']))\n",
    "            new_combos = affected_year_set - existing_set\n",
    "            new_affected_year = merged[\n",
    "                merged.apply(lambda row: (row['siteCode'], row['SpeciesCode']) in new_combos, axis=1)\n",
    "            ]\n",
    "            print(f\"\\nNew site/species combos NOT in notActive_site_species.csv: {len(new_affected_year)}\")\n",
    "            print(new_affected_year)\n",
    "            # Save to output\n",
    "            new_affected_year.to_csv(output_notActive_site_species, index=False, encoding='utf-8')\n",
    "            print(f\"\\nSaved new combos to: {output_notActive_site_species}\")\n",
    "            return new_affected_year\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not filter by notActive_site_species.csv: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nNo notActive_site_species.csv provided for comparison.\")\n",
    "    merged.to_csv(output_notActive_site_species, index=False, encoding='utf-8')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb3bced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total site/species combos with 100% missing in 2023: 41\n",
      "    siteCode SpeciesCode  month_number\n",
      "40       EN5       PM2.5            12\n",
      "61       GT1         NO2            12\n",
      "62       GT1        PM10            12\n",
      "63       GT1       PM2.5            12\n",
      "65       HG1       PM2.5            12\n",
      "80       IS2       PM2.5            12\n",
      "83       IS6       PM2.5            12\n",
      "86       KF1        PM25            12\n",
      "98       ME2         NO2            12\n",
      "99       ME2        PM10            12\n",
      "100      ME2       PM2.5            12\n",
      "101      MEA         NO2            12\n",
      "102      MEA       PM2.5            12\n",
      "103      MEB        PM10            12\n",
      "104      MEB       PM2.5            12\n",
      "105      MR8        PM25            12\n",
      "110      RHI         NO2            12\n",
      "111      RHI        PM10            12\n",
      "112      RHI       PM2.5            12\n",
      "117      RI2       PM2.5            12\n",
      "121      TH2       PM2.5            12\n",
      "124      TH5         NO2            12\n",
      "125      TH5        PM10            12\n",
      "126      TH5       PM2.5            12\n",
      "127      TH6         NO2            12\n",
      "128      TH6          O3            12\n",
      "129      TH6        PM10            12\n",
      "130      TH7         NO2            12\n",
      "131      TH7       PM2.5            12\n",
      "138      WA7       PM2.5            12\n",
      "139      WA9         NO2            12\n",
      "141      WA9       PM2.5            12\n",
      "144      WAA       PM2.5            12\n",
      "146      WAB       PM2.5            12\n",
      "147      WAC         NO2            12\n",
      "148      WAC       PM2.5            12\n",
      "152      WM5       PM2.5            12\n",
      "153      WM6       PM2.5            12\n",
      "154      WME         NO2            12\n",
      "155      WME          O3            12\n",
      "156      WME       PM2.5            12\n",
      "\n",
      "New site/species combos NOT in notActive_site_species.csv: 41\n",
      "    siteCode SpeciesCode  month_number expected_filename_prefix\n",
      "40       EN5       PM2.5            12               en5_pm2.5_\n",
      "61       GT1         NO2            12                 gt1_no2_\n",
      "62       GT1        PM10            12                gt1_pm10_\n",
      "63       GT1       PM2.5            12               gt1_pm2.5_\n",
      "65       HG1       PM2.5            12               hg1_pm2.5_\n",
      "80       IS2       PM2.5            12               is2_pm2.5_\n",
      "83       IS6       PM2.5            12               is6_pm2.5_\n",
      "86       KF1        PM25            12                kf1_pm25_\n",
      "98       ME2         NO2            12                 me2_no2_\n",
      "99       ME2        PM10            12                me2_pm10_\n",
      "100      ME2       PM2.5            12               me2_pm2.5_\n",
      "101      MEA         NO2            12                 mea_no2_\n",
      "102      MEA       PM2.5            12               mea_pm2.5_\n",
      "103      MEB        PM10            12                meb_pm10_\n",
      "104      MEB       PM2.5            12               meb_pm2.5_\n",
      "105      MR8        PM25            12                mr8_pm25_\n",
      "110      RHI         NO2            12                 rhi_no2_\n",
      "111      RHI        PM10            12                rhi_pm10_\n",
      "112      RHI       PM2.5            12               rhi_pm2.5_\n",
      "117      RI2       PM2.5            12               ri2_pm2.5_\n",
      "121      TH2       PM2.5            12               th2_pm2.5_\n",
      "124      TH5         NO2            12                 th5_no2_\n",
      "125      TH5        PM10            12                th5_pm10_\n",
      "126      TH5       PM2.5            12               th5_pm2.5_\n",
      "127      TH6         NO2            12                 th6_no2_\n",
      "128      TH6          O3            12                  th6_o3_\n",
      "129      TH6        PM10            12                th6_pm10_\n",
      "130      TH7         NO2            12                 th7_no2_\n",
      "131      TH7       PM2.5            12               th7_pm2.5_\n",
      "138      WA7       PM2.5            12               wa7_pm2.5_\n",
      "139      WA9         NO2            12                 wa9_no2_\n",
      "141      WA9       PM2.5            12               wa9_pm2.5_\n",
      "144      WAA       PM2.5            12               waa_pm2.5_\n",
      "146      WAB       PM2.5            12               wab_pm2.5_\n",
      "147      WAC         NO2            12                 wac_no2_\n",
      "148      WAC       PM2.5            12               wac_pm2.5_\n",
      "152      WM5       PM2.5            12               wm5_pm2.5_\n",
      "153      WM6       PM2.5            12               wm6_pm2.5_\n",
      "154      WME         NO2            12                 wme_no2_\n",
      "155      WME          O3            12                  wme_o3_\n",
      "156      WME       PM2.5            12               wme_pm2.5_\n",
      "\n",
      "Saved new combos to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/missing/notActive_siteSpecies_2023.csv\n",
      "\n",
      "Total site/species combos with 100% missing in 2024: 63\n",
      "    siteCode SpeciesCode  month_number\n",
      "4        BL0         NO2            12\n",
      "5        BL0          O3            12\n",
      "6        BL0        PM10            12\n",
      "7        BL0       PM2.5            12\n",
      "8        BL0         SO2            12\n",
      "..       ...         ...           ...\n",
      "147      WM0         NO2            12\n",
      "148      WM0       PM2.5            12\n",
      "153      WME         NO2            12\n",
      "154      WME          O3            12\n",
      "155      WME       PM2.5            12\n",
      "\n",
      "[63 rows x 3 columns]\n",
      "\n",
      "New site/species combos NOT in notActive_site_species.csv: 63\n",
      "    siteCode SpeciesCode  month_number expected_filename_prefix\n",
      "4        BL0         NO2            12                 bl0_no2_\n",
      "5        BL0          O3            12                  bl0_o3_\n",
      "6        BL0        PM10            12                bl0_pm10_\n",
      "7        BL0       PM2.5            12               bl0_pm2.5_\n",
      "8        BL0         SO2            12                 bl0_so2_\n",
      "..       ...         ...           ...                      ...\n",
      "147      WM0         NO2            12                 wm0_no2_\n",
      "148      WM0       PM2.5            12               wm0_pm2.5_\n",
      "153      WME         NO2            12                 wme_no2_\n",
      "154      WME          O3            12                  wme_o3_\n",
      "155      WME       PM2.5            12               wme_pm2.5_\n",
      "\n",
      "[63 rows x 4 columns]\n",
      "\n",
      "Saved new combos to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/missing/notActive_siteSpecies_2024.csv\n",
      "\n",
      "Total site/species combos with 100% missing in 2025: 0\n",
      "Empty DataFrame\n",
      "Columns: [siteCode, SpeciesCode, month_number]\n",
      "Index: []\n",
      "\n",
      "New site/species combos NOT in notActive_site_species.csv: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Saved new combos to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/missing/notActive_siteSpecies_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the function for 2023\n",
    "affected_2023 = analyse_affected_sites_year(\n",
    "    nanValue_path,\n",
    "    output_notActive_siteSpecies_2023,\n",
    "    check_nonActive_path=existing_nonactive_path,\n",
    "    year=2023\n",
    ")\n",
    "\n",
    "# Run the function for 2024\n",
    "affected_2024 = analyse_affected_sites_year(\n",
    "    nanValue_path,\n",
    "    output_notActive_siteSpecies_2024,\n",
    "    check_nonActive_path=existing_nonactive_path,\n",
    "    year=2024\n",
    ")\n",
    "\n",
    "# Run the function for 2025\n",
    "affected_2025 = analyse_affected_sites_year(\n",
    "    nanValue_path,\n",
    "    output_notActive_siteSpecies_2025,\n",
    "    check_nonActive_path=existing_nonactive_path,\n",
    "    year=2025\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bf7e8",
   "metadata": {},
   "source": [
    "## 2) calculate_adjusted_issue_rate\n",
    "    This function calculates the adjusted issue rate of files with more than 20% missing @Value in the optimased directory, after excluding files associated with site/species combinations listed in the notActive_siteSpecies_2023.csv and notActive_siteSpecies_2024.csv files.\n",
    "\n",
    "- How it works:\n",
    "\n",
    "    - Reads the log file (logs_missin_value.csv) to count the number of files with >20% missing @Value.\n",
    "    - Recursively counts all CSV files in the optimased directory.\n",
    "    - Excludes from the total any files that match site/species pairs found in the not-active lists for 2023 and 2024.\n",
    "    - Calculates the issue rate as: issue rate = ( Number of files with >20% missing @Value / total number of files) Ã— 100\n",
    "\n",
    "\n",
    "- Purpose:\n",
    "\n",
    "    - To provide a more accurate data quality metric by removing files that are already flagged as not active for 2023 and 2024, so the issue rate reflects only the remaining, relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "008961d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_issue_rate(optimased_root, log_file, notactive_files, all_csv_files):\n",
    "    \"\"\"\n",
    "    Calculate adjusted issue rate by excluding files listed in notActive_siteSpecies_2023/2024 from total files.\n",
    "    \"\"\"\n",
    "    # Read log file (files with >20% missing @Value)\n",
    "    df_log = pd.read_csv(log_file)\n",
    "    files_with_high_missing = df_log['filename'].nunique()\n",
    "\n",
    "    # Normalize filenames to just the filename (no path)\n",
    "    df_log['filename'] = df_log['filename'].apply(lambda x: Path(x).name.strip().lower())\n",
    "    all_csv_filenames = set(f.name.strip().lower() for f in all_csv_files)\n",
    "\n",
    "    # Read notActive_siteSpecies_2023/2024 and collect filenames to exclude\n",
    "    exclude_filenames = set()\n",
    "    for naf in notactive_files:\n",
    "        if Path(naf).exists():\n",
    "            df_exclude = pd.read_csv(naf)\n",
    "            for _, row in df_exclude.iterrows():\n",
    "                site = str(row['siteCode'])\n",
    "                species = str(row['SpeciesCode'])\n",
    "                pattern = f\"{site.lower()}_{species.lower()}_\"\n",
    "                matches = [fname for fname in all_csv_filenames if fname.startswith(pattern)]\n",
    "                print(f\"Excluding for {site}_{species}: {matches}\")  # Debug print\n",
    "                exclude_filenames.update(matches)\n",
    "\n",
    "    # Remove excluded files from total\n",
    "    adjusted_total_files = len([f for f in all_csv_files if f.name not in exclude_filenames])\n",
    "\n",
    "    # Filter log to only include files not in exclude_filenames\n",
    "    df_log_filtered = df_log[~df_log['filename'].isin(exclude_filenames)]\n",
    "    files_with_high_missing = df_log_filtered['filename'].nunique()\n",
    "\n",
    "    print(\"Total CSV files in optimased:\", len(all_csv_files))\n",
    "    print(\"Total files after exclusion:\", adjusted_total_files)\n",
    "    print(\"Files with >20% missing after exclusion:\", files_with_high_missing)\n",
    "    print(\"Sample exclude_filenames:\", list(exclude_filenames)[:5])\n",
    "    print(\"Sample log filenames:\", df_log['filename'].head())\n",
    "    print(\"Sample all_csv_filenames:\", list(all_csv_filenames)[:5])\n",
    "\n",
    "    if adjusted_total_files == 0:\n",
    "        return 0.0\n",
    "    issue_rate = (files_with_high_missing / adjusted_total_files) * 100\n",
    "    return issue_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc1dd25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding for EN5_PM2.5: []\n",
      "Excluding for GT1_NO2: []\n",
      "Excluding for GT1_PM10: []\n",
      "Excluding for GT1_PM2.5: []\n",
      "Excluding for HG1_PM2.5: []\n",
      "Excluding for IS2_PM2.5: []\n",
      "Excluding for IS6_PM2.5: []\n",
      "Excluding for KF1_PM25: []\n",
      "Excluding for ME2_NO2: []\n",
      "Excluding for ME2_PM10: []\n",
      "Excluding for ME2_PM2.5: []\n",
      "Excluding for MEA_NO2: []\n",
      "Excluding for MEA_PM2.5: []\n",
      "Excluding for MEB_PM10: []\n",
      "Excluding for MEB_PM2.5: []\n",
      "Excluding for MR8_PM25: []\n",
      "Excluding for RHI_NO2: []\n",
      "Excluding for RHI_PM10: []\n",
      "Excluding for RHI_PM2.5: []\n",
      "Excluding for RI2_PM2.5: []\n",
      "Excluding for TH2_PM2.5: []\n",
      "Excluding for TH5_NO2: []\n",
      "Excluding for TH5_PM10: []\n",
      "Excluding for TH5_PM2.5: []\n",
      "Excluding for TH6_NO2: []\n",
      "Excluding for TH6_O3: []\n",
      "Excluding for TH6_PM10: []\n",
      "Excluding for TH7_NO2: []\n",
      "Excluding for TH7_PM2.5: []\n",
      "Excluding for WA7_PM2.5: []\n",
      "Excluding for WA9_NO2: []\n",
      "Excluding for WA9_PM2.5: []\n",
      "Excluding for WAA_PM2.5: []\n",
      "Excluding for WAB_PM2.5: []\n",
      "Excluding for WAC_NO2: []\n",
      "Excluding for WAC_PM2.5: []\n",
      "Excluding for WM5_PM2.5: []\n",
      "Excluding for WM6_PM2.5: []\n",
      "Excluding for WME_NO2: []\n",
      "Excluding for WME_O3: []\n",
      "Excluding for WME_PM2.5: []\n",
      "Excluding for BL0_NO2: []\n",
      "Excluding for BL0_O3: []\n",
      "Excluding for BL0_PM10: []\n",
      "Excluding for BL0_PM2.5: []\n",
      "Excluding for BL0_SO2: []\n",
      "Excluding for BT4_O3: []\n",
      "Excluding for EN5_PM10: []\n",
      "Excluding for EN5_PM2.5: []\n",
      "Excluding for GR4_NO2: []\n",
      "Excluding for GR4_O3: []\n",
      "Excluding for GR4_PM10: []\n",
      "Excluding for GR4_PM2.5: []\n",
      "Excluding for GR8_PM2.5: []\n",
      "Excluding for GT1_NO2: []\n",
      "Excluding for GT1_PM10: []\n",
      "Excluding for GT1_PM2.5: []\n",
      "Excluding for HI0_NO2: []\n",
      "Excluding for HI0_O3: []\n",
      "Excluding for HI0_PM10: []\n",
      "Excluding for HK6_NO2: []\n",
      "Excluding for HK6_PM10: []\n",
      "Excluding for HK6_PM2.5: []\n",
      "Excluding for HV1_PM10: []\n",
      "Excluding for HV1_PM2.5: []\n",
      "Excluding for IS2_PM2.5: []\n",
      "Excluding for IS6_PM2.5: []\n",
      "Excluding for KF1_PM25: []\n",
      "Excluding for LB5_NO2: []\n",
      "Excluding for LB5_PM10: []\n",
      "Excluding for LB5_SO2: []\n",
      "Excluding for LH0_NO2: []\n",
      "Excluding for LH0_O3: []\n",
      "Excluding for LH0_PM10: []\n",
      "Excluding for LH0_PM2.5: []\n",
      "Excluding for ME2_NO2: []\n",
      "Excluding for ME2_PM2.5: []\n",
      "Excluding for MEA_NO2: []\n",
      "Excluding for MEA_PM2.5: []\n",
      "Excluding for MEB_PM10: []\n",
      "Excluding for MEB_PM2.5: []\n",
      "Excluding for MR8_PM25: []\n",
      "Excluding for RHI_NO2: []\n",
      "Excluding for RHI_PM10: []\n",
      "Excluding for RHI_PM2.5: []\n",
      "Excluding for RI2_PM2.5: []\n",
      "Excluding for TD5_PM10: []\n",
      "Excluding for TD5_PM2.5: []\n",
      "Excluding for TH2_PM2.5: []\n",
      "Excluding for TH5_NO2: []\n",
      "Excluding for TH5_PM10: []\n",
      "Excluding for TH5_PM2.5: []\n",
      "Excluding for TH6_NO2: []\n",
      "Excluding for TH6_O3: []\n",
      "Excluding for TH6_PM10: []\n",
      "Excluding for TH7_NO2: []\n",
      "Excluding for TH7_PM2.5: []\n",
      "Excluding for WA7_PM2.5: []\n",
      "Excluding for WAB_PM2.5: []\n",
      "Excluding for WM0_NO2: []\n",
      "Excluding for WM0_PM2.5: []\n",
      "Excluding for WME_NO2: []\n",
      "Excluding for WME_O3: []\n",
      "Excluding for WME_PM2.5: []\n",
      "Total CSV files in optimased: 0\n",
      "Total files after exclusion: 0\n",
      "Files with >20% missing after exclusion: 4136\n",
      "Sample exclude_filenames: []\n",
      "Sample log filenames: 0       bl0_co_2023-04-01_2023-04-30.csv\n",
      "1       bt4_o3_2023-04-01_2023-04-30.csv\n",
      "2      bt4_so2_2023-04-01_2023-04-30.csv\n",
      "3    bt5_pm2.5_2023-04-01_2023-04-30.csv\n",
      "4    bt6_pm2.5_2023-04-01_2023-04-30.csv\n",
      "Name: filename, dtype: object\n",
      "Sample all_csv_filenames: []\n",
      "Adjusted issue rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "notactive_files = [output_notActive_siteSpecies_2023, output_notActive_siteSpecies_2024]\n",
    "adjusted_issue_rate = calculate_adjusted_issue_rate(\n",
    "    optimased_root, log_file, notactive_files, all_csv_files\n",
    ")\n",
    "print(\"Adjusted issue rate: {:.2f}%\".format(adjusted_issue_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
