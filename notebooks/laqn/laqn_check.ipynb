{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637f6984",
   "metadata": {},
   "source": [
    "# LAQN dataset Find Missing Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740ff41",
   "metadata": {},
   "source": [
    "- I will identify the missing values and data gaps in the LAQN dataset and decide how to address them.\n",
    "- I’ll start by importing the relevant modules and displaying the initial file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48a2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "base_dir = Path.cwd().resolve()\n",
    "std_dir = base_dir / \"data\" / \"laqn\" / \"std\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc30d1d",
   "metadata": {},
   "source": [
    "## 1) Functions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5c711",
   "metadata": {},
   "source": [
    "### 1.1)The functions for discover and checks data quality metrics before cleaning, below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aba064",
   "metadata": {},
   "source": [
    "#### 1) Data quality function, what it does:\n",
    "- Counts total rows in dataset\n",
    "- Identifies missing values per column (count + percentage)\n",
    "- Counts duplicate rows based on timestamp\n",
    "- Detects negative values in measurements\n",
    "- Checks timestamp format issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea4bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality(self, df: pd.DataFrame, filename: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Checking data quality metrics before start cleaning.\n",
    "        \n",
    "        Returns dict with:\n",
    "        - total_rows\n",
    "        - missing_values\n",
    "        - duplicate_count\n",
    "        - negative_values\n",
    "        - timestamp_format\n",
    "        \"\"\"\n",
    "        assessment = {\n",
    "            'filename': filename,\n",
    "            'total_rows': len(df),\n",
    "            'missing_values': {},\n",
    "            'duplicate_count': 0,\n",
    "            'negative_values': 0,\n",
    "            'timestamp_issues': False\n",
    "        }\n",
    "        \n",
    "        # missing values\n",
    "        for col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                assessment['missing_values'][col] = {\n",
    "                    'count': int(missing),\n",
    "                    'percentage': round(missing / len(df) * 100, 2)\n",
    "                }\n",
    "        \n",
    "        # duplicates\n",
    "        if '@MeasurementDateGMT' in df.columns:\n",
    "            assessment['duplicate_count'] = df.duplicated(\n",
    "                subset=['@MeasurementDateGMT']\n",
    "            ).sum()\n",
    "        \n",
    "        # negative values\n",
    "        if '@Value' in df.columns:\n",
    "            assessment['negative_values'] = (df['@Value'] < 0).sum()\n",
    "        \n",
    "        # timestamp format\n",
    "        if '@MeasurementDateGMT' in df.columns:\n",
    "            assessment['timestamp_issues'] = df['@MeasurementDateGMT'].dtype == 'object'\n",
    "        \n",
    "        return assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80082906",
   "metadata": {},
   "source": [
    "#### 2) Below the function for missing data gasps:\n",
    "- Checks which site/species  are missing in Monthly/year folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0925ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_missing_data(base_dir: Path = base_dir):\n",
    "    monthly_data_dir = base_dir / \"data\" / \"laqn\" / \"monthly_data\"\n",
    "    year_2023_dir = base_dir / \"data\" / \"laqn\" / \"year_2023\"\n",
    "    metadata_file = base_dir / \"data\" / \"laqn\" / \"actv_sites_species.csv\"\n",
    "\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    expected = {(r[\"SiteCode\"], r[\"SpeciesCode\"]) for _, r in metadata.iterrows()}\n",
    "\n",
    "    out = {\n",
    "        \"monthly_data\": {\"by_location\": {}, \"by_pollutant\": {}, \"by_month\": {}, \"missing_combinations\": []},\n",
    "        \"year_2023\": {\"by_location\": {}, \"by_pollutant\": {}, \"missing_combinations\": []},\n",
    "        \"summary\": {\"expected_combinations\": len(expected), \"monthly_data_found\": 0, \"year_2023_found\": 0, \"total_missing\": 0},\n",
    "    }\n",
    "\n",
    "    if monthly_data_dir.exists():\n",
    "        found_m = set()\n",
    "        for fp in monthly_data_dir.rglob(\"*.csv\"):\n",
    "            parts = fp.name.split(\"_\")\n",
    "            if len(parts) >= 2:\n",
    "                site, pol = parts[0], parts[1]\n",
    "                month = fp.parent.name\n",
    "                found_m.add((site, pol))\n",
    "                out[\"monthly_data\"][\"by_location\"].setdefault(site, {}).setdefault(pol, []).append(month)\n",
    "                out[\"monthly_data\"][\"by_pollutant\"].setdefault(pol, {}).setdefault(site, []).append(month)\n",
    "                out[\"monthly_data\"][\"by_month\"].setdefault(month, []).append((site, pol))\n",
    "        missing_m = expected - found_m\n",
    "        for s, p in sorted(missing_m):\n",
    "            out[\"monthly_data\"][\"missing_combinations\"].append({\"site\": s, \"pollutant\": p, \"data_source\": \"monthly_data\"})\n",
    "        out[\"summary\"][\"monthly_data_found\"] = len(found_m)\n",
    "\n",
    "    if year_2023_dir.exists():\n",
    "        found_y = set()\n",
    "        for fp in year_2023_dir.glob(\"*.csv\"):\n",
    "            parts = fp.name.split(\"_\")\n",
    "            if len(parts) >= 2:\n",
    "                site, pol = parts[0], parts[1]\n",
    "                found_y.add((site, pol))\n",
    "                out[\"year_2023\"][\"by_location\"].setdefault(site, []).append(pol)\n",
    "                out[\"year_2023\"][\"by_pollutant\"].setdefault(pol, []).append(site)\n",
    "        missing_y = expected - found_y\n",
    "        for s, p in sorted(missing_y):\n",
    "            out[\"year_2023\"][\"missing_combinations\"].append({\"site\": s, \"pollutant\": p, \"data_source\": \"year_2023\"})\n",
    "        out[\"summary\"][\"year_2023_found\"] = len(found_y)\n",
    "\n",
    "    out[\"summary\"][\"total_missing\"] = (\n",
    "        len(out[\"monthly_data\"][\"missing_combinations\"]) + len(out[\"year_2023\"][\"missing_combinations\"])\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62e4bc",
   "metadata": {},
   "source": [
    "#### 3) Get missing month locations function below:\n",
    "- for each month in monthly_data/ lists available/missing pollutants per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc94e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_month_location(base_dir: Path = base_dir):\n",
    "    monthly_data_dir = base_dir / \"data\" / \"laqn\" / \"monthly_data\"\n",
    "    metadata_file = base_dir / \"data\" / \"laqn\" / \"actv_sites_species.csv\"\n",
    "\n",
    "    md = pd.read_csv(metadata_file)\n",
    "    all_pol = set(md[\"SpeciesCode\"].unique())\n",
    "    all_sites = set(md[\"SiteCode\"].unique())\n",
    "    result = {}\n",
    "\n",
    "    if monthly_data_dir.exists():\n",
    "        for month_dir in sorted([d for d in monthly_data_dir.iterdir() if d.is_dir()]):\n",
    "            month = month_dir.name\n",
    "            result[month] = {}\n",
    "            found = {}\n",
    "            for fp in month_dir.glob(\"*.csv\"):\n",
    "                parts = fp.name.split(\"_\")\n",
    "                if len(parts) >= 2:\n",
    "                    site, pol = parts[0], parts[1]\n",
    "                    found.setdefault(site, set()).add(pol)\n",
    "            for site in sorted(all_sites):\n",
    "                available = found.get(site, set())\n",
    "                missing = all_pol - available\n",
    "                if available or missing:\n",
    "                    result[month][site] = {\n",
    "                        \"available_pollutants\": sorted(available),\n",
    "                        \"missing_pollutants\": sorted(missing),\n",
    "                        \"count_available\": len(available),\n",
    "                        \"count_missing\": len(missing),\n",
    "                    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7464aa",
   "metadata": {},
   "source": [
    "#### 4) Analayse standartised files missing values.\n",
    " - finds atd files that have categoritical columns as the way I standartise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_with_missing(std_dir: Path = std_dir):\n",
    "    if not std_dir.exists():\n",
    "        logger.error(f\"Directory not found: {std_dir}\")\n",
    "        return {}\n",
    "\n",
    "    cat_cols = [\"SiteName\", \"SiteType\", \"SpeciesName\"]\n",
    "    year_dirs = sorted([d for d in std_dir.iterdir() if d.is_dir() and d.name.isdigit()])\n",
    "\n",
    "    results = {\n",
    "        \"files_with_categorical_and_missing\": [],\n",
    "        \"summary\": {\"total_files_checked\": 0, \"files_with_categorical\": 0, \"files_with_both\": 0},\n",
    "    }\n",
    "\n",
    "    for ydir in year_dirs:\n",
    "        year = ydir.name\n",
    "        for fp in sorted(ydir.glob(\"*.csv\")):\n",
    "            results[\"summary\"][\"total_files_checked\"] += 1\n",
    "            try:\n",
    "                df = pd.read_csv(fp)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                has_cat = any(c in df.columns for c in cat_cols)\n",
    "                if has_cat:\n",
    "                    results[\"summary\"][\"files_with_categorical\"] += 1\n",
    "                    if df.isnull().any().any():\n",
    "                        info = {\n",
    "                            \"year\": year,\n",
    "                            \"file\": fp.name,\n",
    "                            \"path\": str(fp),\n",
    "                            \"total_rows\": len(df),\n",
    "                            \"categorical_columns\": [c for c in cat_cols if c in df.columns],\n",
    "                            \"missing_details\": {},\n",
    "                        }\n",
    "                        for col in df.columns:\n",
    "                            miss = df[col].isna().sum()\n",
    "                            if miss > 0:\n",
    "                                pct = (miss / len(df) * 100) if len(df) else 0\n",
    "                                info[\"missing_details\"][col] = {\"missing_count\": int(miss), \"missing_pct\": round(pct, 2)}\n",
    "                        results[\"files_with_categorical_and_missing\"].append(info)\n",
    "                        results[\"summary\"][\"files_with_both\"] += 1\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error analyzing {fp}: {e}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473591e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 5) Function that checks what is categorically missing and creates a log entry to track the pattern.\n",
    "- This extension is useful for understanding why data is missing whether due to system overload, a fetching error/bug, incorrect URL or endpoint requests, or non-responsive endpoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c59398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_missing_log(base_dir: Path = base_dir):\n",
    "    output_dir = base_dir / \"data\" / \"laqn\" / \"std\" / \"missing\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    analysis = analyze_categorical_with_missing(base_dir / \"data\" / \"laqn\" / \"std\")\n",
    "    all_files = analysis[\"files_with_categorical_and_missing\"]\n",
    "    if not all_files:\n",
    "        logger.info(\"No files found with both categorical columns and missing values\")\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for f in all_files:\n",
    "        parts = f[\"file\"].replace(\".csv\", \"\").split(\"_\")\n",
    "        site = parts[0] if len(parts) > 0 else \"\"\n",
    "        species = parts[1] if len(parts) > 1 else \"\"\n",
    "        month = parts[2].split(\"-\")[1] if len(parts) >= 4 else \"\"\n",
    "        val_missing = f[\"missing_details\"].get(\"value\", {})\n",
    "        miss_count = val_missing.get(\"missing_count\", 0)\n",
    "        miss_pct = val_missing.get(\"missing_pct\", 0)\n",
    "        rows.append({\n",
    "            \"File\": f[\"file\"],\n",
    "            \"year\": f[\"year\"],\n",
    "            \"month\": month,\n",
    "            \"siteCode\": site,\n",
    "            \"SpeciesCode\": species,\n",
    "            \"path\": f[\"path\"],\n",
    "            \"total_rows\": f[\"total_rows\"],\n",
    "            \"value\": f\"{miss_count}/{f['total_rows']} rows ({miss_pct}%)\",\n",
    "            \"categorical_columns\": \",\".join(f[\"categorical_columns\"]),\n",
    "        })\n",
    "\n",
    "    df_log = pd.DataFrame(rows)\n",
    "    out_file = output_dir / \"2missing_files_log.csv\"\n",
    "    df_log.to_csv(out_file, index=False)\n",
    "    return str(out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff04e3",
   "metadata": {},
   "source": [
    "### 2) Comprehensive test to find ALL missing and problematic data in standardized files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8fe9d",
   "metadata": {},
   "source": [
    "What the testing function does below:\n",
    "- Scans all year directories in std\n",
    "- Checks each CSV file for 8 types of issues:\n",
    "    - Empty files: No rows\n",
    "    - Column errors: Missing required columns (timestamp, SiteCode, SpeciesCode, value)\n",
    "    - Duplicate timestamps: Multiple measurements at same time\n",
    "    - Missing SiteCode: Null values in SiteCode column\n",
    "    - Missing SpeciesCode: Null values in SpeciesCode column\n",
    "    - High missing values: >20% of value column is null\n",
    "    - Format errors: Cannot read file\n",
    "    - Provides detailed reporting with:\n",
    "        - Total files processed\n",
    "        - Issue statistics\n",
    "        - Examples of each problem type\n",
    "        - Severity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0601c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comprehensive_missing_data_analysis(self):\n",
    "        \"\"\"Comprehensive test to find all missing and problematic data in standardized files\"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"Missing Data Analysis for LAQN Data Files\")\n",
    "        print(\"Checking all files in: data/laqn/std/\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Get all year directories\n",
    "        year_dirs = sorted([d for d in self.std_dir.iterdir() if d.is_dir() and d.name.isdigit()])\n",
    "        \n",
    "        print(f\"\\nFound {len(year_dirs)} year directories: {[d.name for d in year_dirs]}\")\n",
    "        \n",
    "        # Initialize tracking\n",
    "        all_issues = {\n",
    "            'missing_files': [],\n",
    "            'duplicate_timestamps': [],\n",
    "            'missing_sitecode': [],\n",
    "            'missing_speciescode': [],\n",
    "            'high_missing_values': [],\n",
    "            'empty_files': [],\n",
    "            'column_errors': [],\n",
    "            'format_errors': []\n",
    "        }\n",
    "        \n",
    "        total_files = 0\n",
    "        processed_files = 0\n",
    "        files_with_issues = 0\n",
    "        \n",
    "        for year_dir in year_dirs:\n",
    "            year = year_dir.name\n",
    "            print(f\"\\n\" + \"-\"*100)\n",
    "            print(f\"YEAR: {year}\")\n",
    "            print(\"-\"*100)\n",
    "            \n",
    "            csv_files = sorted(list(year_dir.glob('*.csv')))\n",
    "            print(f\"Total files in {year}: {len(csv_files)}\")\n",
    "            \n",
    "            year_issues = 0\n",
    "            year_high_missing = 0\n",
    "            year_duplicates = 0\n",
    "            year_empty = 0\n",
    "            \n",
    "            for filepath in csv_files:\n",
    "                total_files += 1\n",
    "                filename = filepath.name\n",
    "                \n",
    "                try:\n",
    "                    # Read file\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    \n",
    "                    # Check if file is empty\n",
    "                    if df.empty or len(df) == 0:\n",
    "                        all_issues['empty_files'].append({\n",
    "                            'year': year,\n",
    "                            'file': filename,\n",
    "                            'path': str(filepath)\n",
    "                        })\n",
    "                        year_empty += 1\n",
    "                        files_with_issues += 1\n",
    "                        continue\n",
    "                    \n",
    "                    processed_files += 1\n",
    "                    file_issues = []\n",
    "                    \n",
    "                    # Parse filename to extract expected site and species code\n",
    "                    parts = filename.replace('.csv', '').split('_')\n",
    "                    expected_site = parts[0] if len(parts) > 0 else None\n",
    "                    expected_species = parts[1] if len(parts) > 1 else None\n",
    "                    expected_date_range = f\"{parts[2]}_{parts[3]}\" if len(parts) > 3 else None\n",
    "                    \n",
    "                    # rule 1 Check for required columns\n",
    "                    required_columns = ['timestamp', 'SiteCode', 'SpeciesCode', 'value']\n",
    "                    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                    \n",
    "                    if missing_cols:\n",
    "                        all_issues['column_errors'].append({\n",
    "                            'year': year,\n",
    "                            'file': filename,\n",
    "                            'path': str(filepath),\n",
    "                            'missing_columns': missing_cols,\n",
    "                            'actual_columns': list(df.columns)\n",
    "                        })\n",
    "                        files_with_issues += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # rule 2 Check for duplicate timestamps\n",
    "                    if 'timestamp' in df.columns:\n",
    "                        duplicate_timestamps = df['timestamp'].duplicated().sum()\n",
    "                        if duplicate_timestamps > 0:\n",
    "                            all_issues['duplicate_timestamps'].append({\n",
    "                                'year': year,\n",
    "                                'file': filename,\n",
    "                                'path': str(filepath),\n",
    "                                'duplicate_count': int(duplicate_timestamps),\n",
    "                                'total_rows': len(df),\n",
    "                                'duplicate_pct': round(duplicate_timestamps / len(df) * 100, 2)\n",
    "                            })\n",
    "                            year_duplicates += 1\n",
    "                            file_issues.append(f\"duplicate_timestamps({duplicate_timestamps})\")\n",
    "                            files_with_issues += 1\n",
    "                    \n",
    "                    # rule 3 Check SiteCode\n",
    "                    if 'SiteCode' in df.columns:\n",
    "                        # Check if all rows have SiteCode\n",
    "                        missing_sitecode = df['SiteCode'].isna().sum()\n",
    "                        if missing_sitecode > 0:\n",
    "                            all_issues['missing_sitecode'].append({\n",
    "                                'year': year,\n",
    "                                'file': filename,\n",
    "                                'path': str(filepath),\n",
    "                                'missing_count': int(missing_sitecode),\n",
    "                                'total_rows': len(df),\n",
    "                                'missing_pct': round(missing_sitecode / len(df) * 100, 2)\n",
    "                            })\n",
    "                            file_issues.append(f\"missing_sitecode({missing_sitecode})\")\n",
    "                            files_with_issues += 1\n",
    "                        \n",
    "                        # Check if SiteCode matches filename\n",
    "                        unique_sites = df['SiteCode'].unique()\n",
    "                        if len(unique_sites) > 1:\n",
    "                            file_issues.append(f\"multiple_sitecodes({len(unique_sites)})\")\n",
    "                        elif len(unique_sites) == 1 and unique_sites[0] != expected_site:\n",
    "                            file_issues.append(f\"sitecode_mismatch(expected:{expected_site},got:{unique_sites[0]})\")\n",
    "                    \n",
    "                    # RULE 4: Check SpeciesCode\n",
    "                    if 'SpeciesCode' in df.columns:\n",
    "                        # Check if all rows have SpeciesCode\n",
    "                        missing_speciescode = df['SpeciesCode'].isna().sum()\n",
    "                        if missing_speciescode > 0:\n",
    "                            all_issues['missing_speciescode'].append({\n",
    "                                'year': year,\n",
    "                                'file': filename,\n",
    "                                'path': str(filepath),\n",
    "                                'missing_count': int(missing_speciescode),\n",
    "                                'total_rows': len(df),\n",
    "                                'missing_pct': round(missing_speciescode / len(df) * 100, 2)\n",
    "                            })\n",
    "                            file_issues.append(f\"missing_speciescode({missing_speciescode})\")\n",
    "                            files_with_issues += 1\n",
    "                        \n",
    "                        # Check if SpeciesCode matches filename\n",
    "                        unique_species = df['SpeciesCode'].unique()\n",
    "                        if len(unique_species) > 1:\n",
    "                            file_issues.append(f\"multiple_speciescodes({len(unique_species)})\")\n",
    "                        elif len(unique_species) == 1 and unique_species[0] != expected_species:\n",
    "                            file_issues.append(f\"speciescode_mismatch(expected:{expected_species},got:{unique_species[0]})\")\n",
    "                    \n",
    "                    # RULE 5: Check value column (>20% missing is a problem)\n",
    "                    if 'value' in df.columns:\n",
    "                        missing_values = df['value'].isna().sum()\n",
    "                        missing_pct = missing_values / len(df) * 100\n",
    "                        \n",
    "                        if missing_pct > 20:\n",
    "                            all_issues['high_missing_values'].append({\n",
    "                                'year': year,\n",
    "                                'file': filename,\n",
    "                                'path': str(filepath),\n",
    "                                'missing_count': int(missing_values),\n",
    "                                'total_rows': len(df),\n",
    "                                'missing_pct': round(missing_pct, 2)\n",
    "                            })\n",
    "                            year_high_missing += 1\n",
    "                            file_issues.append(f\"high_missing_values({round(missing_pct, 1)}%)\")\n",
    "                            files_with_issues += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    all_issues['format_errors'].append({\n",
    "                        'year': year,\n",
    "                        'file': filename,\n",
    "                        'path': str(filepath),\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                    files_with_issues += 1\n",
    "            \n",
    "            # Year summary\n",
    "            print(f\"\\nYear {year} Summary:\")\n",
    "            print(f\"  Total files: {len(csv_files)}\")\n",
    "            print(f\"  Files with issues: {year_issues + year_duplicates + year_high_missing + year_empty}\")\n",
    "            print(f\"    - Empty files: {year_empty}\")\n",
    "            print(f\"    - Duplicate timestamps: {year_duplicates}\")\n",
    "            print(f\"    - High missing values (>20%): {year_high_missing}\")\n",
    "        \n",
    "        # PRINT DETAILED REPORT\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"Missing Data Analysis Report\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        print(f\"\\nTotal files processed: {total_files}\")\n",
    "        print(f\"Files with issues: {files_with_issues}\")\n",
    "        if total_files > 0:\n",
    "            print(f\"Issue rate: {(files_with_issues/total_files*100):.1f}%\")\n",
    "        \n",
    "        # 1. EMPTY FILES\n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"EMPTY FILES\")\n",
    "        print(\"-\"*100)\n",
    "        if all_issues['empty_files']:\n",
    "            print(f\"Total: {len(all_issues['empty_files'])}\\n\")\n",
    "            for item in all_issues['empty_files'][:20]:\n",
    "                print(f\"  {item['year']}/{item['file']}\")\n",
    "            if len(all_issues['empty_files']) > 20:\n",
    "                print(f\"  ... and {len(all_issues['empty_files']) - 20} more\")\n",
    "        else:\n",
    "            print(\"No empty files found\")\n",
    "        \n",
    "        # 2. COLUMN ERRORS\n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"Column errors for missing required columns\")\n",
    "        print(\"-\"*100)\n",
    "        if all_issues['column_errors']:\n",
    "            print(f\"Total: {len(all_issues['column_errors'])}\\n\")\n",
    "            for item in all_issues['column_errors'][:20]:\n",
    "                print(f\"  {item['year']}/{item['file']}\")\n",
    "                print(f\"    Missing: {item['missing_columns']}\")\n",
    "                print(f\"    Has: {item['actual_columns']}\")\n",
    "            if len(all_issues['column_errors']) > 20:\n",
    "                print(f\"  ... and {len(all_issues['column_errors']) - 20} more\")\n",
    "        else:\n",
    "            print(\" All files have required columns\")\n",
    "        \n",
    "        # 3. duplicate timestamp\n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"duplicate timestamp\")\n",
    "        print(\"-\"*100)\n",
    "        if all_issues['duplicate_timestamps']:\n",
    "            print(f\"Total files with duplicates: {len(all_issues['duplicate_timestamps'])}\\n\")\n",
    "            for item in sorted(all_issues['duplicate_timestamps'], key=lambda x: x['duplicate_pct'], reverse=True)[:20]:\n",
    "                print(f\"  {item['year']}/{item['file']}\")\n",
    "                print(f\"    Duplicates: {item['duplicate_count']}/{item['total_rows']} ({item['duplicate_pct']}%)\")\n",
    "            if len(all_issues['duplicate_timestamps']) > 20:\n",
    "                print(f\"  ... and {len(all_issues['duplicate_timestamps']) - 20} more\")\n",
    "        else:\n",
    "            print(\"  No duplicate timestamps found\")\n",
    "        \n",
    "        # 4.mmissing sitecode\n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"missing sitecode\")\n",
    "        print(\"-\"*100)\n",
    "        if all_issues['missing_sitecode']:\n",
    "            print(f\"Total files affected: {len(all_issues['missing_sitecode'])}\\n\")\n",
    "            for item in sorted(all_issues['missing_sitecode'], key=lambda x: x['missing_pct'], reverse=True)[:20]:\n",
    "                print(f\"  {item['year']}/{item['file']}\")\n",
    "                print(f\"    Missing: {item['missing_count']}/{item['total_rows']} ({item['missing_pct']}%)\")\n",
    "            if len(all_issues['missing_sitecode']) > 20:\n",
    "                print(f\"  ... and {len(all_issues['missing_sitecode']) - 20} more\")\n",
    "        else:\n",
    "            print(\"  No missing SiteCode values found\")\n",
    "        \n",
    "        # 5. Mmissing speciescode\n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"Missing speciescode\")\n",
    "        print(\"-\"*100)\n",
    "        if all_issues['missing_speciescode']:\n",
    "            print(f\"Total files affected: {len(all_issues['missing_speciescode'])}\\n\")\n",
    "            for item in sorted(all_issues['missing_speciescode'], key=lambda x: x['missing_pct'], reverse=True)[:20]:\n",
    "                print(f\"  {item['year']}/{item['file']}\")\n",
    "                print(f\"    Missing: {item['missing_count']}/{item['total_rows']} ({item['missing_pct']}%)\")\n",
    "            if len(all_issues['missing_speciescode']) > 20:\n",
    "                print(f\"  ... and {len(all_issues['missing_speciescode']) - 20} more\")\n",
    "        else:\n",
    "            print(\" No missing SpeciesCode values found\")\n",
    "        \n",
    "        # 6. HIGH MISSING VALUES (>20%)\n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"Missing values (>20%)\")\n",
    "        print(\"-\"*100)\n",
    "        if all_issues['high_missing_values']:\n",
    "            print(f\"Total files with >20% missing: {len(all_issues['high_missing_values'])}\\n\")\n",
    "            for item in sorted(all_issues['high_missing_values'], key=lambda x: x['missing_pct'], reverse=True)[:20]:\n",
    "                print(f\"  {item['year']}/{item['file']}\")\n",
    "                print(f\"    Missing: {item['missing_count']}/{item['total_rows']} ({item['missing_pct']}%)\")\n",
    "            if len(all_issues['high_missing_values']) > 20:\n",
    "                print(f\"  ... and {len(all_issues['high_missing_values']) - 20} more\")\n",
    "        else:\n",
    "            print(\"No files with >20% missing values\")\n",
    "        \n",
    "        # 7. FORMAT ERRORS\n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"format errors (Cannot read file)\")\n",
    "        print(\"-\"*100)\n",
    "        if all_issues['format_errors']:\n",
    "            print(f\"Total files with errors: {len(all_issues['format_errors'])}\\n\")\n",
    "            for item in all_issues['format_errors'][:10]:\n",
    "                print(f\"  {item['year']}/{item['file']}\")\n",
    "                print(f\"    Error: {item['error'][:80]}\")\n",
    "            if len(all_issues['format_errors']) > 10:\n",
    "                print(f\"  ... and {len(all_issues['format_errors']) - 10} more\")\n",
    "        else:\n",
    "            print(\" All files can be read successfully\")\n",
    "        \n",
    "        # FINAL SUMMARY\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"FINAL SUMMARY\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        total_issues = (\n",
    "            len(all_issues['empty_files']) +\n",
    "            len(all_issues['column_errors']) +\n",
    "            len(all_issues['duplicate_timestamps']) +\n",
    "            len(all_issues['missing_sitecode']) +\n",
    "            len(all_issues['missing_speciescode']) +\n",
    "            len(all_issues['high_missing_values']) +\n",
    "            len(all_issues['format_errors'])\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTotal issues found: {total_issues}\")\n",
    "        print(f\"  Empty files: {len(all_issues['empty_files'])}\")\n",
    "        print(f\"  Column errors: {len(all_issues['column_errors'])}\")\n",
    "        print(f\"  Duplicate timestamps: {len(all_issues['duplicate_timestamps'])}\")\n",
    "        print(f\"  Missing SiteCode: {len(all_issues['missing_sitecode'])}\")\n",
    "        print(f\"  Missing SpeciesCode: {len(all_issues['missing_speciescode'])}\")\n",
    "        print(f\"  High missing values (>20%): {len(all_issues['high_missing_values'])}\")\n",
    "        print(f\"  Format errors: {len(all_issues['format_errors'])}\")\n",
    "        \n",
    "        if total_issues == 0:\n",
    "            print(\"\\n All files passed quality checks! \")\n",
    "        else:\n",
    "            print(f\"\\n⚠ {total_issues} issues need attention\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a93489",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
