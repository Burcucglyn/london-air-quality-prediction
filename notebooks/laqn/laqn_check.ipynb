{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637f6984",
   "metadata": {},
   "source": [
    "# LAQN dataset Find Missing Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740ff41",
   "metadata": {},
   "source": [
    "- I will identify the missing values and data gaps in the LAQN dataset and decide how to address them.\n",
    "- I’ll start by importing the relevant modules and displaying the initial file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48a2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "base_dir = Path.cwd().resolve()\n",
    "std_dir = base_dir / \"data\" / \"laqn\" / \"std\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc30d1d",
   "metadata": {},
   "source": [
    "## 1) Functions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5c711",
   "metadata": {},
   "source": [
    "### 1.1)The functions for discover and checks data quality metrics before cleaning, below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aba064",
   "metadata": {},
   "source": [
    "#### 1) Data quality function, what it does:\n",
    "- Counts total rows in dataset\n",
    "- Identifies missing values per column (count + percentage)\n",
    "- Counts duplicate rows based on timestamp\n",
    "- Detects negative values in measurements\n",
    "- Checks timestamp format issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea4bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality(self, df: pd.DataFrame, filename: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Checking data quality metrics before start cleaning.\n",
    "        \n",
    "        Returns dict with:\n",
    "        - total_rows\n",
    "        - missing_values\n",
    "        - duplicate_count\n",
    "        - negative_values\n",
    "        - timestamp_format\n",
    "        \"\"\"\n",
    "        assessment = {\n",
    "            'filename': filename,\n",
    "            'total_rows': len(df),\n",
    "            'missing_values': {},\n",
    "            'duplicate_count': 0,\n",
    "            'negative_values': 0,\n",
    "            'timestamp_issues': False\n",
    "        }\n",
    "        \n",
    "        # missing values\n",
    "        for col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                assessment['missing_values'][col] = {\n",
    "                    'count': int(missing),\n",
    "                    'percentage': round(missing / len(df) * 100, 2)\n",
    "                }\n",
    "        \n",
    "        # duplicates\n",
    "        if '@MeasurementDateGMT' in df.columns:\n",
    "            assessment['duplicate_count'] = df.duplicated(\n",
    "                subset=['@MeasurementDateGMT']\n",
    "            ).sum()\n",
    "        \n",
    "        # negative values\n",
    "        if '@Value' in df.columns:\n",
    "            assessment['negative_values'] = (df['@Value'] < 0).sum()\n",
    "        \n",
    "        # timestamp format\n",
    "        if '@MeasurementDateGMT' in df.columns:\n",
    "            assessment['timestamp_issues'] = df['@MeasurementDateGMT'].dtype == 'object'\n",
    "        \n",
    "        return assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80082906",
   "metadata": {},
   "source": [
    "#### 2) Below the function for missing data gasps:\n",
    "- Checks which site/species  are missing in Monthly/year folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0925ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_missing_data(base_dir: Path = base_dir):\n",
    "    monthly_data_dir = base_dir / \"data\" / \"laqn\" / \"monthly_data\"\n",
    "    year_2023_dir = base_dir / \"data\" / \"laqn\" / \"year_2023\"\n",
    "    metadata_file = base_dir / \"data\" / \"laqn\" / \"actv_sites_species.csv\"\n",
    "\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    expected = {(r[\"SiteCode\"], r[\"SpeciesCode\"]) for _, r in metadata.iterrows()}\n",
    "\n",
    "    out = {\n",
    "        \"monthly_data\": {\"by_location\": {}, \"by_pollutant\": {}, \"by_month\": {}, \"missing_combinations\": []},\n",
    "        \"year_2023\": {\"by_location\": {}, \"by_pollutant\": {}, \"missing_combinations\": []},\n",
    "        \"summary\": {\"expected_combinations\": len(expected), \"monthly_data_found\": 0, \"year_2023_found\": 0, \"total_missing\": 0},\n",
    "    }\n",
    "\n",
    "    if monthly_data_dir.exists():\n",
    "        found_m = set()\n",
    "        for fp in monthly_data_dir.rglob(\"*.csv\"):\n",
    "            parts = fp.name.split(\"_\")\n",
    "            if len(parts) >= 2:\n",
    "                site, pol = parts[0], parts[1]\n",
    "                month = fp.parent.name\n",
    "                found_m.add((site, pol))\n",
    "                out[\"monthly_data\"][\"by_location\"].setdefault(site, {}).setdefault(pol, []).append(month)\n",
    "                out[\"monthly_data\"][\"by_pollutant\"].setdefault(pol, {}).setdefault(site, []).append(month)\n",
    "                out[\"monthly_data\"][\"by_month\"].setdefault(month, []).append((site, pol))\n",
    "        missing_m = expected - found_m\n",
    "        for s, p in sorted(missing_m):\n",
    "            out[\"monthly_data\"][\"missing_combinations\"].append({\"site\": s, \"pollutant\": p, \"data_source\": \"monthly_data\"})\n",
    "        out[\"summary\"][\"monthly_data_found\"] = len(found_m)\n",
    "\n",
    "    if year_2023_dir.exists():\n",
    "        found_y = set()\n",
    "        for fp in year_2023_dir.glob(\"*.csv\"):\n",
    "            parts = fp.name.split(\"_\")\n",
    "            if len(parts) >= 2:\n",
    "                site, pol = parts[0], parts[1]\n",
    "                found_y.add((site, pol))\n",
    "                out[\"year_2023\"][\"by_location\"].setdefault(site, []).append(pol)\n",
    "                out[\"year_2023\"][\"by_pollutant\"].setdefault(pol, []).append(site)\n",
    "        missing_y = expected - found_y\n",
    "        for s, p in sorted(missing_y):\n",
    "            out[\"year_2023\"][\"missing_combinations\"].append({\"site\": s, \"pollutant\": p, \"data_source\": \"year_2023\"})\n",
    "        out[\"summary\"][\"year_2023_found\"] = len(found_y)\n",
    "\n",
    "    out[\"summary\"][\"total_missing\"] = (\n",
    "        len(out[\"monthly_data\"][\"missing_combinations\"]) + len(out[\"year_2023\"][\"missing_combinations\"])\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62e4bc",
   "metadata": {},
   "source": [
    "#### 3) Get missing month locations function below:\n",
    "- for each month in monthly_data/ lists available/missing pollutants per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc94e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_month_location(base_dir: Path = base_dir):\n",
    "    monthly_data_dir = base_dir / \"data\" / \"laqn\" / \"monthly_data\"\n",
    "    metadata_file = base_dir / \"data\" / \"laqn\" / \"actv_sites_species.csv\"\n",
    "\n",
    "    md = pd.read_csv(metadata_file)\n",
    "    all_pol = set(md[\"SpeciesCode\"].unique())\n",
    "    all_sites = set(md[\"SiteCode\"].unique())\n",
    "    result = {}\n",
    "\n",
    "    if monthly_data_dir.exists():\n",
    "        for month_dir in sorted([d for d in monthly_data_dir.iterdir() if d.is_dir()]):\n",
    "            month = month_dir.name\n",
    "            result[month] = {}\n",
    "            found = {}\n",
    "            for fp in month_dir.glob(\"*.csv\"):\n",
    "                parts = fp.name.split(\"_\")\n",
    "                if len(parts) >= 2:\n",
    "                    site, pol = parts[0], parts[1]\n",
    "                    found.setdefault(site, set()).add(pol)\n",
    "            for site in sorted(all_sites):\n",
    "                available = found.get(site, set())\n",
    "                missing = all_pol - available\n",
    "                if available or missing:\n",
    "                    result[month][site] = {\n",
    "                        \"available_pollutants\": sorted(available),\n",
    "                        \"missing_pollutants\": sorted(missing),\n",
    "                        \"count_available\": len(available),\n",
    "                        \"count_missing\": len(missing),\n",
    "                    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7464aa",
   "metadata": {},
   "source": [
    "#### 4) Analayse standartised files missing values.\n",
    " - finds atd files that have categoritical columns as the way I standartise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_with_missing(std_dir: Path = std_dir):\n",
    "    if not std_dir.exists():\n",
    "        logger.error(f\"Directory not found: {std_dir}\")\n",
    "        return {}\n",
    "\n",
    "    cat_cols = [\"SiteName\", \"SiteType\", \"SpeciesName\"]\n",
    "    year_dirs = sorted([d for d in std_dir.iterdir() if d.is_dir() and d.name.isdigit()])\n",
    "\n",
    "    results = {\n",
    "        \"files_with_categorical_and_missing\": [],\n",
    "        \"summary\": {\"total_files_checked\": 0, \"files_with_categorical\": 0, \"files_with_both\": 0},\n",
    "    }\n",
    "\n",
    "    for ydir in year_dirs:\n",
    "        year = ydir.name\n",
    "        for fp in sorted(ydir.glob(\"*.csv\")):\n",
    "            results[\"summary\"][\"total_files_checked\"] += 1\n",
    "            try:\n",
    "                df = pd.read_csv(fp)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                has_cat = any(c in df.columns for c in cat_cols)\n",
    "                if has_cat:\n",
    "                    results[\"summary\"][\"files_with_categorical\"] += 1\n",
    "                    if df.isnull().any().any():\n",
    "                        info = {\n",
    "                            \"year\": year,\n",
    "                            \"file\": fp.name,\n",
    "                            \"path\": str(fp),\n",
    "                            \"total_rows\": len(df),\n",
    "                            \"categorical_columns\": [c for c in cat_cols if c in df.columns],\n",
    "                            \"missing_details\": {},\n",
    "                        }\n",
    "                        for col in df.columns:\n",
    "                            miss = df[col].isna().sum()\n",
    "                            if miss > 0:\n",
    "                                pct = (miss / len(df) * 100) if len(df) else 0\n",
    "                                info[\"missing_details\"][col] = {\"missing_count\": int(miss), \"missing_pct\": round(pct, 2)}\n",
    "                        results[\"files_with_categorical_and_missing\"].append(info)\n",
    "                        results[\"summary\"][\"files_with_both\"] += 1\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error analyzing {fp}: {e}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473591e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 5) Function that checks what is categorically missing and creates a log entry to track the pattern.\n",
    "- This extension is useful for understanding why data is missing whether due to system overload, a fetching error/bug, incorrect URL or endpoint requests, or non-responsive endpoints.\n",
    "- While I was fetching the LAQN 2023 yearly data, I first fetched it quarterly using a parallel multiprocessing method. It took around 20–30 minutes to retrieve one year of data.\n",
    "- After that, I decided to use the same method but fetch the data monthly for 2024 up to 19.11.2025, so I changed the folder structure. I then re-fetched the 2023 datasets monthly to keep everything consistent. However, I did not remove the first trial, so the data overlapped. That’s why I needed two different log files to check and compare.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c59398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_missing_log(base_dir: Path = base_dir):\n",
    "    output_dir = base_dir / \"data\" / \"laqn\" / \"std\" / \"missing\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    analysis = analyze_categorical_with_missing(base_dir / \"data\" / \"laqn\" / \"std\")\n",
    "    all_files = analysis[\"files_with_categorical_and_missing\"]\n",
    "    if not all_files:\n",
    "        logger.info(\"No files found with both categorical columns and missing values\")\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for f in all_files:\n",
    "        parts = f[\"file\"].replace(\".csv\", \"\").split(\"_\")\n",
    "        site = parts[0] if len(parts) > 0 else \"\"\n",
    "        species = parts[1] if len(parts) > 1 else \"\"\n",
    "        month = parts[2].split(\"-\")[1] if len(parts) >= 4 else \"\"\n",
    "        val_missing = f[\"missing_details\"].get(\"value\", {})\n",
    "        miss_count = val_missing.get(\"missing_count\", 0)\n",
    "        miss_pct = val_missing.get(\"missing_pct\", 0)\n",
    "        rows.append({\n",
    "            \"File\": f[\"file\"],\n",
    "            \"year\": f[\"year\"],\n",
    "            \"month\": month,\n",
    "            \"siteCode\": site,\n",
    "            \"SpeciesCode\": species,\n",
    "            \"path\": f[\"path\"],\n",
    "            \"total_rows\": f[\"total_rows\"],\n",
    "            \"value\": f\"{miss_count}/{f['total_rows']} rows ({miss_pct}%)\",\n",
    "            \"categorical_columns\": \",\".join(f[\"categorical_columns\"]),\n",
    "        })\n",
    "\n",
    "    df_log = pd.DataFrame(rows)\n",
    "    out_file = output_dir / \"2missing_files_log.csv\"\n",
    "    df_log.to_csv(out_file, index=False)\n",
    "    return str(out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142f4d9",
   "metadata": {},
   "source": [
    "#### 6) Cross check the missing logs function:\n",
    " - Compares two log files log and log2 files for overlap/differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea792c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_check_logs(base_dir: Path = base_dir):\n",
    "    log1 = base_dir / \"data\" / \"laqn\" / \"std\" / \"missing\" / \"missing_files_log.csv\"\n",
    "    log2 = base_dir / \"data\" / \"laqn\" / \"std\" / \"missing\" / \"2missing_files_log.csv\"\n",
    "\n",
    "    if not log1.exists():\n",
    "        logger.error(f\"First log not found: {log1}\")\n",
    "        return {}\n",
    "    if not log2.exists():\n",
    "        logger.error(f\"Second log not found: {log2}\")\n",
    "        return {}\n",
    "\n",
    "    df1, df2 = pd.read_csv(log1), pd.read_csv(log2)\n",
    "    files1, files2 = set(df1[\"File\"].values), set(df2[\"File\"].values)\n",
    "\n",
    "    only1 = files1 - files2\n",
    "    only2 = files2 - files1\n",
    "    both = files1 & files2\n",
    "\n",
    "    return {\n",
    "        \"only_in_log1\": sorted(only1),\n",
    "        \"only_in_log2\": sorted(only2),\n",
    "        \"in_both\": sorted(both),\n",
    "        \"statistics\": {\n",
    "            \"log1_total\": len(files1),\n",
    "            \"log2_total\": len(files2),\n",
    "            \"only_in_log1_count\": len(only1),\n",
    "            \"only_in_log2_count\": len(only2),\n",
    "            \"in_both_count\": len(both),\n",
    "            \"overlap_percentage\": round((len(both) / len(files1) * 100) if files1 else 0, 2),\n",
    "        },\n",
    "        \"log1_details\": {\n",
    "            \"path\": str(log1),\n",
    "            \"total_files\": len(df1),\n",
    "            \"by_year\": df1[\"year\"].value_counts().to_dict(),\n",
    "            \"unique_sites\": df1[\"siteCode\"].nunique(),\n",
    "            \"unique_species\": df1[\"SpeciesCode\"].nunique(),\n",
    "        },\n",
    "        \"log2_details\": {\n",
    "            \"path\": str(log2),\n",
    "            \"total_files\": len(df2),\n",
    "            \"by_year\": df2[\"year\"].value_counts().to_dict() if len(df2) else {},\n",
    "            \"unique_sites\": df2[\"siteCode\"].nunique() if len(df2) else 0,\n",
    "            \"unique_species\": df2[\"SpeciesCode\"].nunique() if len(df2) else 0,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff04e3",
   "metadata": {},
   "source": [
    "### 2) Comprehensive test to find ALL missing and problematic data in standardized files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931295c",
   "metadata": {},
   "source": [
    "#### 1) testing functions for all the data checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8fe9d",
   "metadata": {},
   "source": [
    "What the testing function does below:\n",
    "- Scans all year directories in std\n",
    "- Checks each CSV file for 8 types of issues:\n",
    "    - Empty files: No rows\n",
    "    - Column errors: Missing required columns (timestamp, SiteCode, SpeciesCode, value)\n",
    "    - Duplicate timestamps: Multiple measurements at same time\n",
    "    - Missing SiteCode: Null values in SiteCode column\n",
    "    - Missing SpeciesCode: Null values in SpeciesCode column\n",
    "    - High missing values: >20% of value column is null\n",
    "    - Format errors: Cannot read file\n",
    "    - Provides detailed reporting with:\n",
    "        - Total files processed\n",
    "        - Issue statistics\n",
    "        - Examples of each problem type\n",
    "        - Severity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_missing_data_analysis(std_dir: Path = std_dir):\n",
    "    \"\"\"Comprehensive test to find all missing and problematic data in standardized files\"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Test result: comprehensive missing data analysis\")\n",
    "    print(\"Checking all files in: data/laqn/std/\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Get all year directories\n",
    "    year_dirs = sorted([d for d in std_dir.iterdir() if d.is_dir() and d.name.isdigit()])\n",
    "    \n",
    "    print(f\"\\nFound {len(year_dirs)} year directories: {[d.name for d in year_dirs]}\")\n",
    "    \n",
    "    # Initialize tracking\n",
    "    all_issues = {\n",
    "        'empty_files': [],\n",
    "        'duplicate_timestamps': [],\n",
    "        'missing_sitecode': [],\n",
    "        'missing_speciescode': [],\n",
    "        'high_missing_values': [],\n",
    "        'column_errors': [],\n",
    "        'format_errors': []\n",
    "    }\n",
    "    \n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    files_with_issues = 0\n",
    "    \n",
    "    for year_dir in year_dirs:\n",
    "        year = year_dir.name\n",
    "        print(f\"\\n\" + \"-\"*100)\n",
    "        print(f\"Year: {year}\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        csv_files = sorted(list(year_dir.glob('*.csv')))\n",
    "        print(f\"Total files in {year}: {len(csv_files)}\")\n",
    "        \n",
    "        year_high_missing = 0\n",
    "        year_duplicates = 0\n",
    "        year_empty = 0\n",
    "        \n",
    "        for filepath in csv_files:\n",
    "            total_files += 1\n",
    "            filename = filepath.name\n",
    "            \n",
    "            try:\n",
    "                # Read file\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Check if file is empty\n",
    "                if df.empty or len(df) == 0:\n",
    "                    all_issues['empty_files'].append({\n",
    "                        'year': year,\n",
    "                        'file': filename,\n",
    "                        'path': str(filepath)\n",
    "                    })\n",
    "                    year_empty += 1\n",
    "                    files_with_issues += 1\n",
    "                    continue\n",
    "                \n",
    "                processed_files += 1\n",
    "                \n",
    "                # Parse filename to extract expected site and species code\n",
    "                parts = filename.replace('.csv', '').split('_')\n",
    "                expected_site = parts[0] if len(parts) > 0 else None\n",
    "                expected_species = parts[1] if len(parts) > 1 else None\n",
    "                \n",
    "                # Rule 1: Check for required columns\n",
    "                required_columns = ['timestamp', 'SiteCode', 'SpeciesCode', 'value']\n",
    "                missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                \n",
    "                if missing_cols:\n",
    "                    all_issues['column_errors'].append({\n",
    "                        'year': year,\n",
    "                        'file': filename,\n",
    "                        'path': str(filepath),\n",
    "                        'missing_columns': missing_cols,\n",
    "                        'actual_columns': list(df.columns)\n",
    "                    })\n",
    "                    files_with_issues += 1\n",
    "                    continue\n",
    "                \n",
    "                # Rule 2: Check for duplicate timestamps\n",
    "                if 'timestamp' in df.columns:\n",
    "                    duplicate_timestamps = df['timestamp'].duplicated().sum()\n",
    "                    if duplicate_timestamps > 0:\n",
    "                        all_issues['duplicate_timestamps'].append({\n",
    "                            'year': year,\n",
    "                            'file': filename,\n",
    "                            'path': str(filepath),\n",
    "                            'duplicate_count': int(duplicate_timestamps),\n",
    "                            'total_rows': len(df),\n",
    "                            'duplicate_pct': round(duplicate_timestamps / len(df) * 100, 2)\n",
    "                        })\n",
    "                        year_duplicates += 1\n",
    "                        files_with_issues += 1\n",
    "                \n",
    "                # Rule 3: Check SiteCode\n",
    "                if 'SiteCode' in df.columns:\n",
    "                    missing_sitecode = df['SiteCode'].isna().sum()\n",
    "                    if missing_sitecode > 0:\n",
    "                        all_issues['missing_sitecode'].append({\n",
    "                            'year': year,\n",
    "                            'file': filename,\n",
    "                            'path': str(filepath),\n",
    "                            'missing_count': int(missing_sitecode),\n",
    "                            'total_rows': len(df),\n",
    "                            'missing_pct': round(missing_sitecode / len(df) * 100, 2)\n",
    "                        })\n",
    "                        files_with_issues += 1\n",
    "                \n",
    "                # Rule 4: Check SpeciesCode\n",
    "                if 'SpeciesCode' in df.columns:\n",
    "                    missing_speciescode = df['SpeciesCode'].isna().sum()\n",
    "                    if missing_speciescode > 0:\n",
    "                        all_issues['missing_speciescode'].append({\n",
    "                            'year': year,\n",
    "                            'file': filename,\n",
    "                            'path': str(filepath),\n",
    "                            'missing_count': int(missing_speciescode),\n",
    "                            'total_rows': len(df),\n",
    "                            'missing_pct': round(missing_speciescode / len(df) * 100, 2)\n",
    "                        })\n",
    "                        files_with_issues += 1\n",
    "                \n",
    "                # Rule 5: Check value column (>20% missing is a problem)\n",
    "                if 'value' in df.columns:\n",
    "                    missing_values = df['value'].isna().sum()\n",
    "                    missing_pct = missing_values / len(df) * 100\n",
    "                    \n",
    "                    if missing_pct > 20:\n",
    "                        all_issues['high_missing_values'].append({\n",
    "                            'year': year,\n",
    "                            'file': filename,\n",
    "                            'path': str(filepath),\n",
    "                            'missing_count': int(missing_values),\n",
    "                            'total_rows': len(df),\n",
    "                            'missing_pct': round(missing_pct, 2)\n",
    "                        })\n",
    "                        year_high_missing += 1\n",
    "                        files_with_issues += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                all_issues['format_errors'].append({\n",
    "                    'year': year,\n",
    "                    'file': filename,\n",
    "                    'path': str(filepath),\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                files_with_issues += 1\n",
    "        \n",
    "        # Year summary\n",
    "        print(f\"\\nYear {year} summary:\")\n",
    "        print(f\"  Total files: {len(csv_files)}\")\n",
    "        print(f\"  Files with issues: {year_empty + year_duplicates + year_high_missing}\")\n",
    "        print(f\"    - Empty files: {year_empty}\")\n",
    "        print(f\"    - Duplicate timestamps: {year_duplicates}\")\n",
    "        print(f\"    - High missing values (>20%): {year_high_missing}\")\n",
    "    \n",
    "    # Print detailed report\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Test result: missing data analysis report\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(f\"\\nTotal files processed: {total_files}\")\n",
    "    print(f\"Files with issues: {files_with_issues}\")\n",
    "    if total_files > 0:\n",
    "        print(f\"Issue rate: {(files_with_issues/total_files*100):.1f}%\")\n",
    "    \n",
    "    # Empty files\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"Empty files\")\n",
    "    print(\"-\"*100)\n",
    "    if all_issues['empty_files']:\n",
    "        print(f\"Total: {len(all_issues['empty_files'])}\\n\")\n",
    "        for item in all_issues['empty_files'][:20]:\n",
    "            print(f\"  {item['year']}/{item['file']}\")\n",
    "        if len(all_issues['empty_files']) > 20:\n",
    "            print(f\"  ... and {len(all_issues['empty_files']) - 20} more\")\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    \n",
    "    # Column errors\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"Column errors (missing required columns)\")\n",
    "    print(\"-\"*100)\n",
    "    if all_issues['column_errors']:\n",
    "        print(f\"Total: {len(all_issues['column_errors'])}\\n\")\n",
    "        for item in all_issues['column_errors'][:20]:\n",
    "            print(f\"  {item['year']}/{item['file']}\")\n",
    "            print(f\"    Missing: {item['missing_columns']}\")\n",
    "        if len(all_issues['column_errors']) > 20:\n",
    "            print(f\"  ... and {len(all_issues['column_errors']) - 20} more\")\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    \n",
    "    # Duplicate timestamps\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"Duplicate timestamps\")\n",
    "    print(\"-\"*100)\n",
    "    if all_issues['duplicate_timestamps']:\n",
    "        print(f\"Total: {len(all_issues['duplicate_timestamps'])}\\n\")\n",
    "        for item in sorted(all_issues['duplicate_timestamps'], key=lambda x: x['duplicate_pct'], reverse=True)[:20]:\n",
    "            print(f\"  {item['year']}/{item['file']}\")\n",
    "            print(f\"    Duplicates: {item['duplicate_count']}/{item['total_rows']} ({item['duplicate_pct']}%)\")\n",
    "        if len(all_issues['duplicate_timestamps']) > 20:\n",
    "            print(f\"  ... and {len(all_issues['duplicate_timestamps']) - 20} more\")\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    \n",
    "    # Missing SiteCode\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"Missing SiteCode values\")\n",
    "    print(\"-\"*100)\n",
    "    if all_issues['missing_sitecode']:\n",
    "        print(f\"Total: {len(all_issues['missing_sitecode'])}\\n\")\n",
    "        for item in sorted(all_issues['missing_sitecode'], key=lambda x: x['missing_pct'], reverse=True)[:20]:\n",
    "            print(f\"  {item['year']}/{item['file']}\")\n",
    "            print(f\"    Missing: {item['missing_count']}/{item['total_rows']} ({item['missing_pct']}%)\")\n",
    "        if len(all_issues['missing_sitecode']) > 20:\n",
    "            print(f\"  ... and {len(all_issues['missing_sitecode']) - 20} more\")\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    \n",
    "    # Missing SpeciesCode\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"Missing SpeciesCode values\")\n",
    "    print(\"-\"*100)\n",
    "    if all_issues['missing_speciescode']:\n",
    "        print(f\"Total: {len(all_issues['missing_speciescode'])}\\n\")\n",
    "        for item in sorted(all_issues['missing_speciescode'], key=lambda x: x['missing_pct'], reverse=True)[:20]:\n",
    "            print(f\"  {item['year']}/{item['file']}\")\n",
    "            print(f\"    Missing: {item['missing_count']}/{item['total_rows']} ({item['missing_pct']}%)\")\n",
    "        if len(all_issues['missing_speciescode']) > 20:\n",
    "            print(f\"  ... and {len(all_issues['missing_speciescode']) - 20} more\")\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    \n",
    "    # High missing values\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"High missing values (>20%)\")\n",
    "    print(\"-\"*100)\n",
    "    if all_issues['high_missing_values']:\n",
    "        print(f\"Total: {len(all_issues['high_missing_values'])}\\n\")\n",
    "        for item in sorted(all_issues['high_missing_values'], key=lambda x: x['missing_pct'], reverse=True)[:20]:\n",
    "            print(f\"  {item['year']}/{item['file']}\")\n",
    "            print(f\"    Missing: {item['missing_count']}/{item['total_rows']} ({item['missing_pct']}%)\")\n",
    "        if len(all_issues['high_missing_values']) > 20:\n",
    "            print(f\"  ... and {len(all_issues['high_missing_values']) - 20} more\")\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    \n",
    "    # Format errors\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"Format errors (Cannot read file)\")\n",
    "    print(\"-\"*100)\n",
    "    if all_issues['format_errors']:\n",
    "        print(f\"Total: {len(all_issues['format_errors'])}\\n\")\n",
    "        for item in all_issues['format_errors'][:10]:\n",
    "            print(f\"  {item['year']}/{item['file']}\")\n",
    "            print(f\"    Error: {item['error'][:80]}\")\n",
    "        if len(all_issues['format_errors']) > 10:\n",
    "            print(f\"  ... and {len(all_issues['format_errors']) - 10} more\")\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Test result: final summary\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    total_issues = sum(len(v) for v in all_issues.values())\n",
    "    \n",
    "    print(f\"\\nTotal issues found: {total_issues}\")\n",
    "    print(f\"  Empty files: {len(all_issues['empty_files'])}\")\n",
    "    print(f\"  Column errors: {len(all_issues['column_errors'])}\")\n",
    "    print(f\"  Duplicate timestamps: {len(all_issues['duplicate_timestamps'])}\")\n",
    "    print(f\"  Missing SiteCode: {len(all_issues['missing_sitecode'])}\")\n",
    "    print(f\"  Missing SpeciesCode: {len(all_issues['missing_speciescode'])}\")\n",
    "    print(f\"  High missing values (>20%): {len(all_issues['high_missing_values'])}\")\n",
    "    print(f\"  Format errors: {len(all_issues['format_errors'])}\")\n",
    "    \n",
    "    if total_issues == 0:\n",
    "        print(\"\\n✓ All files passed quality checks!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ {total_issues} issues need attention\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
