{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccfa1de",
   "metadata": {},
   "source": [
    "# LAQN Dataset Assesment\n",
    "\n",
    "\n",
    "1) I'll be start adding my main paths and modules I will be using in this notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfa1335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible python modules i will be using below\n",
    "from curses import meta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "#function 7 importing the full analysis function from pollution_analysis\n",
    "import sys\n",
    "sys.path.append('/mnt/user-data/outputs')\n",
    "\n",
    "#last detailed anlasye and visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualisation style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "#findings 7 .func\n",
    "# for parse pdf uk pollutant limitations to csv\n",
    "import re\n",
    "# pdfplumber for pdf parsing\n",
    "\n",
    "# function 5. chi-square test\n",
    "from scipy import stats\n",
    "\n",
    "#define base path  without hardcoding\n",
    "base_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"optimased\"\n",
    "#metadata file for pollutant name, location and site names\n",
    "metadata_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"optimased_siteSpecies.csv\"\n",
    "\n",
    "# output path for saving statistics 1. function\n",
    "#the first analyse dataset created without inclitiong nan optimased files, and cross referencing that's why changed the name to dataset_statistics-noNAN-incl.csv\n",
    "os.makedirs(base_dir / \"report\", exist_ok=True)\n",
    "stats_output_path =  Path.home()/\"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"report\"/ \"laqn_stats.csv\"\n",
    "\n",
    "# output paths for saving all the pollutant distribution and nan value analysis.\n",
    "pollutant_distrubution_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\"/\"pollutant_distribution.csv\"\n",
    "nan_val_pollutant_split_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_pollutant.csv\"\n",
    "nan_val_stationPollutant_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_station_pollutant.csv\"\n",
    "\n",
    "\n",
    "# log file from nan replacement process\n",
    "nan_log_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"logs\" / \"NaN_values_record.csv\"\n",
    "\n",
    "# function for uk pollutant regulations pdf to parse csv file path\n",
    "csv_output_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"capabilities\" / \"uk_pollutant_limits.csv\"\n",
    "\n",
    "\n",
    "# data quality metrics report output path\n",
    "quality_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\"/ \"report\" / \"quality_metrics_validation.csv\"\n",
    "quality_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#chi-square test output path func 5\n",
    "chi_square_output1 = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests1.csv\"\n",
    "chi_square_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests.csv\"\n",
    "\n",
    "# detailed last analysis and visualization output directory\n",
    "report_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"detailed_analysis\"\n",
    "\n",
    "report_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d9baf",
   "metadata": {},
   "source": [
    "## 1) Initial Dataset Assessment: Raw Numbers\n",
    "\n",
    "Before conducting quality checks, I need to establish the baseline characteristics of the LAQN dataset. This section calculates comprehensive statistics about the data collection effort, including file counts, measurement records, station coverage, and pollutant distribution.\n",
    "\n",
    "### Purpose\n",
    "- Document the scale and scope of data collection.\n",
    "- Establish baseline metrics for comparison with LAQN.\n",
    "- Provide context for subsequent quality analysis.\n",
    "\n",
    "### Methodology\n",
    "The function `get_laqn_dataset_statistics()` performs the following:\n",
    "1. Loads standardised metadata to identify unique stations and pollutants.\n",
    "2. Counts files across all three yearly directories (2023, 2024, 2025).\n",
    "3. Calculates total measurement records by reading all CSV files.\n",
    "4. Determines spatial coverage from unique coordinate pairs.\n",
    "5. Documents temporal coverage (35 months: January 2023 to November 2025).\n",
    "\n",
    "### Notes\n",
    "- File counting is fast (scans directory structure only).\n",
    "- Record counting can be slow (reads every CSV file).\n",
    "- Results are saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d680dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path):\n",
    "    \"\"\"\n",
    "    Calculate statistics for the LAQN dataset using the new column structure.\n",
    "    This function scans all CSV files recursively under base_dir and calculates key metrics needed for reporting.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir : Path\n",
    "            Base directory containing LAQN data folders.\n",
    "        metadata_path : Path\n",
    "            Path to the standardised metadata csv file.\n",
    "        nan_log_path : Path\n",
    "            Path to the NaN values log file after notice data flags, changed them to NaN.\n",
    "\n",
    "    Returns:\n",
    "        dict : Dictionary containing all calculated statistics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # read metadata to get station and pollutant info\n",
    "    print(\"\\nloading metadata...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "\n",
    "    # calculate metadata statistics\n",
    "    stats['unique_stations'] = metadata['SiteCode'].nunique()\n",
    "    stats['total_combinations'] = len(metadata)\n",
    "    stats['unique_pollutants'] = metadata['SpeciesCode'].nunique()\n",
    "\n",
    "    # get pollutant breakdown\n",
    "    pollutant_counts = metadata['SpeciesCode'].value_counts()\n",
    "    stats['pollutant_distribution'] = pollutant_counts.to_dict()\n",
    "\n",
    "    # create set of expected (SiteCode, SpeciesCode) pairs from metadata\n",
    "    expected_pairs = set(\n",
    "        zip(metadata['SiteCode'], metadata['SpeciesCode'])\n",
    "    )\n",
    "    stats['expected_pairs'] = len(expected_pairs)\n",
    "    print(f\"  expected SiteCode/SpeciesCode pairs from metadata: {len(expected_pairs)}\")\n",
    "\n",
    "    # count unique coordinates for spatial coverage\n",
    "    unique_coords = metadata[['Latitude', 'Longitude']].drop_duplicates()\n",
    "    stats['unique_locations'] = len(unique_coords)\n",
    "\n",
    "    # Scan all CSVs in all subfolders under base_dir\n",
    "    print(\"\\nscanning optimased directory for collected data...\")\n",
    "    all_csv_files = list(Path(base_dir).rglob('*.csv'))\n",
    "    total_files = len(all_csv_files)\n",
    "    print(f\"\\nTotal CSV files found: {total_files}\")\n",
    "    stats['total_files'] = total_files\n",
    "\n",
    "    # Count files, records, and missing values by period (e.g., \"2023_apr\")\n",
    "    files_by_period = defaultdict(int)\n",
    "    records_by_period = defaultdict(int)\n",
    "    missing_by_period = defaultdict(int)\n",
    "\n",
    "    all_csvs = []\n",
    "    total_records = 0\n",
    "    total_missing = 0\n",
    "\n",
    "    print(\"\\nReading all CSV files to calculate statistics...\")\n",
    "    for csv_file in all_csv_files:\n",
    "        period = csv_file.parent.name\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            n_records = len(df)\n",
    "            n_missing = df['@Value'].isna().sum() + (df['@Value'] == \"\").sum() if '@Value' in df.columns else 0\n",
    "            all_csvs.append(df)\n",
    "            files_by_period[period] += 1\n",
    "            records_by_period[period] += n_records\n",
    "            missing_by_period[period] += n_missing\n",
    "            total_records += n_records\n",
    "            total_missing += n_missing\n",
    "        except Exception as e:\n",
    "            print(f\"  warning: could not read {csv_file.name}: {e}\")\n",
    "\n",
    "    stats['files_by_period'] = dict(files_by_period)\n",
    "    stats['records_by_period'] = dict(records_by_period)\n",
    "    stats['missing_by_period'] = dict(missing_by_period)\n",
    "    stats['total_records'] = total_records\n",
    "    stats['total_missing'] = total_missing\n",
    "    stats['overall_completeness'] = ((total_records - total_missing) / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "    for period in files_by_period:\n",
    "        rec = records_by_period[period]\n",
    "        miss = missing_by_period[period]\n",
    "        miss_pct = (miss / rec * 100) if rec > 0 else 0\n",
    "        print(f\"  {period}: {files_by_period[period]} files, {rec:,} records, {miss:,} missing ({miss_pct:.2f}%)\")\n",
    "\n",
    "    # cross-reference metadata with collected data\n",
    "    print(\"\\ncross-referencing collected data with metadata...\")\n",
    "\n",
    "    if all_csvs:\n",
    "        all_data = pd.concat(all_csvs, ignore_index=True)\n",
    "\n",
    "        # check if required columns exist in csv files\n",
    "        if 'SiteCode' in all_data.columns and 'SpeciesCode' in all_data.columns:\n",
    "            # identify actual (SiteCode, SpeciesCode) pairs in collected data\n",
    "            collected_pairs = set(\n",
    "                zip(all_data['SiteCode'], all_data['SpeciesCode'])\n",
    "            )\n",
    "            stats['collected_pairs'] = len(collected_pairs)\n",
    "\n",
    "            # find missing pairs (in metadata but not in collected data)\n",
    "            missing_pairs = expected_pairs - collected_pairs\n",
    "            stats['missing_pairs'] = list(missing_pairs)\n",
    "            stats['missing_pairs_count'] = len(missing_pairs)\n",
    "\n",
    "            # find extra pairs (in collected data but not in metadata)\n",
    "            extra_pairs = collected_pairs - expected_pairs\n",
    "            stats['extra_pairs'] = list(extra_pairs)\n",
    "            stats['extra_pairs_count'] = len(extra_pairs)\n",
    "\n",
    "            print(f\"  expected pairs from metadata: {len(expected_pairs)}\")\n",
    "            print(f\"  actually collected pairs: {len(collected_pairs)}\")\n",
    "            print(f\"  missing pairs (in metadata but not collected): {len(missing_pairs)}\")\n",
    "            print(f\"  extra pairs (collected but not in metadata): {len(extra_pairs)}\")\n",
    "\n",
    "            # group by SiteCode and SpeciesCode, count missing values\n",
    "            missing_breakdown = {}\n",
    "            for (site, species), group in all_data.groupby(['SiteCode', 'SpeciesCode']):\n",
    "                total_rows = len(group)\n",
    "                if '@Value' in group.columns:\n",
    "                    missing_rows = group['@Value'].isna().sum() + (group['@Value'] == \"\").sum()\n",
    "                else:\n",
    "                    missing_rows = 0\n",
    "                missing_breakdown[(site, species)] = (int(missing_rows), int(total_rows))\n",
    "            stats['missing_by_station_pollutant'] = missing_breakdown\n",
    "        else:\n",
    "            print(\"  warning: SiteCode or SpeciesCode columns not found\")\n",
    "            stats['missing_by_station_pollutant'] = {}\n",
    "            stats['collected_pairs'] = 0\n",
    "            stats['missing_pairs'] = []\n",
    "            stats['missing_pairs_count'] = 0\n",
    "            stats['extra_pairs'] = []\n",
    "            stats['extra_pairs_count'] = 0\n",
    "    else:\n",
    "        stats['missing_by_station_pollutant'] = {}\n",
    "        stats['collected_pairs'] = 0\n",
    "        stats['missing_pairs'] = list(expected_pairs)\n",
    "        stats['missing_pairs_count'] = len(expected_pairs)\n",
    "        stats['extra_pairs'] = []\n",
    "        stats['extra_pairs_count'] = 0\n",
    "\n",
    "    # distribution of nan by pollutant over time\n",
    "    if stats['missing_by_station_pollutant']:\n",
    "        pollutant_missing_summary = {}\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            if species not in pollutant_missing_summary:\n",
    "                pollutant_missing_summary[species] = {'total_missing': 0, 'total_records': 0}\n",
    "            pollutant_missing_summary[species]['total_missing'] += missing\n",
    "            pollutant_missing_summary[species]['total_records'] += total\n",
    "        for species in pollutant_missing_summary:\n",
    "            total_missing = pollutant_missing_summary[species]['total_missing']\n",
    "            total_records = pollutant_missing_summary[species]['total_records']\n",
    "            percentage = (total_missing / total_records * 100) if total_records > 0 else 0\n",
    "            pollutant_missing_summary[species]['percentage_missing'] = percentage\n",
    "        stats['missing_by_pollutant_type'] = pollutant_missing_summary\n",
    "    else:\n",
    "        stats['missing_by_pollutant_type'] = {}\n",
    "\n",
    "    # log file created during data cleaning process\n",
    "    if Path(nan_log_path).exists():\n",
    "        nan_log = pd.read_csv(nan_log_path)\n",
    "        replacements_by_year = nan_log.groupby('year_folder')['invalid_flags_replaced'].sum().to_dict()\n",
    "        stats['nan_replacements_by_year'] = replacements_by_year\n",
    "        stats['total_nan_replacements'] = nan_log['invalid_flags_replaced'].sum()\n",
    "        stats['mean_invalid_percentage'] = nan_log['percentage_invalid'].mean()\n",
    "        stats['max_invalid_percentage'] = nan_log['percentage_invalid'].max()\n",
    "    else:\n",
    "        stats['nan_replacements_by_year'] = {}\n",
    "        stats['total_nan_replacements'] = 0\n",
    "        stats['mean_invalid_percentage'] = 0\n",
    "        stats['max_invalid_percentage'] = 0\n",
    "\n",
    "    # calculate temporal coverage based on the files collected\n",
    "    stats['temporal_coverage'] = {\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2025-11-19',\n",
    "        'total_months': 35\n",
    "    }\n",
    "\n",
    "\n",
    "    def extract_year(period):\n",
    "        # period is usually like '2023_apr' or '2024_jan'\n",
    "        return str(period)[:4] if len(str(period)) >= 4 and str(period)[:4].isdigit() else 'unknown'\n",
    "\n",
    "    files_by_year = defaultdict(int)\n",
    "    records_by_year = defaultdict(int)\n",
    "    missing_by_year = defaultdict(int)\n",
    "    for period in files_by_period:\n",
    "        year = extract_year(period)\n",
    "        files_by_year[year] += files_by_period[period]\n",
    "        records_by_year[year] += records_by_period[period]\n",
    "        missing_by_year[year] += missing_by_period[period]\n",
    "    stats['files_by_year'] = dict(files_by_year)\n",
    "    stats['records_by_year'] = dict(records_by_year)\n",
    "    stats['missing_by_year'] = dict(missing_by_year)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e2a9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(stats):\n",
    "    \"\"\"\n",
    "    Print dataset statistics for LAQN using new column structure.\n",
    "\n",
    "    Parameters:\n",
    "        stats : dict\n",
    "            returned by get_laqn_dataset_statistics().\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"LAQN dataset statistics: initial assessment\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    print(\"\\nScale and scope:\")\n",
    "    print(f\"Total files collected: {stats['total_files']:,}\")\n",
    "    print(f\"Total measurement records: {stats['total_records']:,}\")\n",
    "    print(f\"Total missing values (@Value): {stats['total_missing']:,}\")\n",
    "    print(f\"Overall completeness: {stats['overall_completeness']:.2f}%\")\n",
    "    print(f\"Unique monitoring sites (SiteCode): {stats['unique_stations']}\")\n",
    "    print(f\"Total site-species combinations: {stats['total_combinations']}\")\n",
    "    print(f\"Unique pollutant types (SpeciesCode): {stats['unique_pollutants']}\")\n",
    "    print(f\"Unique geographic locations: {stats['unique_locations']}\")\n",
    "\n",
    "    # data collection coverage\n",
    "    print(\"\\nData collection coverage:\")\n",
    "    print(f\"Expected SiteCode/SpeciesCode pairs (from metadata): {stats.get('expected_pairs', 0)}\")\n",
    "    print(f\"Actually collected pairs: {stats.get('collected_pairs', 0)}\")\n",
    "    print(f\"Missing pairs (not collected): {stats.get('missing_pairs_count', 0)}\")\n",
    "    print(f\"Extra pairs (not in metadata): {stats.get('extra_pairs_count', 0)}\")\n",
    "\n",
    "    if stats.get('missing_pairs_count', 0) > 0:\n",
    "        print(f\"\\nWarning: {stats['missing_pairs_count']} SiteCode/SpeciesCode pairs from metadata were not found in collected data.\")\n",
    "        print(\"First 10 missing pairs:\")\n",
    "        for i, (site, species) in enumerate(stats['missing_pairs'][:10], 1):\n",
    "            print(f\"  {i}. {site} - {species}\")\n",
    "\n",
    "    if stats.get('extra_pairs_count', 0) > 0:\n",
    "        print(f\"\\nNote: {stats['extra_pairs_count']} SiteCode/SpeciesCode pairs in collected data are not in metadata.\")\n",
    "\n",
    "    print(\"\\nFiles by year:\")\n",
    "    for year, count in stats['files_by_year'].items():\n",
    "        print(f\"  {year}: {count:,} files\")\n",
    "\n",
    "    print(\"\\nRecords by year:\")\n",
    "    for year, count in stats['records_by_year'].items():\n",
    "        missing = stats['missing_by_year'].get(year, 0)\n",
    "        missing_pct = (missing / count * 100) if count > 0 else 0\n",
    "        print(f\"  {year}: {count:,} records, {missing:,} missing ({missing_pct:.2f}%)\")\n",
    "\n",
    "    # adding nan value summary below\n",
    "    print(\"\\nNaN replacement summary:\")\n",
    "    print(f\"Total invalid flags replaced: {stats['total_nan_replacements']:,}\")\n",
    "    print(f\"Mean invalid percentage per file: {stats['mean_invalid_percentage']:.2f}%\")\n",
    "    print(f\"Max invalid percentage: {stats['max_invalid_percentage']:.2f}%\")\n",
    "\n",
    "    # count of replacements by year\n",
    "    if stats['nan_replacements_by_year']:\n",
    "        print(\"\\nReplacements by year:\")\n",
    "        for year_folder, count in stats['nan_replacements_by_year'].items():\n",
    "            print(f\"  {year_folder}: {count:,} flags replaced\")\n",
    "\n",
    "    print(\"\\nTemporal coverage:\")\n",
    "    print(f\"Start date: {stats['temporal_coverage']['start_date']}\")\n",
    "    print(f\"End date: {stats['temporal_coverage']['end_date']}\")\n",
    "    print(f\"Total months: {stats['temporal_coverage']['total_months']}\")\n",
    "\n",
    "    print(\"\\nPollutant (SpeciesCode) distribution:\")\n",
    "    print(\"Site/species combinations by type:\")\n",
    "    for species, count in sorted(stats['pollutant_distribution'].items(),\n",
    "                                 key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_combinations']) * 100\n",
    "        print(f\"  {species}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # missing value distribution by pollutant type\n",
    "    print(\"\\nMissing value distribution by pollutant type (SpeciesCode):\")\n",
    "    if stats.get('missing_by_pollutant_type'):\n",
    "        # sort by percentage missing (highest first)\n",
    "        sorted_species = sorted(\n",
    "            stats['missing_by_pollutant_type'].items(),\n",
    "            key=lambda x: x[1]['percentage_missing'],\n",
    "            reverse=True\n",
    "        )\n",
    "        print(f\"{'SpeciesCode':<20} {'total records':>15} {'missing':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for species, data in sorted_species:\n",
    "            print(f\"{species:<20} {data['total_records']:>15,} {data['total_missing']:>12,} {data['percentage_missing']:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value distribution available.\")\n",
    "\n",
    "    # print missing values by site/species breakdown with row_number column\n",
    "    print(\"\\nMissing values by site/species (SiteCode/SpeciesCode):\")\n",
    "    if stats.get('missing_by_station_pollutant'):\n",
    "        # prepare a sorted list by missing percentage descending\n",
    "        breakdown = []\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            percent = (missing / total * 100) if total > 0 else 0\n",
    "            breakdown.append((site, species, missing, total, percent))\n",
    "        # sort by percentage descending and take top 20\n",
    "        breakdown.sort(key=lambda x: x[4], reverse=True)\n",
    "        breakdown = breakdown[:20]\n",
    "        print(f\"{'SiteCode':<20} {'SpeciesCode':<20} {'missing':>10} {'total_row':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for site, species, missing, total, percent in breakdown:\n",
    "            print(f\"{site:<20} {species:<20} {missing:>10,} {total:>12,} {percent:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value breakdown available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "602d9919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading metadata...\n",
      "  expected SiteCode/SpeciesCode pairs from metadata: 170\n",
      "\n",
      "scanning optimased directory for collected data...\n",
      "\n",
      "Total CSV files found: 4932\n",
      "\n",
      "Reading all CSV files to calculate statistics...\n",
      "  2023_mar: 141 files, 101,520 records, 14,883 missing (14.66%)\n",
      "  2025_feb: 141 files, 91,368 records, 12,034 missing (13.17%)\n",
      "  2024_feb: 141 files, 94,752 records, 9,582 missing (10.11%)\n",
      "  2025_aug: 141 files, 101,520 records, 16,123 missing (15.88%)\n",
      "  2024_aug: 141 files, 101,520 records, 19,425 missing (19.13%)\n",
      "  2025_mar: 141 files, 101,520 records, 15,384 missing (15.15%)\n",
      "  2023_feb: 141 files, 91,368 records, 12,838 missing (14.05%)\n",
      "  2024_mar: 141 files, 101,520 records, 11,279 missing (11.11%)\n",
      "  2023_aug: 141 files, 101,520 records, 11,360 missing (11.19%)\n",
      "  2024_jul: 141 files, 101,520 records, 12,934 missing (12.74%)\n",
      "  2025_jul: 141 files, 101,520 records, 16,506 missing (16.26%)\n",
      "  2024_oct: 141 files, 101,520 records, 11,079 missing (10.91%)\n",
      "  2023_sep: 141 files, 98,136 records, 11,727 missing (11.95%)\n",
      "  2025_oct: 141 files, 101,520 records, 12,342 missing (12.16%)\n",
      "  2023_jan: 141 files, 101,520 records, 17,911 missing (17.64%)\n",
      "  2023_jul: 141 files, 101,520 records, 11,160 missing (10.99%)\n",
      "  2024_jan: 141 files, 101,520 records, 10,375 missing (10.22%)\n",
      "  2025_sep: 141 files, 98,136 records, 18,344 missing (18.69%)\n",
      "  2024_sep: 141 files, 98,136 records, 15,469 missing (15.76%)\n",
      "  2023_oct: 141 files, 101,520 records, 13,964 missing (13.75%)\n",
      "  2025_jan: 141 files, 101,520 records, 11,198 missing (11.03%)\n",
      "  2024_dec: 141 files, 101,520 records, 9,421 missing (9.28%)\n",
      "  2024_apr: 141 files, 98,136 records, 11,208 missing (11.42%)\n",
      "  2024_nov: 141 files, 98,136 records, 9,063 missing (9.24%)\n",
      "  2023_may: 141 files, 101,520 records, 12,641 missing (12.45%)\n",
      "  2025_nov: 141 files, 60,912 records, 8,613 missing (14.14%)\n",
      "  2025_apr: 141 files, 98,136 records, 10,606 missing (10.81%)\n",
      "  2024_may: 141 files, 101,520 records, 12,839 missing (12.65%)\n",
      "  2025_may: 141 files, 101,520 records, 10,843 missing (10.68%)\n",
      "  2023_nov: 138 files, 96,048 records, 10,851 missing (11.30%)\n",
      "  2023_apr: 141 files, 98,136 records, 10,992 missing (11.20%)\n",
      "  2023_dec: 141 files, 101,520 records, 14,882 missing (14.66%)\n",
      "  2025_jun: 141 files, 98,136 records, 14,018 missing (14.28%)\n",
      "  2024_jun: 141 files, 98,136 records, 9,661 missing (9.84%)\n",
      "  2023_jun: 141 files, 98,136 records, 12,103 missing (12.33%)\n",
      "\n",
      "cross-referencing collected data with metadata...\n",
      "  expected pairs from metadata: 170\n",
      "  actually collected pairs: 141\n",
      "  missing pairs (in metadata but not collected): 53\n",
      "  extra pairs (collected but not in metadata): 24\n",
      "\n",
      "========================================\n",
      "LAQN dataset statistics: initial assessment\n",
      "========================================\n",
      "\n",
      "Scale and scope:\n",
      "Total files collected: 4,932\n",
      "Total measurement records: 3,446,208\n",
      "Total missing values (@Value): 443,658\n",
      "Overall completeness: 87.13%\n",
      "Unique monitoring sites (SiteCode): 78\n",
      "Total site-species combinations: 173\n",
      "Unique pollutant types (SpeciesCode): 6\n",
      "Unique geographic locations: 76\n",
      "\n",
      "Data collection coverage:\n",
      "Expected SiteCode/SpeciesCode pairs (from metadata): 170\n",
      "Actually collected pairs: 141\n",
      "Missing pairs (not collected): 53\n",
      "Extra pairs (not in metadata): 24\n",
      "\n",
      "Warning: 53 SiteCode/SpeciesCode pairs from metadata were not found in collected data.\n",
      "First 10 missing pairs:\n",
      "  1. BL0 - PM25\n",
      "  2. TH4 - PM25\n",
      "  3. BT6 - PM25\n",
      "  4. MEB - PM25\n",
      "  5. GN6 - PM25\n",
      "  6. GR8 - PM25\n",
      "  7. GN3 - PM25\n",
      "  8. TL6 - PM25\n",
      "  9. GT1 - PM25\n",
      "  10. CE3 - PM25\n",
      "\n",
      "Note: 24 SiteCode/SpeciesCode pairs in collected data are not in metadata.\n",
      "\n",
      "Files by year:\n",
      "  2023: 1,689 files\n",
      "  2025: 1,551 files\n",
      "  2024: 1,692 files\n",
      "\n",
      "Records by year:\n",
      "  2023: 1,192,464 records, 155,312 missing (13.02%)\n",
      "  2025: 1,055,808 records, 146,011 missing (13.83%)\n",
      "  2024: 1,197,936 records, 142,335 missing (11.88%)\n",
      "\n",
      "NaN replacement summary:\n",
      "Total invalid flags replaced: 0\n",
      "Mean invalid percentage per file: 0.00%\n",
      "Max invalid percentage: 0.00%\n",
      "\n",
      "Temporal coverage:\n",
      "Start date: 2023-01-01\n",
      "End date: 2025-11-19\n",
      "Total months: 35\n",
      "\n",
      "Pollutant (SpeciesCode) distribution:\n",
      "Site/species combinations by type:\n",
      "  NO2: 60 (34.7%)\n",
      "  PM25: 53 (30.6%)\n",
      "  PM10: 43 (24.9%)\n",
      "  O3: 11 (6.4%)\n",
      "  SO2: 4 (2.3%)\n",
      "  CO: 2 (1.2%)\n",
      "\n",
      "Missing value distribution by pollutant type (SpeciesCode):\n",
      "SpeciesCode            total records      missing    % missing\n",
      "------------------------------------------------------------\n",
      "O3                           268,320       47,056       17.54%\n",
      "PM2.5                        586,944      100,755       17.17%\n",
      "SO2                           97,824       15,803       16.15%\n",
      "PM10                       1,026,456      126,749       12.35%\n",
      "NO2                        1,417,752      148,803       10.50%\n",
      "CO                            48,912        4,492        9.18%\n",
      "\n",
      "Missing values by site/species (SiteCode/SpeciesCode):\n",
      "SiteCode             SpeciesCode             missing    total_row    % missing\n",
      "------------------------------------------------------------\n",
      "WM6                  PM10                     15,357       24,456       62.79%\n",
      "CE3                  NO2                      11,394       24,456       46.59%\n",
      "TL4                  NO2                       9,869       24,456       40.35%\n",
      "RI2                  O3                        9,732       24,456       39.79%\n",
      "WA7                  NO2                       9,236       24,456       37.77%\n",
      "BG1                  SO2                       8,373       24,456       34.24%\n",
      "TH4                  PM2.5                     8,042       24,456       32.88%\n",
      "CD1                  PM10                      7,711       24,456       31.53%\n",
      "WAA                  NO2                       7,697       24,456       31.47%\n",
      "CE3                  PM10                      7,652       24,456       31.29%\n",
      "CE3                  PM2.5                     7,652       24,456       31.29%\n",
      "TH4                  PM10                      7,457       24,456       30.49%\n",
      "CD1                  PM2.5                     7,339       24,456       30.01%\n",
      "GN6                  PM2.5                     7,240       24,456       29.60%\n",
      "CR8                  PM2.5                     7,207       24,456       29.47%\n",
      "GN0                  PM2.5                     7,099       24,456       29.03%\n",
      "HG4                  O3                        7,092       24,456       29.00%\n",
      "CD1                  NO2                       6,984       24,456       28.56%\n",
      "BT5                  PM2.5                     6,783       24,456       27.74%\n",
      "MY1                  O3                        6,634       24,456       27.13%\n",
      "\n",
      "Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/laqn_stats.csv\n",
      "Species (pollutant) distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/pollutant_distribution.csv\n",
      "Missing value distribution by species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_pollutant.csv\n",
      "Missing values by site/species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_station_pollutant.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "stats = get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path)\n",
    "print_dataset_statistics(stats)\n",
    "\n",
    "# Save statistics for later use as csv\n",
    "# Prepare flat data structure for csv\n",
    "stats_rows = []\n",
    "stats_rows.append([\"metric\", \"value\"])\n",
    "stats_rows.append([\"total_files\", stats['total_files']])\n",
    "stats_rows.append([\"total_records\", stats['total_records']])\n",
    "stats_rows.append([\"total_missing\", stats['total_missing']])\n",
    "stats_rows.append([\"overall_completeness_pct\", f\"{stats['overall_completeness']:.2f}\"])\n",
    "stats_rows.append([\"unique_sites\", stats['unique_stations']])\n",
    "stats_rows.append([\"total_site_species_combinations\", stats['total_combinations']])\n",
    "stats_rows.append([\"unique_species\", stats['unique_pollutants']])\n",
    "stats_rows.append([\"unique_locations\", stats['unique_locations']])\n",
    "stats_rows.append([\"expected_site_species_pairs\", stats.get('expected_pairs', 0)])\n",
    "stats_rows.append([\"collected_site_species_pairs\", stats.get('collected_pairs', 0)])\n",
    "stats_rows.append([\"missing_site_species_pairs_count\", stats.get('missing_pairs_count', 0)])\n",
    "stats_rows.append([\"extra_site_species_pairs_count\", stats.get('extra_pairs_count', 0)])\n",
    "stats_rows.append([\"total_nan_replacements\", stats['total_nan_replacements']])\n",
    "stats_rows.append([\"mean_invalid_pct\", f\"{stats['mean_invalid_percentage']:.2f}\"])\n",
    "stats_rows.append([\"max_invalid_pct\", f\"{stats['max_invalid_percentage']:.2f}\"])\n",
    "\n",
    "# Add year-specific metrics\n",
    "for year in ['2023', '2024', '2025']:\n",
    "    stats_rows.append([f\"files_{year}\", stats['files_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"records_{year}\", stats['records_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"missing_{year}\", stats['missing_by_year'].get(year, 0)])\n",
    "    year_key = f'{year}measurements'\n",
    "    stats_rows.append([f\"replacements_{year}\", stats['nan_replacements_by_year'].get(year_key, 0)])\n",
    "\n",
    "# Save to csv stats report\n",
    "pd.DataFrame(stats_rows[1:], columns=stats_rows[0]).to_csv(stats_output_path, index=False)\n",
    "print(f\"\\nStatistics saved to: {stats_output_path}\")\n",
    "\n",
    "# Save species (pollutant) distribution to csv\n",
    "total_combinations = stats['total_combinations']\n",
    "species_distribution_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'count': v,\n",
    "            'percentage': round((v / total_combinations) * 100, 2) if total_combinations > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['pollutant_distribution'].items()\n",
    "    ]\n",
    ")\n",
    "species_distribution_df.to_csv(pollutant_distrubution_path, index=False)\n",
    "print(f\"Species (pollutant) distribution saved to: {pollutant_distrubution_path}\")\n",
    "\n",
    "# Save missing value distribution by species to csv\n",
    "if stats.get('missing_by_pollutant_type'):\n",
    "    missing_by_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'total_records': v['total_records'],\n",
    "            'total_missing': v['total_missing'],\n",
    "            'percentage_missing': v['percentage_missing']\n",
    "        }\n",
    "        for k, v in stats['missing_by_pollutant_type'].items()\n",
    "    ])\n",
    "    missing_by_species_df.to_csv(nan_val_pollutant_split_path, index=False)\n",
    "    print(f\"Missing value distribution by species saved to: {nan_val_pollutant_split_path}\")\n",
    "\n",
    "# Save missing values by site/species to csv\n",
    "if stats.get('missing_by_station_pollutant'):\n",
    "    missing_by_site_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SiteCode': k[0],\n",
    "            'SpeciesCode': k[1],\n",
    "            'missing': v[0],\n",
    "            'total_row': v[1],\n",
    "            'percentage_missing': (v[0] / v[1] * 100) if v[1] > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['missing_by_station_pollutant'].items()\n",
    "    ])\n",
    "    missing_by_site_species_df.to_csv(nan_val_stationPollutant_path, index=False)\n",
    "    print(f\"Missing values by site/species saved to: {nan_val_stationPollutant_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bd249",
   "metadata": {},
   "source": [
    "    loading metadata...\n",
    "    expected SiteCode/SpeciesCode pairs from metadata: 170\n",
    "\n",
    "    scanning optimased directory for collected data...\n",
    "\n",
    "    Total CSV files found: 4932\n",
    "\n",
    "    Reading all CSV files to calculate statistics...\n",
    "    2023_mar: 141 files, 101,520 records, 14,883 missing (14.66%)\n",
    "    2025_feb: 141 files, 91,368 records, 12,034 missing (13.17%)\n",
    "    2024_feb: 141 files, 94,752 records, 9,582 missing (10.11%)\n",
    "    2025_aug: 141 files, 101,520 records, 16,123 missing (15.88%)\n",
    "    2024_aug: 141 files, 101,520 records, 19,425 missing (19.13%)\n",
    "    2025_mar: 141 files, 101,520 records, 15,384 missing (15.15%)\n",
    "    2023_feb: 141 files, 91,368 records, 12,838 missing (14.05%)\n",
    "    2024_mar: 141 files, 101,520 records, 11,279 missing (11.11%)\n",
    "    2023_aug: 141 files, 101,520 records, 11,360 missing (11.19%)\n",
    "    2024_jul: 141 files, 101,520 records, 12,934 missing (12.74%)\n",
    "    2025_jul: 141 files, 101,520 records, 16,506 missing (16.26%)\n",
    "    2024_oct: 141 files, 101,520 records, 11,079 missing (10.91%)\n",
    "    2023_sep: 141 files, 98,136 records, 11,727 missing (11.95%)\n",
    "    2025_oct: 141 files, 101,520 records, 12,342 missing (12.16%)\n",
    "    2023_jan: 141 files, 101,520 records, 17,911 missing (17.64%)\n",
    "    2023_jul: 141 files, 101,520 records, 11,160 missing (10.99%)\n",
    "    2024_jan: 141 files, 101,520 records, 10,375 missing (10.22%)\n",
    "    2025_sep: 141 files, 98,136 records, 18,344 missing (18.69%)\n",
    "    2024_sep: 141 files, 98,136 records, 15,469 missing (15.76%)\n",
    "    2023_oct: 141 files, 101,520 records, 13,964 missing (13.75%)\n",
    "    2025_jan: 141 files, 101,520 records, 11,198 missing (11.03%)\n",
    "    2024_dec: 141 files, 101,520 records, 9,421 missing (9.28%)\n",
    "    2024_apr: 141 files, 98,136 records, 11,208 missing (11.42%)\n",
    "    2024_nov: 141 files, 98,136 records, 9,063 missing (9.24%)\n",
    "    2023_may: 141 files, 101,520 records, 12,641 missing (12.45%)\n",
    "    2025_nov: 141 files, 60,912 records, 8,613 missing (14.14%)\n",
    "    2025_apr: 141 files, 98,136 records, 10,606 missing (10.81%)\n",
    "    2024_may: 141 files, 101,520 records, 12,839 missing (12.65%)\n",
    "    2025_may: 141 files, 101,520 records, 10,843 missing (10.68%)\n",
    "    2023_nov: 138 files, 96,048 records, 10,851 missing (11.30%)\n",
    "    2023_apr: 141 files, 98,136 records, 10,992 missing (11.20%)\n",
    "    2023_dec: 141 files, 101,520 records, 14,882 missing (14.66%)\n",
    "    2025_jun: 141 files, 98,136 records, 14,018 missing (14.28%)\n",
    "    2024_jun: 141 files, 98,136 records, 9,661 missing (9.84%)\n",
    "    2023_jun: 141 files, 98,136 records, 12,103 missing (12.33%)\n",
    "\n",
    "    cross-referencing collected data with metadata...\n",
    "    expected pairs from metadata: 170\n",
    "    actually collected pairs: 141\n",
    "    missing pairs (in metadata but not collected): 53\n",
    "    extra pairs (collected but not in metadata): 24\n",
    "\n",
    "    ========================================\n",
    "    LAQN dataset statistics: initial assessment\n",
    "    ========================================\n",
    "\n",
    "    Scale and scope:\n",
    "    Total files collected: 4,932\n",
    "    Total measurement records: 3,446,208\n",
    "    Total missing values (@Value): 443,658\n",
    "    Overall completeness: 87.13%\n",
    "    Unique monitoring sites (SiteCode): 78\n",
    "    Total site-species combinations: 173\n",
    "    Unique pollutant types (SpeciesCode): 6\n",
    "    Unique geographic locations: 76\n",
    "\n",
    "    Data collection coverage:\n",
    "    Expected SiteCode/SpeciesCode pairs (from metadata): 170\n",
    "    Actually collected pairs: 141\n",
    "    Missing pairs (not collected): 53\n",
    "    Extra pairs (not in metadata): 24\n",
    "\n",
    "    Warning: 53 SiteCode/SpeciesCode pairs from metadata were not found in collected data.\n",
    "    First 10 missing pairs:\n",
    "    1. BL0 - PM25\n",
    "    2. TH4 - PM25\n",
    "    3. BT6 - PM25\n",
    "    4. MEB - PM25\n",
    "    5. GN6 - PM25\n",
    "    6. GR8 - PM25\n",
    "    7. GN3 - PM25\n",
    "    8. TL6 - PM25\n",
    "    9. GT1 - PM25\n",
    "    10. CE3 - PM25\n",
    "\n",
    "    Note: 24 SiteCode/SpeciesCode pairs in collected data are not in metadata.\n",
    "\n",
    "    Files by year:\n",
    "    2023: 1,689 files\n",
    "    2025: 1,551 files\n",
    "    2024: 1,692 files\n",
    "\n",
    "    Records by year:\n",
    "    2023: 1,192,464 records, 155,312 missing (13.02%)\n",
    "    2025: 1,055,808 records, 146,011 missing (13.83%)\n",
    "    2024: 1,197,936 records, 142,335 missing (11.88%)\n",
    "\n",
    "    NaN replacement summary:\n",
    "    Total invalid flags replaced: 0\n",
    "    Mean invalid percentage per file: 0.00%\n",
    "    Max invalid percentage: 0.00%\n",
    "\n",
    "    Temporal coverage:\n",
    "    Start date: 2023-01-01\n",
    "    End date: 2025-11-19\n",
    "    Total months: 35\n",
    "\n",
    "    Pollutant (SpeciesCode) distribution:\n",
    "    Site/species combinations by type:\n",
    "    NO2: 60 (34.7%)\n",
    "    PM25: 53 (30.6%)\n",
    "    PM10: 43 (24.9%)\n",
    "    O3: 11 (6.4%)\n",
    "    SO2: 4 (2.3%)\n",
    "    CO: 2 (1.2%)\n",
    "\n",
    "    Missing value distribution by pollutant type (SpeciesCode):\n",
    "    SpeciesCode            total records      missing    % missing\n",
    "    ------------------------------------------------------------\n",
    "    O3                           268,320       47,056       17.54%\n",
    "    PM2.5                        586,944      100,755       17.17%\n",
    "    SO2                           97,824       15,803       16.15%\n",
    "    PM10                       1,026,456      126,749       12.35%\n",
    "    NO2                        1,417,752      148,803       10.50%\n",
    "    CO                            48,912        4,492        9.18%\n",
    "\n",
    "    Missing values by site/species (SiteCode/SpeciesCode):\n",
    "    SiteCode             SpeciesCode             missing    total_row    % missing\n",
    "    ------------------------------------------------------------\n",
    "    WM6                  PM10                     15,357       24,456       62.79%\n",
    "    CE3                  NO2                      11,394       24,456       46.59%\n",
    "    TL4                  NO2                       9,869       24,456       40.35%\n",
    "    RI2                  O3                        9,732       24,456       39.79%\n",
    "    WA7                  NO2                       9,236       24,456       37.77%\n",
    "    BG1                  SO2                       8,373       24,456       34.24%\n",
    "    TH4                  PM2.5                     8,042       24,456       32.88%\n",
    "    CD1                  PM10                      7,711       24,456       31.53%\n",
    "    WAA                  NO2                       7,697       24,456       31.47%\n",
    "    CE3                  PM10                      7,652       24,456       31.29%\n",
    "    CE3                  PM2.5                     7,652       24,456       31.29%\n",
    "    TH4                  PM10                      7,457       24,456       30.49%\n",
    "    CD1                  PM2.5                     7,339       24,456       30.01%\n",
    "    GN6                  PM2.5                     7,240       24,456       29.60%\n",
    "    CR8                  PM2.5                     7,207       24,456       29.47%\n",
    "    GN0                  PM2.5                     7,099       24,456       29.03%\n",
    "    HG4                  O3                        7,092       24,456       29.00%\n",
    "    CD1                  NO2                       6,984       24,456       28.56%\n",
    "    BT5                  PM2.5                     6,783       24,456       27.74%\n",
    "    MY1                  O3                        6,634       24,456       27.13%\n",
    "\n",
    "    Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/laqn_stats.csv\n",
    "    Species (pollutant) distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/pollutant_distribution.csv\n",
    "    Missing value distribution by species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_pollutant.csv\n",
    "    Missing values by site/species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_station_pollutant.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19af0ad",
   "metadata": {},
   "source": [
    "## 3) Data Quality validations:\n",
    "\n",
    "\n",
    "A critical gap from the laqn report by applying formal statistical tests to validate data quality patterns. While descriptive statistics show 0% (before I notice the flags of the dataset) issue rate, I need statistical evidence that this pattern is real and not due to chance.\n",
    "\n",
    "\n",
    "#### Purpuse:\n",
    " Checking data qualities if it is in the limits of eea, and make sence for general logic.\n",
    "- Outlier detection in pollutant measurements.\n",
    "- Data validity ranges based on WHO/EEA standards.\n",
    "- Measurement consistency across time periods.\n",
    "- Quality flags and suspicious patterns.\n",
    "\n",
    "### methodology\n",
    " applies environmental data quality assessment standards:\n",
    "1. Load aggregated measurement data from all csv files.\n",
    "2. Calculate statistical distributions for each pollutant type.\n",
    "3. Identify outliers using IQR method and domain knowledge.\n",
    "4. Check values against established valid ranges.\n",
    "5. Flag suspicious patterns constant values, extreme spikes.\n",
    "6. Calculate quality scores for each station-pollutant combination.\n",
    "\n",
    "#### air quality measurement standards\n",
    "\n",
    "- Uk air quality objectives, limits and policy.\n",
    "- https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://uk-air.defra.gov.uk/assets/documents/Air_Quality_Objectives_Update_20230403.pdf\n",
    "\n",
    "- DEFRA. (2023). *Air Pollution in the UK 2022*.\n",
    "  - Source: https://uk-air.defra.gov.uk/library/annualreport/\n",
    "  - Air Quality Objectives and limit values\n",
    "  - Compliance assessment methodology\n",
    "\n",
    "- UK Air Information Resource. (2024). *Air Pollution: UK Limits*.\n",
    "  - Source: https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "  - Current UK air quality objectives\n",
    "  - Legal limit values and target dates\n",
    "  - Measurement unit specifications (µg/m³)\n",
    "\n",
    "  -  for the rest of the pollutants\n",
    "\n",
    "\n",
    "- uk voc policy:\n",
    "  - https://assets.publishing.service.gov.uk/media/5d7a2912ed915d522e4164a5/VO__statement_Final_12092019_CS__1_.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_negative_values(base_dir, dry_run=True):\n",
    "    summary = []\n",
    "    all_csvs = list(Path(base_dir).rglob(\"*.csv\"))\n",
    "    for csv_file in all_csvs:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if '@Value' in df.columns and 'SpeciesCode' in df.columns and 'SiteCode' in df.columns:\n",
    "                df_valid = df[pd.to_numeric(df['@Value'], errors='coerce').notnull()].copy()\n",
    "                df_valid['@Value'] = df_valid['@Value'].astype(float)\n",
    "                negatives = df_valid[df_valid['@Value'] < 0]\n",
    "                if not negatives.empty:\n",
    "                    grouped = negatives.groupby(['SpeciesCode', 'SiteCode']).size().reset_index(name='neg_count')\n",
    "                    for _, row in grouped.iterrows():\n",
    "                        total = df_valid[(df_valid['SpeciesCode'] == row['SpeciesCode']) & (df_valid['SiteCode'] == row['SiteCode'])].shape[0]\n",
    "                        percent = (row['neg_count'] / total * 100) if total > 0 else 0\n",
    "                        summary.append({\n",
    "                            'SpeciesCode': row['SpeciesCode'],\n",
    "                            'SiteCode': row['SiteCode'],\n",
    "                            'NegativeCount': row['neg_count'],\n",
    "                            'TotalCount': total,\n",
    "                            'PercentNegative': round(percent, 2),\n",
    "                            'File': str(csv_file)\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process {csv_file}: {e}\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    if not summary_df.empty:\n",
    "        print(\"Negative value summary (by pollutant and site):\")\n",
    "        display(summary_df)\n",
    "    else:\n",
    "        print(\"No negative values found in the scanned files.\")\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecd274c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative values found in the scanned files.\n"
     ]
    }
   ],
   "source": [
    "#  Example usage:\n",
    "neg_summary = find_negative_values(base_dir)\n",
    "neg_summary.to_csv(\"laqn_negative_value_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfdc10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negatives_with_nan(base_dir):\n",
    "    base_dir = Path(base_dir)\n",
    "    for csv_file in base_dir.rglob('*.csv'):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if '@Value' in df.columns:\n",
    "            df['@Value'] = pd.to_numeric(df['@Value'], errors='coerce')\n",
    "            df.loc[df['@Value'] < 0, '@Value'] = np.nan\n",
    "            df.to_csv(csv_file, index=False)\n",
    "        else:\n",
    "            print(f\"Skipped (no @Value column): {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e803925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "replace_negatives_with_nan(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89d21ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_metrics(base_dir, csv_output_path):\n",
    "    \"\"\"\n",
    "    validates laqn measurements against uk air quality objectives with averaging periods.\n",
    "    \n",
    "    - path: data/laqn/optimased/YYYY_month/SiteCode_SpeciesCode_YYYY-MM-DD_YYYY-MM-DD.csv\n",
    "    - columns: @MeasurementDateGMT, @Value, SpeciesCode, SiteCode, SpeciesName, SiteName, SiteType, Latitude, Longitude\n",
    "    \n",
    "    parameters:\n",
    "        base_dir : path to laqn optimased directory (e.g., data/laqn/optimased/)\n",
    "        uk_limits_path : path to uk_pollutant_limits.csv from parsed pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    if not Path(csv_output_path).exists():\n",
    "        print(f\"error: uk limits file not found at {csv_output_path}\")\n",
    "        return {}\n",
    "    \n",
    "    # load uk legal limits from parsed pdf\n",
    "    uk_limits = pd.read_csv(csv_output_path, encoding=\"utf-8\")\n",
    "    uk_limits.columns = [col.strip().replace(' ', '_') for col in uk_limits.columns]\n",
    "    uk_limits_dict = {}\n",
    "    \n",
    "    for _, row in uk_limits.iterrows():\n",
    "        poll_std = row['pollutant_std']\n",
    "        limit_val = row['limit']\n",
    "        conc_type = str(row['concentration_measured_as']).lower().strip()\n",
    "        unit = row['unit']\n",
    "        \n",
    "        if pd.notna(poll_std) and pd.notna(limit_val):\n",
    "            if poll_std not in uk_limits_dict:\n",
    "                uk_limits_dict[poll_std] = []\n",
    "            \n",
    "            # averaging period detection\n",
    "            avg_period = 'unknown'\n",
    "            if 'annual' in conc_type and 'running' in conc_type:\n",
    "                avg_period = 'running_annual'\n",
    "            elif 'running annual' in conc_type:\n",
    "                avg_period = 'running_annual'\n",
    "            elif 'annual' in conc_type:\n",
    "                avg_period = 'annual'\n",
    "            elif '24 hour' in conc_type or '24-hour' in conc_type:\n",
    "                avg_period = '24hour'\n",
    "            elif '8 hour' in conc_type or '8-hour' in conc_type:\n",
    "                avg_period = '8hour'\n",
    "            elif '1 hour' in conc_type or '1-hour' in conc_type or 'hour mean' in conc_type:\n",
    "                avg_period = '1hour'\n",
    "            elif 'maximum daily' in conc_type:\n",
    "                avg_period = 'daily_max'\n",
    "            \n",
    "            uk_limits_dict[poll_std].append({\n",
    "                'limit': float(limit_val),\n",
    "                'type': conc_type,\n",
    "                'unit': unit,\n",
    "                'avg_period': avg_period\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nuk limits loaded for {len(uk_limits_dict)} pollutants:\")\n",
    "    for poll, limits in uk_limits_dict.items():\n",
    "        period_info = ', '.join([f\"{lim['avg_period']}: {lim['limit']}\" for lim in limits])\n",
    "        print(f\"  {poll}: {period_info}\")\n",
    "    \n",
    "    # load all laqn measurement data with timestamp\n",
    "    print(\"\\nloading laqn measurement data...\")\n",
    "    all_data = []\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    # iterate through year-month folders (e.g., 2023_jan, 2024_feb, etc.)\n",
    "    for year_month_dir in base_path.glob('*'):\n",
    "        if year_month_dir.is_dir():\n",
    "            print(f\"processing {year_month_dir.name}...\")\n",
    "            # iterate through csv files (SiteCode_SpeciesCode_YYYY-MM-DD_YYYY-MM-DD.csv)\n",
    "            for csv_file in year_month_dir.glob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    # check for required columns\n",
    "                    required_cols = {'@MeasurementDateGMT', '@Value', 'SpeciesCode'}\n",
    "                    if not required_cols.issubset(df.columns):\n",
    "                        print(f\"skipped {csv_file}: missing required columns. found: {list(df.columns)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if not df.empty:\n",
    "                        all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"error reading {csv_file}: {e}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"error: no measurement data found\")\n",
    "        return {}\n",
    "    \n",
    "    df_all = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"loaded {len(df_all):,} total records\")\n",
    "    \n",
    "    # filter valid values and parse timestamp\n",
    "    df_all['@Value'] = pd.to_numeric(df_all['@Value'], errors='coerce')\n",
    "    df_valid = df_all[df_all['@Value'].notna()].copy()\n",
    "    \n",
    "    # parse timestamp - laqn uses ISO format with timezone\n",
    "    df_valid['@MeasurementDateGMT'] = pd.to_datetime(df_valid['@MeasurementDateGMT'], errors='coerce')\n",
    "    df_valid = df_valid[df_valid['@MeasurementDateGMT'].notna()]\n",
    "    \n",
    "    print(f\"analysing {len(df_valid):,} valid measurements with timestamps\")\n",
    "    \n",
    "    # calculate quality metrics for each pollutant\n",
    "    print(\"\\nprocessing quality metrics by pollutant...\")\n",
    "    quality_results = {}\n",
    "    \n",
    "    for pollutant in df_valid['SpeciesCode'].unique():\n",
    "        if pd.isna(pollutant):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nprocessing {pollutant}...\")\n",
    "        \n",
    "        poll_data = df_valid[df_valid['SpeciesCode'] == pollutant].copy()\n",
    "        \n",
    "        if len(poll_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # basic statistics on raw hourly data\n",
    "        q_metrics = {\n",
    "            'pollutant': pollutant,\n",
    "            'count': int(len(poll_data)),\n",
    "            'mean_hourly': float(poll_data['@Value'].mean()),\n",
    "            'median_hourly': float(poll_data['@Value'].median()),\n",
    "            'std_hourly': float(poll_data['@Value'].std()),\n",
    "            'min': float(poll_data['@Value'].min()),\n",
    "            'max': float(poll_data['@Value'].max()),\n",
    "            'p95': float(poll_data['@Value'].quantile(0.95)),\n",
    "            'p99': float(poll_data['@Value'].quantile(0.99))\n",
    "        }\n",
    "        \n",
    "        # check for suspicious values\n",
    "        negative_count = (poll_data['@Value'] < 0).sum()\n",
    "        zero_count = (poll_data['@Value'] == 0).sum()\n",
    "        \n",
    "        q_metrics['negative_values'] = int(negative_count)\n",
    "        q_metrics['negative_pct'] = float((negative_count / len(poll_data) * 100))\n",
    "        q_metrics['zero_values'] = int(zero_count)\n",
    "        q_metrics['zero_pct'] = float((zero_count / len(poll_data) * 100))\n",
    "        \n",
    "        # laqn uses standard codes (NO2, PM10, PM25, SO2, CO, O3) - direct match to uk limits\n",
    "        # but PM2.5 might appear as PM25 in laqn\n",
    "        poll_std_code = pollutant\n",
    "        if pollutant == 'PM25':\n",
    "            poll_std_code = 'PM25'  # uk limits use PM25\n",
    "        \n",
    "        # check against uk limits with proper averaging\n",
    "        if poll_std_code in uk_limits_dict:\n",
    "            uk_poll_limits = uk_limits_dict[poll_std_code]\n",
    "            \n",
    "            for limit_info in uk_poll_limits:\n",
    "                avg_period = limit_info['avg_period']\n",
    "                limit_value = limit_info['limit']\n",
    "                \n",
    "                if avg_period == 'annual':\n",
    "                    poll_data['year'] = poll_data['@MeasurementDateGMT'].dt.year\n",
    "                    annual_means = poll_data.groupby('year')['@Value'].mean()\n",
    "                    \n",
    "                    q_metrics['uk_annual_limit'] = limit_value\n",
    "                    q_metrics['mean_annual'] = float(annual_means.mean())\n",
    "                    q_metrics['exceeds_uk_annual'] = q_metrics['mean_annual'] > limit_value\n",
    "                    \n",
    "                    print(f\"  annual mean: {q_metrics['mean_annual']:.2f} vs limit {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '24hour':\n",
    "                    poll_data['date'] = poll_data['@MeasurementDateGMT'].dt.date\n",
    "                    daily_means = poll_data.groupby('date')['@Value'].mean()\n",
    "                    \n",
    "                    exceedances = (daily_means > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_24hour_limit'] = limit_value\n",
    "                    q_metrics['daily_exceedances'] = int(exceedances)\n",
    "                    q_metrics['daily_exceedances_pct'] = float((exceedances / len(daily_means) * 100))\n",
    "                    \n",
    "                    print(f\"  24-hour: {exceedances} days exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '8hour':\n",
    "                    poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "                    poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "                    \n",
    "                    exceedances = (poll_data_sorted['rolling_8h'] > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_8hour_limit'] = limit_value\n",
    "                    q_metrics['8hour_exceedances'] = int(exceedances)\n",
    "                    q_metrics['8hour_exceedances_pct'] = float((exceedances / len(poll_data_sorted) * 100))\n",
    "                    \n",
    "                    print(f\"  8-hour: {exceedances} periods exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '1hour':\n",
    "                    exceedances = (poll_data['@Value'] > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_1hour_limit'] = limit_value\n",
    "                    q_metrics['hourly_exceedances'] = int(exceedances)\n",
    "                    q_metrics['hourly_exceedances_pct'] = float((exceedances / len(poll_data) * 100))\n",
    "                    \n",
    "                    print(f\"  1-hour: {exceedances} hours exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == 'running_annual':\n",
    "                    poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "                    poll_data_sorted['rolling_annual'] = poll_data_sorted['@Value'].rolling(window=24*365, min_periods=24*300).mean()\n",
    "                    \n",
    "                    q_metrics['uk_running_annual_limit'] = limit_value\n",
    "                    q_metrics['mean_running_annual'] = float(poll_data_sorted['rolling_annual'].mean())\n",
    "                    q_metrics['exceeds_running_annual'] = q_metrics['mean_running_annual'] > limit_value\n",
    "                    \n",
    "                    print(f\"  running annual: {q_metrics['mean_running_annual']:.2f} vs limit {limit_value}\")\n",
    "                \n",
    "                elif avg_period == 'daily_max':\n",
    "                    poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "                    poll_data_sorted['date'] = poll_data_sorted['@MeasurementDateGMT'].dt.date\n",
    "                    poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "                    \n",
    "                    daily_max = poll_data_sorted.groupby('date')['rolling_8h'].max()\n",
    "                    exceedances = (daily_max > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_daily_max_limit'] = limit_value\n",
    "                    q_metrics['daily_max_exceedances'] = int(exceedances)\n",
    "                    \n",
    "                    print(f\"  daily max 8h: {exceedances} days exceed {limit_value}\")\n",
    "            \n",
    "            # overall assessment: use most restrictive limit for out of range check\n",
    "            all_limits = [lim['limit'] for lim in uk_poll_limits]\n",
    "            max_limit = max(all_limits)\n",
    "            \n",
    "            # define extreme threshold as 10x highest uk limit\n",
    "            extreme_threshold = max_limit * 10\n",
    "            out_of_range = (poll_data['@Value'] > extreme_threshold).sum()\n",
    "            \n",
    "            q_metrics['extreme_threshold'] = extreme_threshold\n",
    "            q_metrics['out_of_range'] = int(out_of_range)\n",
    "            q_metrics['out_of_range_pct'] = float((out_of_range / len(poll_data) * 100))\n",
    "        \n",
    "        else:\n",
    "            print(f\"  no uk limits defined for {poll_std_code}\")\n",
    "            q_metrics['uk_annual_limit'] = None\n",
    "            q_metrics['exceeds_uk_annual'] = False\n",
    "            q_metrics['out_of_range'] = 0\n",
    "            q_metrics['out_of_range_pct'] = 0.0\n",
    "        \n",
    "        # always calculate annual mean\n",
    "        poll_data['year'] = poll_data['@MeasurementDateGMT'].dt.year\n",
    "        annual_means = poll_data.groupby('year')['@Value'].mean()\n",
    "        q_metrics['mean_annual'] = float(annual_means.mean())\n",
    "        \n",
    "        # if annual limit exists, compare\n",
    "        if 'uk_annual_limit' in q_metrics and q_metrics['uk_annual_limit'] is not None:\n",
    "            q_metrics['exceeds_uk_annual'] = q_metrics['mean_annual'] > q_metrics['uk_annual_limit']\n",
    "        else:\n",
    "            q_metrics['exceeds_uk_annual'] = None\n",
    "        \n",
    "        # ozone (O3): count days where max running 8-hour mean > 100 µg/m³\n",
    "        if poll_std_code == 'O3':\n",
    "            poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "            poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "            poll_data_sorted['date'] = poll_data_sorted['@MeasurementDateGMT'].dt.date\n",
    "            daily_max_8h = poll_data_sorted.groupby('date')['rolling_8h'].max()\n",
    "            o3_exceedance_days = (daily_max_8h > 100).sum()\n",
    "            q_metrics['o3_exceedance_days'] = int(o3_exceedance_days)\n",
    "        \n",
    "        # CO: maximum daily running 8-hour mean\n",
    "        if poll_std_code == 'CO':\n",
    "            poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "            poll_data_sorted['date'] = poll_data_sorted['@MeasurementDateGMT'].dt.date\n",
    "            poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "            daily_max_8h = poll_data_sorted.groupby('date')['rolling_8h'].max()\n",
    "            co_max_daily_8h_mean = daily_max_8h.max()\n",
    "            q_metrics['co_max_daily_8h_mean'] = float(co_max_daily_8h_mean)\n",
    "        \n",
    "        quality_results[pollutant] = q_metrics\n",
    "    \n",
    "    return quality_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d40c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quality_metrics(quality_results):\n",
    "    \"\"\"\n",
    "    print comprehensive quality metrics report with uk compliance for laqn data.\n",
    "    \n",
    "    parameters:\n",
    "        quality_results : dict\n",
    "            dictionary returned by calculate_quality_metrics_laqn\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"laqn quality metrics report\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for poll, metrics in quality_results.items():\n",
    "        print(f\"\\n{poll}:\")\n",
    "        print(f\"  total measurements: {metrics['count']:,}\")\n",
    "        print(f\"  hourly mean: {metrics['mean_hourly']:.2f}\")\n",
    "        \n",
    "        if 'mean_annual' in metrics:\n",
    "            print(f\"  annual mean: {metrics['mean_annual']:.2f}\", end=\"\")\n",
    "            if 'uk_annual_limit' in metrics and metrics['uk_annual_limit'] is not None:\n",
    "                print(f\" (limit: {metrics['uk_annual_limit']})\")\n",
    "                status = \"exceeds\" if metrics['exceeds_uk_annual'] else \"compliant\"\n",
    "                print(f\"    status: {status}\")\n",
    "            else:\n",
    "                print(\" (no uk annual limit)\")\n",
    "        \n",
    "        if 'o3_exceedance_days' in metrics:\n",
    "            print(f\"  O3 8-hour mean exceedance days: {metrics['o3_exceedance_days']}\")\n",
    "        if 'co_max_daily_8h_mean' in metrics:\n",
    "            print(f\"  CO max daily 8-hour mean: {metrics['co_max_daily_8h_mean']:.2f}\")\n",
    "        \n",
    "        if 'daily_exceedances' in metrics:\n",
    "            print(f\"  24-hour exceedances: {metrics['daily_exceedances']} days\")\n",
    "        if 'hourly_exceedances' in metrics:\n",
    "            print(f\"  1-hour exceedances: {metrics['hourly_exceedances']} hours\")\n",
    "        if metrics['negative_values'] > 0:\n",
    "            print(f\"  warning: {metrics['negative_values']} negative values\")\n",
    "        if metrics['out_of_range'] > 0:\n",
    "            print(f\"  warning: {metrics['out_of_range']} extreme values\")\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c9eb27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "uk limits loaded for 11 pollutants:\n",
      "  PM10: 24hour: 50.0, annual: 40.0\n",
      "  PM2.5: annual: 20.0\n",
      "  NO2: annual: 40.0\n",
      "  O3: 8hour: 100.0\n",
      "  SO2: 24hour: 125.0\n",
      "  PAH: annual: 0.25\n",
      "  Benzene: running_annual: 16.25\n",
      "  1,3-butadiene: running_annual: 2.25\n",
      "  CO: daily_max: 10.0\n",
      "  LEAD: annual: 0.5\n",
      "  NOx: annual: 30.0\n",
      "\n",
      "loading laqn measurement data...\n",
      "processing 2023_mar...\n",
      "processing 2025_feb...\n",
      "processing 2024_feb...\n",
      "processing 2025_aug...\n",
      "processing 2024_aug...\n",
      "processing 2025_mar...\n",
      "processing 2023_feb...\n",
      "processing 2024_mar...\n",
      "processing 2023_aug...\n",
      "processing 2024_jul...\n",
      "processing 2025_jul...\n",
      "processing 2024_oct...\n",
      "processing 2023_sep...\n",
      "processing 2025_oct...\n",
      "processing 2023_jan...\n",
      "processing 2023_jul...\n",
      "processing 2024_jan...\n",
      "processing 2025_sep...\n",
      "processing 2024_sep...\n",
      "processing 2023_oct...\n",
      "processing 2025_jan...\n",
      "processing 2024_dec...\n",
      "processing 2024_apr...\n",
      "processing 2024_nov...\n",
      "processing 2023_may...\n",
      "processing 2025_nov...\n",
      "processing 2025_apr...\n",
      "processing 2024_may...\n",
      "processing 2025_may...\n",
      "processing 2023_nov...\n",
      "processing 2023_apr...\n",
      "processing 2023_dec...\n",
      "processing report...\n",
      "processing 2025_jun...\n",
      "processing 2024_jun...\n",
      "processing 2023_jun...\n",
      "loaded 3,446,208 total records\n",
      "analysing 2,981,417 valid measurements with timestamps\n",
      "\n",
      "processing quality metrics by pollutant...\n",
      "\n",
      "processing NO2...\n",
      "  annual mean: 23.30 vs limit 40.0\n",
      "\n",
      "processing PM2.5...\n",
      "  annual mean: 9.29 vs limit 20.0\n",
      "\n",
      "processing PM10...\n",
      "  24-hour: 3 days exceed 50.0\n",
      "  annual mean: 17.32 vs limit 40.0\n",
      "\n",
      "processing SO2...\n",
      "  24-hour: 0 days exceed 125.0\n",
      "\n",
      "processing O3...\n",
      "  8-hour: 2741 periods exceed 100.0\n",
      "\n",
      "processing CO...\n",
      "  daily max 8h: 0 days exceed 10.0\n",
      "\n",
      "========================================\n",
      "laqn quality metrics report\n",
      "========================================\n",
      "\n",
      "NO2:\n",
      "  total measurements: 1,267,018\n",
      "  hourly mean: 23.34\n",
      "  annual mean: 23.30 (limit: 40.0)\n",
      "    status: compliant\n",
      "\n",
      "PM2.5:\n",
      "  total measurements: 482,559\n",
      "  hourly mean: 9.25\n",
      "  annual mean: 9.29 (limit: 20.0)\n",
      "    status: compliant\n",
      "  warning: 11 extreme values\n",
      "\n",
      "PM10:\n",
      "  total measurements: 896,146\n",
      "  hourly mean: 17.23\n",
      "  annual mean: 17.32 (limit: 40.0)\n",
      "    status: compliant\n",
      "  24-hour exceedances: 3 days\n",
      "  warning: 3 extreme values\n",
      "\n",
      "SO2:\n",
      "  total measurements: 71,217\n",
      "  hourly mean: 1.47\n",
      "  annual mean: 1.52 (no uk annual limit)\n",
      "  24-hour exceedances: 0 days\n",
      "\n",
      "O3:\n",
      "  total measurements: 220,912\n",
      "  hourly mean: 47.71\n",
      "  annual mean: 47.80 (no uk annual limit)\n",
      "  O3 8-hour mean exceedance days: 70\n",
      "\n",
      "CO:\n",
      "  total measurements: 43,565\n",
      "  hourly mean: 0.20\n",
      "  annual mean: 0.20 (no uk annual limit)\n",
      "  CO max daily 8-hour mean: 2.14\n",
      "========================================\n",
      "\n",
      "saving laqn quality metrics report...\n",
      "saved to: {'NO2': {'pollutant': 'NO2', 'count': 1267018, 'mean_hourly': 23.33811129755063, 'median_hourly': 19.0, 'std_hourly': 17.169706453593633, 'min': 0.0, 'max': 376.3, 'p95': 57.1, 'p99': 78.3, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 632, 'zero_pct': 0.049880901455227944, 'uk_annual_limit': 40.0, 'mean_annual': 23.296558581709963, 'exceeds_uk_annual': False, 'extreme_threshold': 400.0, 'out_of_range': 0, 'out_of_range_pct': 0.0}, 'PM2.5': {'pollutant': 'PM2.5', 'count': 482559, 'mean_hourly': 9.249439550396948, 'median_hourly': 7.0, 'std_hourly': 7.869002875250606, 'min': 0.0, 'max': 909.0, 'p95': 23.9, 'p99': 38.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2090, 'zero_pct': 0.4331076614465796, 'uk_annual_limit': 20.0, 'mean_annual': 9.287265567932431, 'exceeds_uk_annual': False, 'extreme_threshold': 200.0, 'out_of_range': 11, 'out_of_range_pct': 0.0022795140076135767}, 'PM10': {'pollutant': 'PM10', 'count': 896146, 'mean_hourly': 17.23079754861373, 'median_hourly': 14.4, 'std_hourly': 12.470894582830518, 'min': 0.0, 'max': 759.0, 'p95': 39.0, 'p99': 61.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 835, 'zero_pct': 0.09317678146194928, 'uk_24hour_limit': 50.0, 'daily_exceedances': 3, 'daily_exceedances_pct': 0.2944062806673209, 'uk_annual_limit': 40.0, 'mean_annual': 17.3172696741254, 'exceeds_uk_annual': False, 'extreme_threshold': 500.0, 'out_of_range': 3, 'out_of_range_pct': 0.0003347668795040094}, 'SO2': {'pollutant': 'SO2', 'count': 71217, 'mean_hourly': 1.46688431133016, 'median_hourly': 1.1, 'std_hourly': 3.2138526496443744, 'min': 0.0, 'max': 271.4, 'p95': 4.2, 'p99': 7.2, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2348, 'zero_pct': 3.296965612143168, 'uk_24hour_limit': 125.0, 'daily_exceedances': 0, 'daily_exceedances_pct': 0.0, 'extreme_threshold': 1250.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 1.5215136717118256, 'exceeds_uk_annual': None}, 'O3': {'pollutant': 'O3', 'count': 220912, 'mean_hourly': 47.70532836604621, 'median_hourly': 48.1, 'std_hourly': 23.953550068547706, 'min': 0.0, 'max': 198.6, 'p95': 85.6, 'p99': 108.68899999999849, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 260, 'zero_pct': 0.11769392337220251, 'uk_8hour_limit': 100.0, '8hour_exceedances': 2741, '8hour_exceedances_pct': 1.2407655537046427, 'extreme_threshold': 1000.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 47.798197519325676, 'exceeds_uk_annual': None, 'o3_exceedance_days': 70}, 'CO': {'pollutant': 'CO', 'count': 43565, 'mean_hourly': 0.20249512223114888, 'median_hourly': 0.2, 'std_hourly': 0.18651998992979038, 'min': 0.0, 'max': 4.9, 'p95': 0.5, 'p99': 0.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 6773, 'zero_pct': 15.546883966486858, 'uk_daily_max_limit': 10.0, 'daily_max_exceedances': 0, 'extreme_threshold': 100.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 0.2006708313657066, 'exceeds_uk_annual': None, 'co_max_daily_8h_mean': 2.1374999999999997}}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# calculate quality metrics\n",
    "quality_results = calculate_quality_metrics(base_dir, csv_output_path)\n",
    "\n",
    "print_quality_metrics(quality_results)\n",
    "\n",
    "if quality_results:\n",
    "    # save comprehensive report\n",
    "    print(\"\\nsaving laqn quality metrics report...\")\n",
    "    \n",
    "    quality_rows = []\n",
    "    for poll, metrics in quality_results.items():\n",
    "        row = {\n",
    "            'pollutant': metrics['pollutant'],\n",
    "            'total_measurements': metrics['count'],\n",
    "            'mean_hourly': f\"{metrics['mean_hourly']:.2f}\",\n",
    "            'min': f\"{metrics['min']:.2f}\",\n",
    "            'max': f\"{metrics['max']:.2f}\",\n",
    "            'p95': f\"{metrics['p95']:.2f}\",\n",
    "            'negative_values': metrics['negative_values'],\n",
    "            'zero_values': metrics['zero_values'],\n",
    "            'out_of_range': metrics['out_of_range']\n",
    "        }\n",
    "        \n",
    "        # add uk limit compliance fields\n",
    "        if 'uk_annual_limit' in metrics and metrics['uk_annual_limit']:\n",
    "            row['uk_annual_limit'] = metrics['uk_annual_limit']\n",
    "            row['mean_annual'] = f\"{metrics['mean_annual']:.2f}\" if 'mean_annual' in metrics else 'n/a'\n",
    "            row['exceeds_annual'] = 'yes' if metrics.get('exceeds_uk_annual', False) else 'no'\n",
    "        \n",
    "        if 'daily_exceedances' in metrics:\n",
    "            row['uk_24hour_limit'] = metrics['uk_24hour_limit']\n",
    "            row['daily_exceedances'] = metrics['daily_exceedances']\n",
    "        \n",
    "        if 'hourly_exceedances' in metrics:\n",
    "            row['uk_1hour_limit'] = metrics['uk_1hour_limit']\n",
    "            row['hourly_exceedances'] = metrics['hourly_exceedances']\n",
    "        \n",
    "        if 'o3_exceedance_days' in metrics:\n",
    "            row['o3_exceedance_days'] = metrics['o3_exceedance_days']\n",
    "        \n",
    "        if 'co_max_daily_8h_mean' in metrics:\n",
    "            row['co_max_daily_8h_mean'] = f\"{metrics['co_max_daily_8h_mean']:.2f}\"\n",
    "        \n",
    "        quality_rows.append(row)\n",
    "    \n",
    "    #ssave to quality metrics csv\n",
    "    pd.DataFrame(quality_rows).to_csv(\"quality_metrics.csv\", index=False)\n",
    "    print(f\"saved to: {quality_results}\")\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"laqn quality metrics calculation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f20fa",
   "metadata": {},
   "source": [
    "    uk limits loaded for 11 pollutants:\n",
    "    PM10: 24hour: 50.0, annual: 40.0\n",
    "    PM2.5: annual: 20.0\n",
    "    NO2: annual: 40.0\n",
    "    O3: 8hour: 100.0\n",
    "    SO2: 24hour: 125.0\n",
    "    PAH: annual: 0.25\n",
    "    Benzene: running_annual: 16.25\n",
    "    1,3-butadiene: running_annual: 2.25\n",
    "    CO: daily_max: 10.0\n",
    "    LEAD: annual: 0.5\n",
    "    NOx: annual: 30.0\n",
    "\n",
    "    loading laqn measurement data...\n",
    "    processing 2023_mar...\n",
    "    processing 2025_feb...\n",
    "    processing 2024_feb...\n",
    "    processing 2025_aug...\n",
    "    processing 2024_aug...\n",
    "    processing 2025_mar...\n",
    "    processing 2023_feb...\n",
    "    processing 2024_mar...\n",
    "    processing 2023_aug...\n",
    "    processing 2024_jul...\n",
    "    processing 2025_jul...\n",
    "    processing 2024_oct...\n",
    "    processing 2023_sep...\n",
    "    processing 2025_oct...\n",
    "    processing 2023_jan...\n",
    "    processing 2023_jul...\n",
    "    processing 2024_jan...\n",
    "    processing 2025_sep...\n",
    "    processing 2024_sep...\n",
    "    processing 2023_oct...\n",
    "    processing 2025_jan...\n",
    "    processing 2024_dec...\n",
    "    processing 2024_apr...\n",
    "    processing 2024_nov...\n",
    "    processing 2023_may...\n",
    "    processing 2025_nov...\n",
    "    processing 2025_apr...\n",
    "    processing 2024_may...\n",
    "    processing 2025_may...\n",
    "    processing 2023_nov...\n",
    "    processing 2023_apr...\n",
    "    processing 2023_dec...\n",
    "    processing report...\n",
    "    processing 2025_jun...\n",
    "    processing 2024_jun...\n",
    "    processing 2023_jun...\n",
    "    loaded 3,446,208 total records\n",
    "    analysing 2,981,417 valid measurements with timestamps\n",
    "\n",
    "    processing quality metrics by pollutant...\n",
    "\n",
    "    processing NO2...\n",
    "    annual mean: 23.30 vs limit 40.0\n",
    "\n",
    "    processing PM2.5...\n",
    "    annual mean: 9.29 vs limit 20.0\n",
    "\n",
    "    processing PM10...\n",
    "    24-hour: 3 days exceed 50.0\n",
    "    annual mean: 17.32 vs limit 40.0\n",
    "\n",
    "    processing SO2...\n",
    "    24-hour: 0 days exceed 125.0\n",
    "\n",
    "    processing O3...\n",
    "    8-hour: 2741 periods exceed 100.0\n",
    "\n",
    "    processing CO...\n",
    "    daily max 8h: 0 days exceed 10.0\n",
    "\n",
    "    ========================================\n",
    "    laqn quality metrics report\n",
    "    ========================================\n",
    "\n",
    "    NO2:\n",
    "    total measurements: 1,267,018\n",
    "    hourly mean: 23.34\n",
    "    annual mean: 23.30 (limit: 40.0)\n",
    "        status: compliant\n",
    "\n",
    "    PM2.5:\n",
    "    total measurements: 482,559\n",
    "    hourly mean: 9.25\n",
    "    annual mean: 9.29 (limit: 20.0)\n",
    "        status: compliant\n",
    "    warning: 11 extreme values\n",
    "\n",
    "    PM10:\n",
    "    total measurements: 896,146\n",
    "    hourly mean: 17.23\n",
    "    annual mean: 17.32 (limit: 40.0)\n",
    "        status: compliant\n",
    "    24-hour exceedances: 3 days\n",
    "    warning: 3 extreme values\n",
    "\n",
    "    SO2:\n",
    "    total measurements: 71,217\n",
    "    hourly mean: 1.47\n",
    "    annual mean: 1.52 (no uk annual limit)\n",
    "    24-hour exceedances: 0 days\n",
    "\n",
    "    O3:\n",
    "    total measurements: 220,912\n",
    "    hourly mean: 47.71\n",
    "    annual mean: 47.80 (no uk annual limit)\n",
    "    O3 8-hour mean exceedance days: 70\n",
    "\n",
    "    CO:\n",
    "    total measurements: 43,565\n",
    "    hourly mean: 0.20\n",
    "    annual mean: 0.20 (no uk annual limit)\n",
    "    CO max daily 8-hour mean: 2.14\n",
    "    ========================================\n",
    "\n",
    "    saving laqn quality metrics report...\n",
    "    saved to: {'NO2': {'pollutant': 'NO2', 'count': 1267018, 'mean_hourly': 23.33811129755063, 'median_hourly': 19.0, 'std_hourly': 17.169706453593633, 'min': 0.0, 'max': 376.3, 'p95': 57.1, 'p99': 78.3, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 632, 'zero_pct': 0.049880901455227944, 'uk_annual_limit': 40.0, 'mean_annual': 23.296558581709963, 'exceeds_uk_annual': False, 'extreme_threshold': 400.0, 'out_of_range': 0, 'out_of_range_pct': 0.0}, 'PM2.5': {'pollutant': 'PM2.5', 'count': 482559, 'mean_hourly': 9.249439550396948, 'median_hourly': 7.0, 'std_hourly': 7.869002875250606, 'min': 0.0, 'max': 909.0, 'p95': 23.9, 'p99': 38.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2090, 'zero_pct': 0.4331076614465796, 'uk_annual_limit': 20.0, 'mean_annual': 9.287265567932431, 'exceeds_uk_annual': False, 'extreme_threshold': 200.0, 'out_of_range': 11, 'out_of_range_pct': 0.0022795140076135767}, 'PM10': {'pollutant': 'PM10', 'count': 896146, 'mean_hourly': 17.23079754861373, 'median_hourly': 14.4, 'std_hourly': 12.470894582830518, 'min': 0.0, 'max': 759.0, 'p95': 39.0, 'p99': 61.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 835, 'zero_pct': 0.09317678146194928, 'uk_24hour_limit': 50.0, 'daily_exceedances': 3, 'daily_exceedances_pct': 0.2944062806673209, 'uk_annual_limit': 40.0, 'mean_annual': 17.3172696741254, 'exceeds_uk_annual': False, 'extreme_threshold': 500.0, 'out_of_range': 3, 'out_of_range_pct': 0.0003347668795040094}, 'SO2': {'pollutant': 'SO2', 'count': 71217, 'mean_hourly': 1.46688431133016, 'median_hourly': 1.1, 'std_hourly': 3.2138526496443744, 'min': 0.0, 'max': 271.4, 'p95': 4.2, 'p99': 7.2, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2348, 'zero_pct': 3.296965612143168, 'uk_24hour_limit': 125.0, 'daily_exceedances': 0, 'daily_exceedances_pct': 0.0, 'extreme_threshold': 1250.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 1.5215136717118256, 'exceeds_uk_annual': None}, 'O3': {'pollutant': 'O3', 'count': 220912, 'mean_hourly': 47.70532836604621, 'median_hourly': 48.1, 'std_hourly': 23.953550068547706, 'min': 0.0, 'max': 198.6, 'p95': 85.6, 'p99': 108.68899999999849, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 260, 'zero_pct': 0.11769392337220251, 'uk_8hour_limit': 100.0, '8hour_exceedances': 2741, '8hour_exceedances_pct': 1.2407655537046427, 'extreme_threshold': 1000.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 47.798197519325676, 'exceeds_uk_annual': None, 'o3_exceedance_days': 70}, 'CO': {'pollutant': 'CO', 'count': 43565, 'mean_hourly': 0.20249512223114888, 'median_hourly': 0.2, 'std_hourly': 0.18651998992979038, 'min': 0.0, 'max': 4.9, 'p95': 0.5, 'p99': 0.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 6773, 'zero_pct': 15.546883966486858, 'uk_daily_max_limit': 10.0, 'daily_max_exceedances': 0, 'extreme_threshold': 100.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 0.2006708313657066, 'exceeds_uk_annual': None, 'co_max_daily_8h_mean': 2.1374999999999997}}\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed540e1",
   "metadata": {},
   "source": [
    "## 5) Chi-Square Test for LAQN Data Quality\n",
    "\n",
    "Uses statistical tests to mathematically prove that LAQN data collection process was consistent and reliable across time. \n",
    "It serves as a quality control check that ensures we didn't accidentally collect more data in some months than others, which could bias our analysis.\n",
    "\n",
    "#### Why Chi-Square Test?\n",
    " - The chi-square test answers one simple question: Are my monthly file counts similar enough to trust, or are some months suspiciously different?\n",
    " - Air pollution varies by season\n",
    " - Policy decisions need unbiased evidence\n",
    " - Academic reviewers will question imbalanced datasets\n",
    "\n",
    "### What Chi-Square Test Does\n",
    "\n",
    "The chi-square test answers one simple question: Are my monthly file counts similar enough to trust, or are some months suspiciously different?\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. What we observe: Count how many data files we have for each month.\n",
    "2. What we expect: If data collection was perfect, each month should have roughly the same count.\n",
    "3. The test: Measures how far observed counts are from the expected counts.\n",
    "4. The result: Gives us a p-value that tells us if the differences are just random variation or a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e36dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
