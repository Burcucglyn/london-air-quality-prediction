{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccfa1de",
   "metadata": {},
   "source": [
    "# LAQN Dataset Assesment\n",
    "\n",
    "\n",
    "1) I'll be start adding my main paths and modules I will be using in this notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfa1335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible python modules i will be using below\n",
    "from curses import meta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "#function 7 importing the full analysis function from pollution_analysis\n",
    "import sys\n",
    "sys.path.append('/mnt/user-data/outputs')\n",
    "\n",
    "#last detailed anlasye and visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualisation style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "#findings 7 .func\n",
    "# for parse pdf uk pollutant limitations to csv\n",
    "import re\n",
    "# pdfplumber for pdf parsing\n",
    "\n",
    "# function 5. chi-square test\n",
    "from scipy import stats\n",
    "\n",
    "#define base path  without hardcoding\n",
    "base_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"optimased\"\n",
    "#metadata file for pollutant name, location and site names\n",
    "metadata_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"optimased_siteSpecies.csv\"\n",
    "\n",
    "# output path for saving statistics 1. function\n",
    "#the first analyse dataset created without inclitiong nan optimased files, and cross referencing that's why changed the name to dataset_statistics-noNAN-incl.csv\n",
    "os.makedirs(base_dir / \"report\", exist_ok=True)\n",
    "stats_output_path =  Path.home()/\"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"report\"/ \"laqn_stats.csv\"\n",
    "\n",
    "# output paths for saving all the pollutant distribution and nan value analysis.\n",
    "pollutant_distrubution_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\"/\"pollutant_distribution.csv\"\n",
    "nan_val_pollutant_split_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_pollutant.csv\"\n",
    "nan_val_stationPollutant_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_station_pollutant.csv\"\n",
    "\n",
    "\n",
    "# log file from nan replacement process\n",
    "nan_log_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"logs\" / \"NaN_values_record.csv\"\n",
    "\n",
    "# function for uk pollutant regulations pdf to parse csv file path\n",
    "csv_output_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"capabilities\" / \"uk_pollutant_limits.csv\"\n",
    "\n",
    "\n",
    "# data quality metrics report output path\n",
    "quality_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\"/ \"report\" / \"quality_metrics_validation.csv\"\n",
    "quality_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#chi-square test output path func 5\n",
    "chi_square_output1 = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests1.csv\"\n",
    "chi_square_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests.csv\"\n",
    "\n",
    "# detailed last analysis and visualization output directory\n",
    "report_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"detailed_analysis\"\n",
    "\n",
    "report_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d9baf",
   "metadata": {},
   "source": [
    "## 1) Initial Dataset Assessment: Raw Numbers\n",
    "\n",
    "Before conducting quality checks, I need to establish the baseline characteristics of the LAQN dataset. This section calculates comprehensive statistics about the data collection effort, including file counts, measurement records, station coverage, and pollutant distribution.\n",
    "\n",
    "### Purpose\n",
    "- Document the scale and scope of data collection.\n",
    "- Establish baseline metrics for comparison with LAQN.\n",
    "- Provide context for subsequent quality analysis.\n",
    "\n",
    "### Methodology\n",
    "The function `get_laqn_dataset_statistics()` performs the following:\n",
    "1. Loads standardised metadata to identify unique stations and pollutants.\n",
    "2. Counts files across all three yearly directories (2023, 2024, 2025).\n",
    "3. Calculates total measurement records by reading all CSV files.\n",
    "4. Determines spatial coverage from unique coordinate pairs.\n",
    "5. Documents temporal coverage (35 months: January 2023 to November 2025).\n",
    "\n",
    "### Notes\n",
    "- File counting is fast (scans directory structure only).\n",
    "- Record counting can be slow (reads every CSV file).\n",
    "- Results are saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d680dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path):\n",
    "    \"\"\"\n",
    "    Calculate statistics for the LAQN dataset using the new column structure.\n",
    "    This function scans all CSV files recursively under base_dir and calculates key metrics needed for reporting.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir : Path\n",
    "            Base directory containing LAQN data folders.\n",
    "        metadata_path : Path\n",
    "            Path to the standardised metadata csv file.\n",
    "        nan_log_path : Path\n",
    "            Path to the NaN values log file after notice data flags, changed them to NaN.\n",
    "\n",
    "    Returns:\n",
    "        dict : Dictionary containing all calculated statistics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # read metadata to get station and pollutant info\n",
    "    print(\"\\nloading metadata...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "\n",
    "    # calculate metadata statistics\n",
    "    stats['unique_stations'] = metadata['SiteCode'].nunique()\n",
    "    stats['total_combinations'] = len(metadata)\n",
    "    stats['unique_pollutants'] = metadata['SpeciesCode'].nunique()\n",
    "\n",
    "    # get pollutant breakdown\n",
    "    pollutant_counts = metadata['SpeciesCode'].value_counts()\n",
    "    stats['pollutant_distribution'] = pollutant_counts.to_dict()\n",
    "\n",
    "    # create set of expected (SiteCode, SpeciesCode) pairs from metadata\n",
    "    expected_pairs = set(\n",
    "        zip(metadata['SiteCode'], metadata['SpeciesCode'])\n",
    "    )\n",
    "    stats['expected_pairs'] = len(expected_pairs)\n",
    "    print(f\"  expected SiteCode/SpeciesCode pairs from metadata: {len(expected_pairs)}\")\n",
    "\n",
    "    # count unique coordinates for spatial coverage\n",
    "    unique_coords = metadata[['Latitude', 'Longitude']].drop_duplicates()\n",
    "    stats['unique_locations'] = len(unique_coords)\n",
    "\n",
    "    # Scan all CSVs in all subfolders under base_dir\n",
    "    print(\"\\nscanning optimased directory for collected data...\")\n",
    "    all_csv_files = list(Path(base_dir).rglob('*.csv'))\n",
    "    total_files = len(all_csv_files)\n",
    "    print(f\"\\nTotal CSV files found: {total_files}\")\n",
    "    stats['total_files'] = total_files\n",
    "\n",
    "    # Count files, records, and missing values by period (e.g., \"2023_apr\")\n",
    "    files_by_period = defaultdict(int)\n",
    "    records_by_period = defaultdict(int)\n",
    "    missing_by_period = defaultdict(int)\n",
    "\n",
    "    all_csvs = []\n",
    "    total_records = 0\n",
    "    total_missing = 0\n",
    "\n",
    "    print(\"\\nReading all CSV files to calculate statistics...\")\n",
    "    for csv_file in all_csv_files:\n",
    "        period = csv_file.parent.name\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            n_records = len(df)\n",
    "            n_missing = df['@Value'].isna().sum() + (df['@Value'] == \"\").sum() if '@Value' in df.columns else 0\n",
    "            all_csvs.append(df)\n",
    "            files_by_period[period] += 1\n",
    "            records_by_period[period] += n_records\n",
    "            missing_by_period[period] += n_missing\n",
    "            total_records += n_records\n",
    "            total_missing += n_missing\n",
    "        except Exception as e:\n",
    "            print(f\"  warning: could not read {csv_file.name}: {e}\")\n",
    "\n",
    "    stats['files_by_period'] = dict(files_by_period)\n",
    "    stats['records_by_period'] = dict(records_by_period)\n",
    "    stats['missing_by_period'] = dict(missing_by_period)\n",
    "    stats['total_records'] = total_records\n",
    "    stats['total_missing'] = total_missing\n",
    "    stats['overall_completeness'] = ((total_records - total_missing) / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "    for period in files_by_period:\n",
    "        rec = records_by_period[period]\n",
    "        miss = missing_by_period[period]\n",
    "        miss_pct = (miss / rec * 100) if rec > 0 else 0\n",
    "        print(f\"  {period}: {files_by_period[period]} files, {rec:,} records, {miss:,} missing ({miss_pct:.2f}%)\")\n",
    "\n",
    "    # cross-reference metadata with collected data\n",
    "    print(\"\\ncross-referencing collected data with metadata...\")\n",
    "\n",
    "    if all_csvs:\n",
    "        all_data = pd.concat(all_csvs, ignore_index=True)\n",
    "\n",
    "        # check if required columns exist in csv files\n",
    "        if 'SiteCode' in all_data.columns and 'SpeciesCode' in all_data.columns:\n",
    "            # identify actual (SiteCode, SpeciesCode) pairs in collected data\n",
    "            collected_pairs = set(\n",
    "                zip(all_data['SiteCode'], all_data['SpeciesCode'])\n",
    "            )\n",
    "            stats['collected_pairs'] = len(collected_pairs)\n",
    "\n",
    "            # find missing pairs (in metadata but not in collected data)\n",
    "            missing_pairs = expected_pairs - collected_pairs\n",
    "            stats['missing_pairs'] = list(missing_pairs)\n",
    "            stats['missing_pairs_count'] = len(missing_pairs)\n",
    "\n",
    "            # find extra pairs (in collected data but not in metadata)\n",
    "            extra_pairs = collected_pairs - expected_pairs\n",
    "            stats['extra_pairs'] = list(extra_pairs)\n",
    "            stats['extra_pairs_count'] = len(extra_pairs)\n",
    "\n",
    "            print(f\"  expected pairs from metadata: {len(expected_pairs)}\")\n",
    "            print(f\"  actually collected pairs: {len(collected_pairs)}\")\n",
    "            print(f\"  missing pairs (in metadata but not collected): {len(missing_pairs)}\")\n",
    "            print(f\"  extra pairs (collected but not in metadata): {len(extra_pairs)}\")\n",
    "\n",
    "            # group by SiteCode and SpeciesCode, count missing values\n",
    "            missing_breakdown = {}\n",
    "            for (site, species), group in all_data.groupby(['SiteCode', 'SpeciesCode']):\n",
    "                total_rows = len(group)\n",
    "                if '@Value' in group.columns:\n",
    "                    missing_rows = group['@Value'].isna().sum() + (group['@Value'] == \"\").sum()\n",
    "                else:\n",
    "                    missing_rows = 0\n",
    "                missing_breakdown[(site, species)] = (int(missing_rows), int(total_rows))\n",
    "            stats['missing_by_station_pollutant'] = missing_breakdown\n",
    "        else:\n",
    "            print(\"  warning: SiteCode or SpeciesCode columns not found\")\n",
    "            stats['missing_by_station_pollutant'] = {}\n",
    "            stats['collected_pairs'] = 0\n",
    "            stats['missing_pairs'] = []\n",
    "            stats['missing_pairs_count'] = 0\n",
    "            stats['extra_pairs'] = []\n",
    "            stats['extra_pairs_count'] = 0\n",
    "    else:\n",
    "        stats['missing_by_station_pollutant'] = {}\n",
    "        stats['collected_pairs'] = 0\n",
    "        stats['missing_pairs'] = list(expected_pairs)\n",
    "        stats['missing_pairs_count'] = len(expected_pairs)\n",
    "        stats['extra_pairs'] = []\n",
    "        stats['extra_pairs_count'] = 0\n",
    "\n",
    "    # distribution of nan by pollutant over time\n",
    "    if stats['missing_by_station_pollutant']:\n",
    "        pollutant_missing_summary = {}\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            if species not in pollutant_missing_summary:\n",
    "                pollutant_missing_summary[species] = {'total_missing': 0, 'total_records': 0}\n",
    "            pollutant_missing_summary[species]['total_missing'] += missing\n",
    "            pollutant_missing_summary[species]['total_records'] += total\n",
    "        for species in pollutant_missing_summary:\n",
    "            total_missing = pollutant_missing_summary[species]['total_missing']\n",
    "            total_records = pollutant_missing_summary[species]['total_records']\n",
    "            percentage = (total_missing / total_records * 100) if total_records > 0 else 0\n",
    "            pollutant_missing_summary[species]['percentage_missing'] = percentage\n",
    "        stats['missing_by_pollutant_type'] = pollutant_missing_summary\n",
    "    else:\n",
    "        stats['missing_by_pollutant_type'] = {}\n",
    "\n",
    "    # log file created during data cleaning process\n",
    "    if Path(nan_log_path).exists():\n",
    "        nan_log = pd.read_csv(nan_log_path)\n",
    "        replacements_by_year = nan_log.groupby('year_folder')['invalid_flags_replaced'].sum().to_dict()\n",
    "        stats['nan_replacements_by_year'] = replacements_by_year\n",
    "        stats['total_nan_replacements'] = nan_log['invalid_flags_replaced'].sum()\n",
    "        stats['mean_invalid_percentage'] = nan_log['percentage_invalid'].mean()\n",
    "        stats['max_invalid_percentage'] = nan_log['percentage_invalid'].max()\n",
    "    else:\n",
    "        stats['nan_replacements_by_year'] = {}\n",
    "        stats['total_nan_replacements'] = 0\n",
    "        stats['mean_invalid_percentage'] = 0\n",
    "        stats['max_invalid_percentage'] = 0\n",
    "\n",
    "    # calculate temporal coverage based on the files collected\n",
    "    stats['temporal_coverage'] = {\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2025-11-19',\n",
    "        'total_months': 35\n",
    "    }\n",
    "\n",
    "\n",
    "    def extract_year(period):\n",
    "        # period is usually like '2023_apr' or '2024_jan'\n",
    "        return str(period)[:4] if len(str(period)) >= 4 and str(period)[:4].isdigit() else 'unknown'\n",
    "\n",
    "    files_by_year = defaultdict(int)\n",
    "    records_by_year = defaultdict(int)\n",
    "    missing_by_year = defaultdict(int)\n",
    "    for period in files_by_period:\n",
    "        year = extract_year(period)\n",
    "        files_by_year[year] += files_by_period[period]\n",
    "        records_by_year[year] += records_by_period[period]\n",
    "        missing_by_year[year] += missing_by_period[period]\n",
    "    stats['files_by_year'] = dict(files_by_year)\n",
    "    stats['records_by_year'] = dict(records_by_year)\n",
    "    stats['missing_by_year'] = dict(missing_by_year)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e2a9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(stats):\n",
    "    \"\"\"\n",
    "    Print dataset statistics for LAQN using new column structure.\n",
    "\n",
    "    Parameters:\n",
    "        stats : dict\n",
    "            returned by get_laqn_dataset_statistics().\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"LAQN dataset statistics: initial assessment\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    print(\"\\nScale and scope:\")\n",
    "    print(f\"Total files collected: {stats['total_files']:,}\")\n",
    "    print(f\"Total measurement records: {stats['total_records']:,}\")\n",
    "    print(f\"Total missing values (@Value): {stats['total_missing']:,}\")\n",
    "    print(f\"Overall completeness: {stats['overall_completeness']:.2f}%\")\n",
    "    print(f\"Unique monitoring sites (SiteCode): {stats['unique_stations']}\")\n",
    "    print(f\"Total site-species combinations: {stats['total_combinations']}\")\n",
    "    print(f\"Unique pollutant types (SpeciesCode): {stats['unique_pollutants']}\")\n",
    "    print(f\"Unique geographic locations: {stats['unique_locations']}\")\n",
    "\n",
    "    # data collection coverage\n",
    "    print(\"\\nData collection coverage:\")\n",
    "    print(f\"Expected SiteCode/SpeciesCode pairs (from metadata): {stats.get('expected_pairs', 0)}\")\n",
    "    print(f\"Actually collected pairs: {stats.get('collected_pairs', 0)}\")\n",
    "    print(f\"Missing pairs (not collected): {stats.get('missing_pairs_count', 0)}\")\n",
    "    print(f\"Extra pairs (not in metadata): {stats.get('extra_pairs_count', 0)}\")\n",
    "\n",
    "    if stats.get('missing_pairs_count', 0) > 0:\n",
    "        print(f\"\\nWarning: {stats['missing_pairs_count']} SiteCode/SpeciesCode pairs from metadata were not found in collected data.\")\n",
    "        print(\"First 10 missing pairs:\")\n",
    "        for i, (site, species) in enumerate(stats['missing_pairs'][:10], 1):\n",
    "            print(f\"  {i}. {site} - {species}\")\n",
    "\n",
    "    if stats.get('extra_pairs_count', 0) > 0:\n",
    "        print(f\"\\nNote: {stats['extra_pairs_count']} SiteCode/SpeciesCode pairs in collected data are not in metadata.\")\n",
    "\n",
    "    print(\"\\nFiles by year:\")\n",
    "    for year, count in stats['files_by_year'].items():\n",
    "        print(f\"  {year}: {count:,} files\")\n",
    "\n",
    "    print(\"\\nRecords by year:\")\n",
    "    for year, count in stats['records_by_year'].items():\n",
    "        missing = stats['missing_by_year'].get(year, 0)\n",
    "        missing_pct = (missing / count * 100) if count > 0 else 0\n",
    "        print(f\"  {year}: {count:,} records, {missing:,} missing ({missing_pct:.2f}%)\")\n",
    "\n",
    "    # adding nan value summary below\n",
    "    print(\"\\nNaN replacement summary:\")\n",
    "    print(f\"Total invalid flags replaced: {stats['total_nan_replacements']:,}\")\n",
    "    print(f\"Mean invalid percentage per file: {stats['mean_invalid_percentage']:.2f}%\")\n",
    "    print(f\"Max invalid percentage: {stats['max_invalid_percentage']:.2f}%\")\n",
    "\n",
    "    # count of replacements by year\n",
    "    if stats['nan_replacements_by_year']:\n",
    "        print(\"\\nReplacements by year:\")\n",
    "        for year_folder, count in stats['nan_replacements_by_year'].items():\n",
    "            print(f\"  {year_folder}: {count:,} flags replaced\")\n",
    "\n",
    "    print(\"\\nTemporal coverage:\")\n",
    "    print(f\"Start date: {stats['temporal_coverage']['start_date']}\")\n",
    "    print(f\"End date: {stats['temporal_coverage']['end_date']}\")\n",
    "    print(f\"Total months: {stats['temporal_coverage']['total_months']}\")\n",
    "\n",
    "    print(\"\\nPollutant (SpeciesCode) distribution:\")\n",
    "    print(\"Site/species combinations by type:\")\n",
    "    for species, count in sorted(stats['pollutant_distribution'].items(),\n",
    "                                 key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_combinations']) * 100\n",
    "        print(f\"  {species}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # missing value distribution by pollutant type\n",
    "    print(\"\\nMissing value distribution by pollutant type (SpeciesCode):\")\n",
    "    if stats.get('missing_by_pollutant_type'):\n",
    "        # sort by percentage missing (highest first)\n",
    "        sorted_species = sorted(\n",
    "            stats['missing_by_pollutant_type'].items(),\n",
    "            key=lambda x: x[1]['percentage_missing'],\n",
    "            reverse=True\n",
    "        )\n",
    "        print(f\"{'SpeciesCode':<20} {'total records':>15} {'missing':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for species, data in sorted_species:\n",
    "            print(f\"{species:<20} {data['total_records']:>15,} {data['total_missing']:>12,} {data['percentage_missing']:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value distribution available.\")\n",
    "\n",
    "    # print missing values by site/species breakdown with row_number column\n",
    "    print(\"\\nMissing values by site/species (SiteCode/SpeciesCode):\")\n",
    "    if stats.get('missing_by_station_pollutant'):\n",
    "        # prepare a sorted list by missing percentage descending\n",
    "        breakdown = []\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            percent = (missing / total * 100) if total > 0 else 0\n",
    "            breakdown.append((site, species, missing, total, percent))\n",
    "        # sort by percentage descending and take top 20\n",
    "        breakdown.sort(key=lambda x: x[4], reverse=True)\n",
    "        breakdown = breakdown[:20]\n",
    "        print(f\"{'SiteCode':<20} {'SpeciesCode':<20} {'missing':>10} {'total_row':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for site, species, missing, total, percent in breakdown:\n",
    "            print(f\"{site:<20} {species:<20} {missing:>10,} {total:>12,} {percent:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value breakdown available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "602d9919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading metadata...\n",
      "  expected SiteCode/SpeciesCode pairs from metadata: 170\n",
      "\n",
      "scanning optimased directory for collected data...\n",
      "\n",
      "Total CSV files found: 4932\n",
      "\n",
      "Reading all CSV files to calculate statistics...\n",
      "  2023_mar: 141 files, 101,520 records, 14,883 missing (14.66%)\n",
      "  2025_feb: 141 files, 91,368 records, 12,034 missing (13.17%)\n",
      "  2024_feb: 141 files, 94,752 records, 9,582 missing (10.11%)\n",
      "  2025_aug: 141 files, 101,520 records, 16,123 missing (15.88%)\n",
      "  2024_aug: 141 files, 101,520 records, 19,425 missing (19.13%)\n",
      "  2025_mar: 141 files, 101,520 records, 15,384 missing (15.15%)\n",
      "  2023_feb: 141 files, 91,368 records, 12,838 missing (14.05%)\n",
      "  2024_mar: 141 files, 101,520 records, 11,279 missing (11.11%)\n",
      "  2023_aug: 141 files, 101,520 records, 11,360 missing (11.19%)\n",
      "  2024_jul: 141 files, 101,520 records, 12,934 missing (12.74%)\n",
      "  2025_jul: 141 files, 101,520 records, 16,506 missing (16.26%)\n",
      "  2024_oct: 141 files, 101,520 records, 11,079 missing (10.91%)\n",
      "  2023_sep: 141 files, 98,136 records, 11,727 missing (11.95%)\n",
      "  2025_oct: 141 files, 101,520 records, 12,342 missing (12.16%)\n",
      "  2023_jan: 141 files, 101,520 records, 17,911 missing (17.64%)\n",
      "  2023_jul: 141 files, 101,520 records, 11,160 missing (10.99%)\n",
      "  2024_jan: 141 files, 101,520 records, 10,375 missing (10.22%)\n",
      "  2025_sep: 141 files, 98,136 records, 18,344 missing (18.69%)\n",
      "  2024_sep: 141 files, 98,136 records, 15,469 missing (15.76%)\n",
      "  2023_oct: 141 files, 101,520 records, 13,964 missing (13.75%)\n",
      "  2025_jan: 141 files, 101,520 records, 11,198 missing (11.03%)\n",
      "  2024_dec: 141 files, 101,520 records, 9,421 missing (9.28%)\n",
      "  2024_apr: 141 files, 98,136 records, 11,208 missing (11.42%)\n",
      "  2024_nov: 141 files, 98,136 records, 9,063 missing (9.24%)\n",
      "  2023_may: 141 files, 101,520 records, 12,641 missing (12.45%)\n",
      "  2025_nov: 141 files, 60,912 records, 8,613 missing (14.14%)\n",
      "  2025_apr: 141 files, 98,136 records, 10,606 missing (10.81%)\n",
      "  2024_may: 141 files, 101,520 records, 12,839 missing (12.65%)\n",
      "  2025_may: 141 files, 101,520 records, 10,843 missing (10.68%)\n",
      "  2023_nov: 138 files, 96,048 records, 10,851 missing (11.30%)\n",
      "  2023_apr: 141 files, 98,136 records, 10,992 missing (11.20%)\n",
      "  2023_dec: 141 files, 101,520 records, 14,882 missing (14.66%)\n",
      "  2025_jun: 141 files, 98,136 records, 14,018 missing (14.28%)\n",
      "  2024_jun: 141 files, 98,136 records, 9,661 missing (9.84%)\n",
      "  2023_jun: 141 files, 98,136 records, 12,103 missing (12.33%)\n",
      "\n",
      "cross-referencing collected data with metadata...\n",
      "  expected pairs from metadata: 170\n",
      "  actually collected pairs: 141\n",
      "  missing pairs (in metadata but not collected): 53\n",
      "  extra pairs (collected but not in metadata): 24\n",
      "\n",
      "========================================\n",
      "LAQN dataset statistics: initial assessment\n",
      "========================================\n",
      "\n",
      "Scale and scope:\n",
      "Total files collected: 4,932\n",
      "Total measurement records: 3,446,208\n",
      "Total missing values (@Value): 443,658\n",
      "Overall completeness: 87.13%\n",
      "Unique monitoring sites (SiteCode): 78\n",
      "Total site-species combinations: 173\n",
      "Unique pollutant types (SpeciesCode): 6\n",
      "Unique geographic locations: 76\n",
      "\n",
      "Data collection coverage:\n",
      "Expected SiteCode/SpeciesCode pairs (from metadata): 170\n",
      "Actually collected pairs: 141\n",
      "Missing pairs (not collected): 53\n",
      "Extra pairs (not in metadata): 24\n",
      "\n",
      "Warning: 53 SiteCode/SpeciesCode pairs from metadata were not found in collected data.\n",
      "First 10 missing pairs:\n",
      "  1. BL0 - PM25\n",
      "  2. TH4 - PM25\n",
      "  3. BT6 - PM25\n",
      "  4. MEB - PM25\n",
      "  5. GN6 - PM25\n",
      "  6. GR8 - PM25\n",
      "  7. GN3 - PM25\n",
      "  8. TL6 - PM25\n",
      "  9. GT1 - PM25\n",
      "  10. CE3 - PM25\n",
      "\n",
      "Note: 24 SiteCode/SpeciesCode pairs in collected data are not in metadata.\n",
      "\n",
      "Files by year:\n",
      "  2023: 1,689 files\n",
      "  2025: 1,551 files\n",
      "  2024: 1,692 files\n",
      "\n",
      "Records by year:\n",
      "  2023: 1,192,464 records, 155,312 missing (13.02%)\n",
      "  2025: 1,055,808 records, 146,011 missing (13.83%)\n",
      "  2024: 1,197,936 records, 142,335 missing (11.88%)\n",
      "\n",
      "NaN replacement summary:\n",
      "Total invalid flags replaced: 0\n",
      "Mean invalid percentage per file: 0.00%\n",
      "Max invalid percentage: 0.00%\n",
      "\n",
      "Temporal coverage:\n",
      "Start date: 2023-01-01\n",
      "End date: 2025-11-19\n",
      "Total months: 35\n",
      "\n",
      "Pollutant (SpeciesCode) distribution:\n",
      "Site/species combinations by type:\n",
      "  NO2: 60 (34.7%)\n",
      "  PM25: 53 (30.6%)\n",
      "  PM10: 43 (24.9%)\n",
      "  O3: 11 (6.4%)\n",
      "  SO2: 4 (2.3%)\n",
      "  CO: 2 (1.2%)\n",
      "\n",
      "Missing value distribution by pollutant type (SpeciesCode):\n",
      "SpeciesCode            total records      missing    % missing\n",
      "------------------------------------------------------------\n",
      "O3                           268,320       47,056       17.54%\n",
      "PM2.5                        586,944      100,755       17.17%\n",
      "SO2                           97,824       15,803       16.15%\n",
      "PM10                       1,026,456      126,749       12.35%\n",
      "NO2                        1,417,752      148,803       10.50%\n",
      "CO                            48,912        4,492        9.18%\n",
      "\n",
      "Missing values by site/species (SiteCode/SpeciesCode):\n",
      "SiteCode             SpeciesCode             missing    total_row    % missing\n",
      "------------------------------------------------------------\n",
      "WM6                  PM10                     15,357       24,456       62.79%\n",
      "CE3                  NO2                      11,394       24,456       46.59%\n",
      "TL4                  NO2                       9,869       24,456       40.35%\n",
      "RI2                  O3                        9,732       24,456       39.79%\n",
      "WA7                  NO2                       9,236       24,456       37.77%\n",
      "BG1                  SO2                       8,373       24,456       34.24%\n",
      "TH4                  PM2.5                     8,042       24,456       32.88%\n",
      "CD1                  PM10                      7,711       24,456       31.53%\n",
      "WAA                  NO2                       7,697       24,456       31.47%\n",
      "CE3                  PM10                      7,652       24,456       31.29%\n",
      "CE3                  PM2.5                     7,652       24,456       31.29%\n",
      "TH4                  PM10                      7,457       24,456       30.49%\n",
      "CD1                  PM2.5                     7,339       24,456       30.01%\n",
      "GN6                  PM2.5                     7,240       24,456       29.60%\n",
      "CR8                  PM2.5                     7,207       24,456       29.47%\n",
      "GN0                  PM2.5                     7,099       24,456       29.03%\n",
      "HG4                  O3                        7,092       24,456       29.00%\n",
      "CD1                  NO2                       6,984       24,456       28.56%\n",
      "BT5                  PM2.5                     6,783       24,456       27.74%\n",
      "MY1                  O3                        6,634       24,456       27.13%\n",
      "\n",
      "Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/laqn_stats.csv\n",
      "Species (pollutant) distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/pollutant_distribution.csv\n",
      "Missing value distribution by species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_pollutant.csv\n",
      "Missing values by site/species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_station_pollutant.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "stats = get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path)\n",
    "print_dataset_statistics(stats)\n",
    "\n",
    "# Save statistics for later use as csv\n",
    "# Prepare flat data structure for csv\n",
    "stats_rows = []\n",
    "stats_rows.append([\"metric\", \"value\"])\n",
    "stats_rows.append([\"total_files\", stats['total_files']])\n",
    "stats_rows.append([\"total_records\", stats['total_records']])\n",
    "stats_rows.append([\"total_missing\", stats['total_missing']])\n",
    "stats_rows.append([\"overall_completeness_pct\", f\"{stats['overall_completeness']:.2f}\"])\n",
    "stats_rows.append([\"unique_sites\", stats['unique_stations']])\n",
    "stats_rows.append([\"total_site_species_combinations\", stats['total_combinations']])\n",
    "stats_rows.append([\"unique_species\", stats['unique_pollutants']])\n",
    "stats_rows.append([\"unique_locations\", stats['unique_locations']])\n",
    "stats_rows.append([\"expected_site_species_pairs\", stats.get('expected_pairs', 0)])\n",
    "stats_rows.append([\"collected_site_species_pairs\", stats.get('collected_pairs', 0)])\n",
    "stats_rows.append([\"missing_site_species_pairs_count\", stats.get('missing_pairs_count', 0)])\n",
    "stats_rows.append([\"extra_site_species_pairs_count\", stats.get('extra_pairs_count', 0)])\n",
    "stats_rows.append([\"total_nan_replacements\", stats['total_nan_replacements']])\n",
    "stats_rows.append([\"mean_invalid_pct\", f\"{stats['mean_invalid_percentage']:.2f}\"])\n",
    "stats_rows.append([\"max_invalid_pct\", f\"{stats['max_invalid_percentage']:.2f}\"])\n",
    "\n",
    "# Add year-specific metrics\n",
    "for year in ['2023', '2024', '2025']:\n",
    "    stats_rows.append([f\"files_{year}\", stats['files_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"records_{year}\", stats['records_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"missing_{year}\", stats['missing_by_year'].get(year, 0)])\n",
    "    year_key = f'{year}measurements'\n",
    "    stats_rows.append([f\"replacements_{year}\", stats['nan_replacements_by_year'].get(year_key, 0)])\n",
    "\n",
    "# Save to csv stats report\n",
    "pd.DataFrame(stats_rows[1:], columns=stats_rows[0]).to_csv(stats_output_path, index=False)\n",
    "print(f\"\\nStatistics saved to: {stats_output_path}\")\n",
    "\n",
    "# Save species (pollutant) distribution to csv\n",
    "total_combinations = stats['total_combinations']\n",
    "species_distribution_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'count': v,\n",
    "            'percentage': round((v / total_combinations) * 100, 2) if total_combinations > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['pollutant_distribution'].items()\n",
    "    ]\n",
    ")\n",
    "species_distribution_df.to_csv(pollutant_distrubution_path, index=False)\n",
    "print(f\"Species (pollutant) distribution saved to: {pollutant_distrubution_path}\")\n",
    "\n",
    "# Save missing value distribution by species to csv\n",
    "if stats.get('missing_by_pollutant_type'):\n",
    "    missing_by_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'total_records': v['total_records'],\n",
    "            'total_missing': v['total_missing'],\n",
    "            'percentage_missing': v['percentage_missing']\n",
    "        }\n",
    "        for k, v in stats['missing_by_pollutant_type'].items()\n",
    "    ])\n",
    "    missing_by_species_df.to_csv(nan_val_pollutant_split_path, index=False)\n",
    "    print(f\"Missing value distribution by species saved to: {nan_val_pollutant_split_path}\")\n",
    "\n",
    "# Save missing values by site/species to csv\n",
    "if stats.get('missing_by_station_pollutant'):\n",
    "    missing_by_site_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SiteCode': k[0],\n",
    "            'SpeciesCode': k[1],\n",
    "            'missing': v[0],\n",
    "            'total_row': v[1],\n",
    "            'percentage_missing': (v[0] / v[1] * 100) if v[1] > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['missing_by_station_pollutant'].items()\n",
    "    ])\n",
    "    missing_by_site_species_df.to_csv(nan_val_stationPollutant_path, index=False)\n",
    "    print(f\"Missing values by site/species saved to: {nan_val_stationPollutant_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bd249",
   "metadata": {},
   "source": [
    "    loading metadata...\n",
    "    expected SiteCode/SpeciesCode pairs from metadata: 170\n",
    "\n",
    "    scanning optimased directory for collected data...\n",
    "\n",
    "    Total CSV files found: 4932\n",
    "\n",
    "    Reading all CSV files to calculate statistics...\n",
    "    2023_mar: 141 files, 101,520 records, 14,883 missing (14.66%)\n",
    "    2025_feb: 141 files, 91,368 records, 12,034 missing (13.17%)\n",
    "    2024_feb: 141 files, 94,752 records, 9,582 missing (10.11%)\n",
    "    2025_aug: 141 files, 101,520 records, 16,123 missing (15.88%)\n",
    "    2024_aug: 141 files, 101,520 records, 19,425 missing (19.13%)\n",
    "    2025_mar: 141 files, 101,520 records, 15,384 missing (15.15%)\n",
    "    2023_feb: 141 files, 91,368 records, 12,838 missing (14.05%)\n",
    "    2024_mar: 141 files, 101,520 records, 11,279 missing (11.11%)\n",
    "    2023_aug: 141 files, 101,520 records, 11,360 missing (11.19%)\n",
    "    2024_jul: 141 files, 101,520 records, 12,934 missing (12.74%)\n",
    "    2025_jul: 141 files, 101,520 records, 16,506 missing (16.26%)\n",
    "    2024_oct: 141 files, 101,520 records, 11,079 missing (10.91%)\n",
    "    2023_sep: 141 files, 98,136 records, 11,727 missing (11.95%)\n",
    "    2025_oct: 141 files, 101,520 records, 12,342 missing (12.16%)\n",
    "    2023_jan: 141 files, 101,520 records, 17,911 missing (17.64%)\n",
    "    2023_jul: 141 files, 101,520 records, 11,160 missing (10.99%)\n",
    "    2024_jan: 141 files, 101,520 records, 10,375 missing (10.22%)\n",
    "    2025_sep: 141 files, 98,136 records, 18,344 missing (18.69%)\n",
    "    2024_sep: 141 files, 98,136 records, 15,469 missing (15.76%)\n",
    "    2023_oct: 141 files, 101,520 records, 13,964 missing (13.75%)\n",
    "    2025_jan: 141 files, 101,520 records, 11,198 missing (11.03%)\n",
    "    2024_dec: 141 files, 101,520 records, 9,421 missing (9.28%)\n",
    "    2024_apr: 141 files, 98,136 records, 11,208 missing (11.42%)\n",
    "    2024_nov: 141 files, 98,136 records, 9,063 missing (9.24%)\n",
    "    2023_may: 141 files, 101,520 records, 12,641 missing (12.45%)\n",
    "    2025_nov: 141 files, 60,912 records, 8,613 missing (14.14%)\n",
    "    2025_apr: 141 files, 98,136 records, 10,606 missing (10.81%)\n",
    "    2024_may: 141 files, 101,520 records, 12,839 missing (12.65%)\n",
    "    2025_may: 141 files, 101,520 records, 10,843 missing (10.68%)\n",
    "    2023_nov: 138 files, 96,048 records, 10,851 missing (11.30%)\n",
    "    2023_apr: 141 files, 98,136 records, 10,992 missing (11.20%)\n",
    "    2023_dec: 141 files, 101,520 records, 14,882 missing (14.66%)\n",
    "    2025_jun: 141 files, 98,136 records, 14,018 missing (14.28%)\n",
    "    2024_jun: 141 files, 98,136 records, 9,661 missing (9.84%)\n",
    "    2023_jun: 141 files, 98,136 records, 12,103 missing (12.33%)\n",
    "\n",
    "    cross-referencing collected data with metadata...\n",
    "    expected pairs from metadata: 170\n",
    "    actually collected pairs: 141\n",
    "    missing pairs (in metadata but not collected): 53\n",
    "    extra pairs (collected but not in metadata): 24\n",
    "\n",
    "    ========================================\n",
    "    LAQN dataset statistics: initial assessment\n",
    "    ========================================\n",
    "\n",
    "    Scale and scope:\n",
    "    Total files collected: 4,932\n",
    "    Total measurement records: 3,446,208\n",
    "    Total missing values (@Value): 443,658\n",
    "    Overall completeness: 87.13%\n",
    "    Unique monitoring sites (SiteCode): 78\n",
    "    Total site-species combinations: 173\n",
    "    Unique pollutant types (SpeciesCode): 6\n",
    "    Unique geographic locations: 76\n",
    "\n",
    "    Data collection coverage:\n",
    "    Expected SiteCode/SpeciesCode pairs (from metadata): 170\n",
    "    Actually collected pairs: 141\n",
    "    Missing pairs (not collected): 53\n",
    "    Extra pairs (not in metadata): 24\n",
    "\n",
    "    Warning: 53 SiteCode/SpeciesCode pairs from metadata were not found in collected data.\n",
    "    First 10 missing pairs:\n",
    "    1. BL0 - PM25\n",
    "    2. TH4 - PM25\n",
    "    3. BT6 - PM25\n",
    "    4. MEB - PM25\n",
    "    5. GN6 - PM25\n",
    "    6. GR8 - PM25\n",
    "    7. GN3 - PM25\n",
    "    8. TL6 - PM25\n",
    "    9. GT1 - PM25\n",
    "    10. CE3 - PM25\n",
    "\n",
    "    Note: 24 SiteCode/SpeciesCode pairs in collected data are not in metadata.\n",
    "\n",
    "    Files by year:\n",
    "    2023: 1,689 files\n",
    "    2025: 1,551 files\n",
    "    2024: 1,692 files\n",
    "\n",
    "    Records by year:\n",
    "    2023: 1,192,464 records, 155,312 missing (13.02%)\n",
    "    2025: 1,055,808 records, 146,011 missing (13.83%)\n",
    "    2024: 1,197,936 records, 142,335 missing (11.88%)\n",
    "\n",
    "    NaN replacement summary:\n",
    "    Total invalid flags replaced: 0\n",
    "    Mean invalid percentage per file: 0.00%\n",
    "    Max invalid percentage: 0.00%\n",
    "\n",
    "    Temporal coverage:\n",
    "    Start date: 2023-01-01\n",
    "    End date: 2025-11-19\n",
    "    Total months: 35\n",
    "\n",
    "    Pollutant (SpeciesCode) distribution:\n",
    "    Site/species combinations by type:\n",
    "    NO2: 60 (34.7%)\n",
    "    PM25: 53 (30.6%)\n",
    "    PM10: 43 (24.9%)\n",
    "    O3: 11 (6.4%)\n",
    "    SO2: 4 (2.3%)\n",
    "    CO: 2 (1.2%)\n",
    "\n",
    "    Missing value distribution by pollutant type (SpeciesCode):\n",
    "    SpeciesCode            total records      missing    % missing\n",
    "    ------------------------------------------------------------\n",
    "    O3                           268,320       47,056       17.54%\n",
    "    PM2.5                        586,944      100,755       17.17%\n",
    "    SO2                           97,824       15,803       16.15%\n",
    "    PM10                       1,026,456      126,749       12.35%\n",
    "    NO2                        1,417,752      148,803       10.50%\n",
    "    CO                            48,912        4,492        9.18%\n",
    "\n",
    "    Missing values by site/species (SiteCode/SpeciesCode):\n",
    "    SiteCode             SpeciesCode             missing    total_row    % missing\n",
    "    ------------------------------------------------------------\n",
    "    WM6                  PM10                     15,357       24,456       62.79%\n",
    "    CE3                  NO2                      11,394       24,456       46.59%\n",
    "    TL4                  NO2                       9,869       24,456       40.35%\n",
    "    RI2                  O3                        9,732       24,456       39.79%\n",
    "    WA7                  NO2                       9,236       24,456       37.77%\n",
    "    BG1                  SO2                       8,373       24,456       34.24%\n",
    "    TH4                  PM2.5                     8,042       24,456       32.88%\n",
    "    CD1                  PM10                      7,711       24,456       31.53%\n",
    "    WAA                  NO2                       7,697       24,456       31.47%\n",
    "    CE3                  PM10                      7,652       24,456       31.29%\n",
    "    CE3                  PM2.5                     7,652       24,456       31.29%\n",
    "    TH4                  PM10                      7,457       24,456       30.49%\n",
    "    CD1                  PM2.5                     7,339       24,456       30.01%\n",
    "    GN6                  PM2.5                     7,240       24,456       29.60%\n",
    "    CR8                  PM2.5                     7,207       24,456       29.47%\n",
    "    GN0                  PM2.5                     7,099       24,456       29.03%\n",
    "    HG4                  O3                        7,092       24,456       29.00%\n",
    "    CD1                  NO2                       6,984       24,456       28.56%\n",
    "    BT5                  PM2.5                     6,783       24,456       27.74%\n",
    "    MY1                  O3                        6,634       24,456       27.13%\n",
    "\n",
    "    Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/laqn_stats.csv\n",
    "    Species (pollutant) distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/pollutant_distribution.csv\n",
    "    Missing value distribution by species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_pollutant.csv\n",
    "    Missing values by site/species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_station_pollutant.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
