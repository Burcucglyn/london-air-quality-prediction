{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccfa1de",
   "metadata": {},
   "source": [
    "# LAQN Dataset Assesment\n",
    "\n",
    "\n",
    "1) I'll be start adding my main paths and modules I will be using in this notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa1335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible python modules i will be using below\n",
    "from curses import meta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "#function 7 importing the full analysis function from pollution_analysis\n",
    "import sys\n",
    "sys.path.append('/mnt/user-data/outputs')\n",
    "\n",
    "#last detailed anlasye and visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualisation style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "#findings 7 .func\n",
    "# for parse pdf uk pollutant limitations to csv\n",
    "import re\n",
    "# pdfplumber for pdf parsing\n",
    "\n",
    "# function 5. chi-square test\n",
    "from scipy import stats\n",
    "\n",
    "#define base path  without hardcoding\n",
    "base_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"optimised\"\n",
    "#metadata file for pollutant name, location and site names\n",
    "metadata_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"optimased_siteSpecies.csv\"\n",
    "\n",
    "# output path for saving statistics 1. function\n",
    "#the first analyse dataset created without inclitiong nan optimased files, and cross referencing that's why changed the name to dataset_statistics-noNAN-incl.csv\n",
    "os.makedirs(base_dir / \"report\", exist_ok=True)\n",
    "stats_output_path =  Path.home()/\"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"report\"/ \"laqn_stats.csv\"\n",
    "\n",
    "# output paths for saving all the pollutant distribution and nan value analysis.\n",
    "pollutant_distrubution_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\"/\"pollutant_distribution.csv\"\n",
    "nan_val_pollutant_split_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_pollutant.csv\"\n",
    "nan_val_stationPollutant_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_station_pollutant.csv\"\n",
    "\n",
    "\n",
    "# log file from nan replacement process\n",
    "nan_log_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"logs\" / \"NaN_values_record.csv\"\n",
    "\n",
    "# function for uk pollutant regulations pdf to parse csv file path\n",
    "csv_output_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"capabilities\" / \"uk_pollutant_limits.csv\"\n",
    "\n",
    "\n",
    "# data quality metrics report output path\n",
    "quality_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\"/ \"report\" / \"quality_metrics_validation.csv\"\n",
    "quality_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#chi-square test output path func 5\n",
    "chi_square_output1 = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests1.csv\"\n",
    "chi_square_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests.csv\"\n",
    "\n",
    "# detailed last analysis and visualization output directory\n",
    "report_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"detailed_analysis\"\n",
    "\n",
    "report_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d9baf",
   "metadata": {},
   "source": [
    "## 1) Initial Dataset Assessment: Raw Numbers\n",
    "\n",
    "Before conducting quality checks, I need to establish the baseline characteristics of the LAQN dataset. This section calculates comprehensive statistics about the data collection effort, including file counts, measurement records, station coverage, and pollutant distribution.\n",
    "\n",
    "### Purpose\n",
    "- Document the scale and scope of data collection.\n",
    "- Establish baseline metrics for comparison with LAQN.\n",
    "- Provide context for subsequent quality analysis.\n",
    "\n",
    "### Methodology\n",
    "The function `get_laqn_dataset_statistics()` performs the following:\n",
    "1. Loads standardised metadata to identify unique stations and pollutants.\n",
    "2. Counts files across all three yearly directories (2023, 2024, 2025).\n",
    "3. Calculates total measurement records by reading all CSV files.\n",
    "4. Determines spatial coverage from unique coordinate pairs.\n",
    "5. Documents temporal coverage (35 months: January 2023 to November 2025).\n",
    "\n",
    "### Notes\n",
    "- File counting is fast (scans directory structure only).\n",
    "- Record counting can be slow (reads every CSV file).\n",
    "- Results are saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d680dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path):\n",
    "    \"\"\"\n",
    "    Calculate statistics for the LAQN dataset using the new column structure.\n",
    "    This function walks through the monthly data directories 2023, 2024, 2025 and calculates key metrics needed for reporting.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir : Path\n",
    "            Base directory containing LAQN data folders.\n",
    "        metadata_path : Path\n",
    "            Path to the standardised metadata csv file.\n",
    "        nan_log_path : Path\n",
    "            Path to the NaN values log file after notice data flags, changed them to NaN.\n",
    "\n",
    "    Returns:\n",
    "        dict : Dictionary containing all calculated statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    # read metadata to get station and pollutant info\n",
    "    print(\"\\nloading metadata...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "\n",
    "    # calculate metadata statistics\n",
    "    stats['unique_stations'] = metadata['SiteCode'].nunique()\n",
    "    stats['total_combinations'] = len(metadata)\n",
    "    stats['unique_pollutants'] = metadata['SpeciesCode'].nunique()\n",
    "\n",
    "    # get pollutant breakdown\n",
    "    pollutant_counts = metadata['SpeciesCode'].value_counts()\n",
    "    stats['pollutant_distribution'] = pollutant_counts.to_dict()\n",
    "\n",
    "    # create set of expected (SiteCode, SpeciesCode) pairs from metadata\n",
    "    expected_pairs = set(\n",
    "        zip(metadata['SiteCode'], metadata['SpeciesCode'])\n",
    "    )\n",
    "    stats['expected_pairs'] = len(expected_pairs)\n",
    "    print(f\"  expected SiteCode/SpeciesCode pairs from metadata: {len(expected_pairs)}\")\n",
    "\n",
    "    # count unique coordinates for spatial coverage\n",
    "    unique_coords = metadata[['Latitude', 'Longitude']].drop_duplicates()\n",
    "    stats['unique_locations'] = len(unique_coords)\n",
    "\n",
    "    # count files in monthly data directories\n",
    "    total_files = 0\n",
    "    files_by_year = {}\n",
    "\n",
    "    print(\"\\nscanning optimised directory for collected data...\")\n",
    "    # Scan all CSVs in all subfolders under base_dir\n",
    "    all_csv_files = list(Path(base_dir).rglob('*.csv'))\n",
    "    total_files = len(all_csv_files)\n",
    "    print(f\"\\nTotal CSV files found: {total_files}\")\n",
    "    stats['total_files'] = total_files\n",
    "\n",
    "    # Optionally, count files by month/year if your subfolders are named like \"2023_apr\", \"2023_may\", etc.\n",
    "    from collections import defaultdict\n",
    "    files_by_period = defaultdict(int)\n",
    "    for f in all_csv_files:\n",
    "        # Extract the immediate subfolder name (e.g., \"2023_apr\")\n",
    "        period = f.parent.name\n",
    "        files_by_period[period] += 1\n",
    "    stats['files_by_period'] = dict(files_by_period)\n",
    "    for period, count in stats['files_by_period'].items():\n",
    "        print(f\"  {period}: {count} files\")\n",
    "    stats['total_files'] = total_files\n",
    "    stats['files_by_year'] = files_by_year\n",
    "\n",
    "    # calculate total measurement records, this requires reading all csv files and counting rows\n",
    "    total_records = 0\n",
    "    records_by_year = {}\n",
    "    total_missing = 0\n",
    "    missing_by_year = {}\n",
    "\n",
    "    all_csvs = []\n",
    "\n",
    "    print(\"\\nreading all CSV files to calculate statistics...\")\n",
    "    for year in ['2023', '2024', '2025']:\n",
    "        year_dir = Path(base_dir) / f'{year}measurements'\n",
    "        year_records = 0\n",
    "        year_missing = 0\n",
    "\n",
    "        if year_dir.exists():\n",
    "            for csv_file in year_dir.rglob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    year_records += len(df)\n",
    "                    # count missing NaN or empty string values in @Value column\n",
    "                    if '@Value' in df.columns:\n",
    "                        missing_in_file = df['@Value'].isna().sum() + (df['@Value'] == \"\").sum()\n",
    "                        year_missing += missing_in_file\n",
    "                    all_csvs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"  warning: could not read {csv_file.name}: {e}\")\n",
    "            records_by_year[year] = year_records\n",
    "            missing_by_year[year] = year_missing\n",
    "            total_records += year_records\n",
    "            total_missing += year_missing\n",
    "            print(f\"  {year}: {year_records:,} records, {year_missing:,} missing ({(year_missing/year_records*100) if year_records else 0:.2f}%)\")\n",
    "        else:\n",
    "            records_by_year[year] = 0\n",
    "            missing_by_year[year] = 0\n",
    "\n",
    "    stats['total_records'] = total_records\n",
    "    stats['records_by_year'] = records_by_year\n",
    "    stats['missing_by_year'] = missing_by_year\n",
    "    stats['total_missing'] = total_missing\n",
    "    stats['overall_completeness'] = ((total_records - total_missing) / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "    # cross-reference metadata with collected data\n",
    "    print(\"\\ncross-referencing collected data with metadata...\")\n",
    "\n",
    "    if all_csvs:\n",
    "        all_data = pd.concat(all_csvs, ignore_index=True)\n",
    "\n",
    "        # check if required columns exist in csv files\n",
    "        if 'SiteCode' in all_data.columns and 'SpeciesCode' in all_data.columns:\n",
    "            # identify actual (SiteCode, SpeciesCode) pairs in collected data\n",
    "            collected_pairs = set(\n",
    "                zip(all_data['SiteCode'], all_data['SpeciesCode'])\n",
    "            )\n",
    "            stats['collected_pairs'] = len(collected_pairs)\n",
    "\n",
    "            # find missing pairs (in metadata but not in collected data)\n",
    "            missing_pairs = expected_pairs - collected_pairs\n",
    "            stats['missing_pairs'] = list(missing_pairs)\n",
    "            stats['missing_pairs_count'] = len(missing_pairs)\n",
    "\n",
    "            # find extra pairs (in collected data but not in metadata)\n",
    "            extra_pairs = collected_pairs - expected_pairs\n",
    "            stats['extra_pairs'] = list(extra_pairs)\n",
    "            stats['extra_pairs_count'] = len(extra_pairs)\n",
    "\n",
    "            print(f\"  expected pairs from metadata: {len(expected_pairs)}\")\n",
    "            print(f\"  actually collected pairs: {len(collected_pairs)}\")\n",
    "            print(f\"  missing pairs (in metadata but not collected): {len(missing_pairs)}\")\n",
    "            print(f\"  extra pairs (collected but not in metadata): {len(extra_pairs)}\")\n",
    "\n",
    "            # group by SiteCode and SpeciesCode, count missing values\n",
    "            missing_breakdown = {}\n",
    "            for (site, species), group in all_data.groupby(['SiteCode', 'SpeciesCode']):\n",
    "                total_rows = len(group)\n",
    "                if '@Value' in group.columns:\n",
    "                    missing_rows = group['@Value'].isna().sum() + (group['@Value'] == \"\").sum()\n",
    "                else:\n",
    "                    missing_rows = 0\n",
    "                missing_breakdown[(site, species)] = (int(missing_rows), int(total_rows))\n",
    "            stats['missing_by_station_pollutant'] = missing_breakdown\n",
    "        else:\n",
    "            print(\"  warning: SiteCode or SpeciesCode columns not found\")\n",
    "            stats['missing_by_station_pollutant'] = {}\n",
    "            stats['collected_pairs'] = 0\n",
    "            stats['missing_pairs'] = []\n",
    "            stats['missing_pairs_count'] = 0\n",
    "            stats['extra_pairs'] = []\n",
    "            stats['extra_pairs_count'] = 0\n",
    "    else:\n",
    "        stats['missing_by_station_pollutant'] = {}\n",
    "        stats['collected_pairs'] = 0\n",
    "        stats['missing_pairs'] = list(expected_pairs)\n",
    "        stats['missing_pairs_count'] = len(expected_pairs)\n",
    "        stats['extra_pairs'] = []\n",
    "        stats['extra_pairs_count'] = 0\n",
    "\n",
    "    # distribution of nan by pollutant over time\n",
    "    if stats['missing_by_station_pollutant']:\n",
    "        pollutant_missing_summary = {}\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            if species not in pollutant_missing_summary:\n",
    "                pollutant_missing_summary[species] = {'total_missing': 0, 'total_records': 0}\n",
    "            pollutant_missing_summary[species]['total_missing'] += missing\n",
    "            pollutant_missing_summary[species]['total_records'] += total\n",
    "        for species in pollutant_missing_summary:\n",
    "            total_missing = pollutant_missing_summary[species]['total_missing']\n",
    "            total_records = pollutant_missing_summary[species]['total_records']\n",
    "            percentage = (total_missing / total_records * 100) if total_records > 0 else 0\n",
    "            pollutant_missing_summary[species]['percentage_missing'] = percentage\n",
    "        stats['missing_by_pollutant_type'] = pollutant_missing_summary\n",
    "    else:\n",
    "        stats['missing_by_pollutant_type'] = {}\n",
    "\n",
    "    # log file created during data cleaning process\n",
    "    if Path(nan_log_path).exists():\n",
    "        nan_log = pd.read_csv(nan_log_path)\n",
    "        replacements_by_year = nan_log.groupby('year_folder')['invalid_flags_replaced'].sum().to_dict()\n",
    "        stats['nan_replacements_by_year'] = replacements_by_year\n",
    "        stats['total_nan_replacements'] = nan_log['invalid_flags_replaced'].sum()\n",
    "        stats['mean_invalid_percentage'] = nan_log['percentage_invalid'].mean()\n",
    "        stats['max_invalid_percentage'] = nan_log['percentage_invalid'].max()\n",
    "    else:\n",
    "        stats['nan_replacements_by_year'] = {}\n",
    "        stats['total_nan_replacements'] = 0\n",
    "        stats['mean_invalid_percentage'] = 0\n",
    "        stats['max_invalid_percentage'] = 0\n",
    "\n",
    "    # calculate temporal coverage based on the files collected\n",
    "    stats['temporal_coverage'] = {\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2025-11-19',\n",
    "        'total_months': 35\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e2a9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(stats):\n",
    "    \"\"\"\n",
    "    Print dataset statistics for LAQN using new column structure.\n",
    "\n",
    "    Parameters:\n",
    "        stats : dict\n",
    "            returned by get_laqn_dataset_statistics().\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"LAQN dataset statistics: initial assessment\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    print(\"\\nScale and scope:\")\n",
    "    print(f\"Total files collected: {stats['total_files']:,}\")\n",
    "    print(f\"Total measurement records: {stats['total_records']:,}\")\n",
    "    print(f\"Total missing values (@Value): {stats['total_missing']:,}\")\n",
    "    print(f\"Overall completeness: {stats['overall_completeness']:.2f}%\")\n",
    "    print(f\"Unique monitoring sites (SiteCode): {stats['unique_stations']}\")\n",
    "    print(f\"Total site-species combinations: {stats['total_combinations']}\")\n",
    "    print(f\"Unique pollutant types (SpeciesCode): {stats['unique_pollutants']}\")\n",
    "    print(f\"Unique geographic locations: {stats['unique_locations']}\")\n",
    "\n",
    "    # data collection coverage\n",
    "    print(\"\\nData collection coverage:\")\n",
    "    print(f\"Expected SiteCode/SpeciesCode pairs (from metadata): {stats.get('expected_pairs', 0)}\")\n",
    "    print(f\"Actually collected pairs: {stats.get('collected_pairs', 0)}\")\n",
    "    print(f\"Missing pairs (not collected): {stats.get('missing_pairs_count', 0)}\")\n",
    "    print(f\"Extra pairs (not in metadata): {stats.get('extra_pairs_count', 0)}\")\n",
    "\n",
    "    if stats.get('missing_pairs_count', 0) > 0:\n",
    "        print(f\"\\nWarning: {stats['missing_pairs_count']} SiteCode/SpeciesCode pairs from metadata were not found in collected data.\")\n",
    "        print(\"First 10 missing pairs:\")\n",
    "        for i, (site, species) in enumerate(stats['missing_pairs'][:10], 1):\n",
    "            print(f\"  {i}. {site} - {species}\")\n",
    "\n",
    "    if stats.get('extra_pairs_count', 0) > 0:\n",
    "        print(f\"\\nNote: {stats['extra_pairs_count']} SiteCode/SpeciesCode pairs in collected data are not in metadata.\")\n",
    "\n",
    "    print(\"\\nFiles by year:\")\n",
    "    for year, count in stats['files_by_year'].items():\n",
    "        print(f\"  {year}: {count:,} files\")\n",
    "\n",
    "    print(\"\\nRecords by year:\")\n",
    "    for year, count in stats['records_by_year'].items():\n",
    "        missing = stats['missing_by_year'].get(year, 0)\n",
    "        missing_pct = (missing / count * 100) if count > 0 else 0\n",
    "        print(f\"  {year}: {count:,} records, {missing:,} missing ({missing_pct:.2f}%)\")\n",
    "\n",
    "    # adding nan value summary below\n",
    "    print(\"\\nNaN replacement summary:\")\n",
    "    print(f\"Total invalid flags replaced: {stats['total_nan_replacements']:,}\")\n",
    "    print(f\"Mean invalid percentage per file: {stats['mean_invalid_percentage']:.2f}%\")\n",
    "    print(f\"Max invalid percentage: {stats['max_invalid_percentage']:.2f}%\")\n",
    "\n",
    "    # count of replacements by year\n",
    "    if stats['nan_replacements_by_year']:\n",
    "        print(\"\\nReplacements by year:\")\n",
    "        for year_folder, count in stats['nan_replacements_by_year'].items():\n",
    "            print(f\"  {year_folder}: {count:,} flags replaced\")\n",
    "\n",
    "    print(\"\\nTemporal coverage:\")\n",
    "    print(f\"Start date: {stats['temporal_coverage']['start_date']}\")\n",
    "    print(f\"End date: {stats['temporal_coverage']['end_date']}\")\n",
    "    print(f\"Total months: {stats['temporal_coverage']['total_months']}\")\n",
    "\n",
    "    print(\"\\nPollutant (SpeciesCode) distribution:\")\n",
    "    print(\"Site/species combinations by type:\")\n",
    "    for species, count in sorted(stats['pollutant_distribution'].items(),\n",
    "                                 key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_combinations']) * 100\n",
    "        print(f\"  {species}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # missing value distribution by pollutant type\n",
    "    print(\"\\nMissing value distribution by pollutant type (SpeciesCode):\")\n",
    "    if stats.get('missing_by_pollutant_type'):\n",
    "        # sort by percentage missing (highest first)\n",
    "        sorted_species = sorted(\n",
    "            stats['missing_by_pollutant_type'].items(),\n",
    "            key=lambda x: x[1]['percentage_missing'],\n",
    "            reverse=True\n",
    "        )\n",
    "        print(f\"{'SpeciesCode':<20} {'total records':>15} {'missing':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for species, data in sorted_species:\n",
    "            print(f\"{species:<20} {data['total_records']:>15,} {data['total_missing']:>12,} {data['percentage_missing']:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value distribution available.\")\n",
    "\n",
    "    # print missing values by site/species breakdown with row_number column\n",
    "    print(\"\\nMissing values by site/species (SiteCode/SpeciesCode):\")\n",
    "    if stats.get('missing_by_station_pollutant'):\n",
    "        # prepare a sorted list by missing percentage descending\n",
    "        breakdown = []\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            percent = (missing / total * 100) if total > 0 else 0\n",
    "            breakdown.append((site, species, missing, total, percent))\n",
    "        # sort by percentage descending and take top 20\n",
    "        breakdown.sort(key=lambda x: x[4], reverse=True)\n",
    "        breakdown = breakdown[:20]\n",
    "        print(f\"{'SiteCode':<20} {'SpeciesCode':<20} {'missing':>10} {'total_row':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for site, species, missing, total, percent in breakdown:\n",
    "            print(f\"{site:<20} {species:<20} {missing:>10,} {total:>12,} {percent:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value breakdown available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602d9919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading metadata...\n",
      "  expected SiteCode/SpeciesCode pairs from metadata: 170\n",
      "\n",
      "scanning optimised directory for collected data...\n",
      "  2023: directory not found\n",
      "  2024: directory not found\n",
      "  2025: directory not found\n",
      "\n",
      "reading all CSV files to calculate statistics...\n",
      "\n",
      "cross-referencing collected data with metadata...\n",
      "\n",
      "========================================\n",
      "LAQN dataset statistics: initial assessment\n",
      "========================================\n",
      "\n",
      "Scale and scope:\n",
      "Total files collected: 0\n",
      "Total measurement records: 0\n",
      "Total missing values (@Value): 0\n",
      "Overall completeness: 0.00%\n",
      "Unique monitoring sites (SiteCode): 78\n",
      "Total site-species combinations: 173\n",
      "Unique pollutant types (SpeciesCode): 6\n",
      "Unique geographic locations: 76\n",
      "\n",
      "Data collection coverage:\n",
      "Expected SiteCode/SpeciesCode pairs (from metadata): 170\n",
      "Actually collected pairs: 0\n",
      "Missing pairs (not collected): 170\n",
      "Extra pairs (not in metadata): 0\n",
      "\n",
      "Warning: 170 SiteCode/SpeciesCode pairs from metadata were not found in collected data.\n",
      "First 10 missing pairs:\n",
      "  1. BQ7 - PM10\n",
      "  2. MEB - PM25\n",
      "  3. TL6 - PM25\n",
      "  4. GN3 - O3\n",
      "  5. BX2 - PM10\n",
      "  6. BT5 - NO2\n",
      "  7. SK5 - PM10\n",
      "  8. CE2 - PM10\n",
      "  9. HV3 - PM10\n",
      "  10. BY7 - PM10\n",
      "\n",
      "Files by year:\n",
      "  2023: 0 files\n",
      "  2024: 0 files\n",
      "  2025: 0 files\n",
      "\n",
      "Records by year:\n",
      "  2023: 0 records, 0 missing (0.00%)\n",
      "  2024: 0 records, 0 missing (0.00%)\n",
      "  2025: 0 records, 0 missing (0.00%)\n",
      "\n",
      "NaN replacement summary:\n",
      "Total invalid flags replaced: 0\n",
      "Mean invalid percentage per file: 0.00%\n",
      "Max invalid percentage: 0.00%\n",
      "\n",
      "Temporal coverage:\n",
      "Start date: 2023-01-01\n",
      "End date: 2025-11-19\n",
      "Total months: 35\n",
      "\n",
      "Pollutant (SpeciesCode) distribution:\n",
      "Site/species combinations by type:\n",
      "  NO2: 60 (34.7%)\n",
      "  PM25: 53 (30.6%)\n",
      "  PM10: 43 (24.9%)\n",
      "  O3: 11 (6.4%)\n",
      "  SO2: 4 (2.3%)\n",
      "  CO: 2 (1.2%)\n",
      "\n",
      "Missing value distribution by pollutant type (SpeciesCode):\n",
      "  No missing value distribution available.\n",
      "\n",
      "Missing values by site/species (SiteCode/SpeciesCode):\n",
      "  No missing value breakdown available.\n",
      "\n",
      "Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/laqn_stats.csv\n",
      "Species (pollutant) distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/pollutant_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "stats = get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path)\n",
    "print_dataset_statistics(stats)\n",
    "\n",
    "# Save statistics for later use as csv\n",
    "# Prepare flat data structure for csv\n",
    "stats_rows = []\n",
    "stats_rows.append([\"metric\", \"value\"])\n",
    "stats_rows.append([\"total_files\", stats['total_files']])\n",
    "stats_rows.append([\"total_records\", stats['total_records']])\n",
    "stats_rows.append([\"total_missing\", stats['total_missing']])\n",
    "stats_rows.append([\"overall_completeness_pct\", f\"{stats['overall_completeness']:.2f}\"])\n",
    "stats_rows.append([\"unique_sites\", stats['unique_stations']])\n",
    "stats_rows.append([\"total_site_species_combinations\", stats['total_combinations']])\n",
    "stats_rows.append([\"unique_species\", stats['unique_pollutants']])\n",
    "stats_rows.append([\"unique_locations\", stats['unique_locations']])\n",
    "stats_rows.append([\"expected_site_species_pairs\", stats.get('expected_pairs', 0)])\n",
    "stats_rows.append([\"collected_site_species_pairs\", stats.get('collected_pairs', 0)])\n",
    "stats_rows.append([\"missing_site_species_pairs_count\", stats.get('missing_pairs_count', 0)])\n",
    "stats_rows.append([\"extra_site_species_pairs_count\", stats.get('extra_pairs_count', 0)])\n",
    "stats_rows.append([\"total_nan_replacements\", stats['total_nan_replacements']])\n",
    "stats_rows.append([\"mean_invalid_pct\", f\"{stats['mean_invalid_percentage']:.2f}\"])\n",
    "stats_rows.append([\"max_invalid_pct\", f\"{stats['max_invalid_percentage']:.2f}\"])\n",
    "\n",
    "# Add year-specific metrics\n",
    "for year in ['2023', '2024', '2025']:\n",
    "    stats_rows.append([f\"files_{year}\", stats['files_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"records_{year}\", stats['records_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"missing_{year}\", stats['missing_by_year'].get(year, 0)])\n",
    "    year_key = f'{year}measurements'\n",
    "    stats_rows.append([f\"replacements_{year}\", stats['nan_replacements_by_year'].get(year_key, 0)])\n",
    "\n",
    "# Save to csv stats report\n",
    "pd.DataFrame(stats_rows[1:], columns=stats_rows[0]).to_csv(stats_output_path, index=False)\n",
    "print(f\"\\nStatistics saved to: {stats_output_path}\")\n",
    "\n",
    "# Save species (pollutant) distribution to csv\n",
    "total_combinations = stats['total_combinations']\n",
    "species_distribution_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'count': v,\n",
    "            'percentage': round((v / total_combinations) * 100, 2) if total_combinations > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['pollutant_distribution'].items()\n",
    "    ]\n",
    ")\n",
    "species_distribution_df.to_csv(pollutant_distrubution_path, index=False)\n",
    "print(f\"Species (pollutant) distribution saved to: {pollutant_distrubution_path}\")\n",
    "\n",
    "# Save missing value distribution by species to csv\n",
    "if stats.get('missing_by_pollutant_type'):\n",
    "    missing_by_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'total_records': v['total_records'],\n",
    "            'total_missing': v['total_missing'],\n",
    "            'percentage_missing': v['percentage_missing']\n",
    "        }\n",
    "        for k, v in stats['missing_by_pollutant_type'].items()\n",
    "    ])\n",
    "    missing_by_species_df.to_csv(nan_val_pollutant_split_path, index=False)\n",
    "    print(f\"Missing value distribution by species saved to: {nan_val_pollutant_split_path}\")\n",
    "\n",
    "# Save missing values by site/species to csv\n",
    "if stats.get('missing_by_station_pollutant'):\n",
    "    missing_by_site_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SiteCode': k[0],\n",
    "            'SpeciesCode': k[1],\n",
    "            'missing': v[0],\n",
    "            'total_row': v[1],\n",
    "            'percentage_missing': (v[0] / v[1] * 100) if v[1] > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['missing_by_station_pollutant'].items()\n",
    "    ])\n",
    "    missing_by_site_species_df.to_csv(nan_val_stationPollutant_path, index=False)\n",
    "    print(f\"Missing values by site/species saved to: {nan_val_stationPollutant_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
