{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccfa1de",
   "metadata": {},
   "source": [
    "# LAQN Dataset Assesment\n",
    "\n",
    "\n",
    "1) I'll be start adding my main paths and modules I will be using in this notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cfa1335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible python modules i will be using below\n",
    "from curses import meta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "#function 7 importing the full analysis function from pollution_analysis\n",
    "import sys\n",
    "sys.path.append('/mnt/user-data/outputs')\n",
    "\n",
    "#last detailed anlasye and visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualisation style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "#findings 7 .func\n",
    "# for parse pdf uk pollutant limitations to csv\n",
    "import re\n",
    "# pdfplumber for pdf parsing\n",
    "\n",
    "# function 5. chi-square test\n",
    "from scipy import stats\n",
    "\n",
    "#define base path  without hardcoding\n",
    "base_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"optimased\"\n",
    "#metadata file for pollutant name, location and site names\n",
    "metadata_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"optimased_siteSpecies.csv\"\n",
    "\n",
    "# output path for saving statistics 1. function\n",
    "#the first analyse dataset created without inclitiong nan optimased files, and cross referencing that's why changed the name to dataset_statistics-noNAN-incl.csv\n",
    "os.makedirs(base_dir / \"report\", exist_ok=True)\n",
    "stats_output_path =  Path.home()/\"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\"/\"report\"/ \"laqn_stats.csv\"\n",
    "\n",
    "# output paths for saving all the pollutant distribution and nan value analysis.\n",
    "pollutant_distrubution_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\"/\"pollutant_distribution.csv\"\n",
    "nan_val_pollutant_split_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_pollutant.csv\"\n",
    "nan_val_stationPollutant_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" /\"laqn\" / \"report\" / \"nan_values_by_station_pollutant.csv\"\n",
    "\n",
    "\n",
    "# log file from nan replacement process\n",
    "nan_log_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"logs\" / \"NaN_values_record.csv\"\n",
    "\n",
    "# function for uk pollutant regulations pdf to parse csv file path\n",
    "csv_output_path = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"defra\" / \"capabilities\" / \"uk_pollutant_limits.csv\"\n",
    "\n",
    "\n",
    "# data quality metrics report output path\n",
    "quality_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\"/ \"report\" / \"quality_metrics_validation.csv\"\n",
    "quality_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#chi-square test output path func 5\n",
    "chi_square_output1 = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests1.csv\"\n",
    "# save results to csv\n",
    "chi_square_laqn_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"chi_square_tests.csv\"\n",
    "\n",
    "\n",
    " # chi-square yearly comperison analyse path\n",
    "year_diff_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"year_coverage_analysis.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# detailed last analysis and visualization output directory\n",
    "report_dir = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"detailed_analysis\"\n",
    "\n",
    "report_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d9baf",
   "metadata": {},
   "source": [
    "## 1) Initial Dataset Assessment: Raw Numbers\n",
    "\n",
    "Before conducting quality checks, I need to establish the baseline characteristics of the LAQN dataset. This section calculates comprehensive statistics about the data collection effort, including file counts, measurement records, station coverage, and pollutant distribution.\n",
    "\n",
    "### Purpose\n",
    "- Document the scale and scope of data collection.\n",
    "- Establish baseline metrics for comparison with LAQN.\n",
    "- Provide context for subsequent quality analysis.\n",
    "\n",
    "### Methodology\n",
    "The function `get_laqn_dataset_statistics()` performs the following:\n",
    "1. Loads standardised metadata to identify unique stations and pollutants.\n",
    "2. Counts files across all three yearly directories (2023, 2024, 2025).\n",
    "3. Calculates total measurement records by reading all CSV files.\n",
    "4. Determines spatial coverage from unique coordinate pairs.\n",
    "5. Documents temporal coverage (35 months: January 2023 to November 2025).\n",
    "\n",
    "### Notes\n",
    "- File counting is fast (scans directory structure only).\n",
    "- Record counting can be slow (reads every CSV file).\n",
    "- Results are saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d680dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path):\n",
    "    \"\"\"\n",
    "    Calculate statistics for the LAQN dataset using the new column structure.\n",
    "    This function scans all CSV files recursively under base_dir and calculates key metrics needed for reporting.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir : Path\n",
    "            Base directory containing LAQN data folders.\n",
    "        metadata_path : Path\n",
    "            Path to the standardised metadata csv file.\n",
    "        nan_log_path : Path\n",
    "            Path to the NaN values log file after notice data flags, changed them to NaN.\n",
    "\n",
    "    Returns:\n",
    "        dict : Dictionary containing all calculated statistics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # read metadata to get station and pollutant info\n",
    "    print(\"\\nloading metadata...\")\n",
    "    metadata = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
    "\n",
    "    # calculate metadata statistics\n",
    "    stats['unique_stations'] = metadata['SiteCode'].nunique()\n",
    "    stats['total_combinations'] = len(metadata)\n",
    "    stats['unique_pollutants'] = metadata['SpeciesCode'].nunique()\n",
    "\n",
    "    # get pollutant breakdown\n",
    "    pollutant_counts = metadata['SpeciesCode'].value_counts()\n",
    "    stats['pollutant_distribution'] = pollutant_counts.to_dict()\n",
    "\n",
    "    # create set of expected (SiteCode, SpeciesCode) pairs from metadata\n",
    "    expected_pairs = set(\n",
    "        zip(metadata['SiteCode'], metadata['SpeciesCode'])\n",
    "    )\n",
    "    stats['expected_pairs'] = len(expected_pairs)\n",
    "    print(f\"  expected SiteCode/SpeciesCode pairs from metadata: {len(expected_pairs)}\")\n",
    "\n",
    "    # count unique coordinates for spatial coverage\n",
    "    unique_coords = metadata[['Latitude', 'Longitude']].drop_duplicates()\n",
    "    stats['unique_locations'] = len(unique_coords)\n",
    "\n",
    "    # Scan all CSVs in all subfolders under base_dir\n",
    "    print(\"\\nscanning optimased directory for collected data...\")\n",
    "    all_csv_files = list(Path(base_dir).rglob('*.csv'))\n",
    "    total_files = len(all_csv_files)\n",
    "    print(f\"\\nTotal CSV files found: {total_files}\")\n",
    "    stats['total_files'] = total_files\n",
    "\n",
    "    # Count files, records, and missing values by period (e.g., \"2023_apr\")\n",
    "    files_by_period = defaultdict(int)\n",
    "    records_by_period = defaultdict(int)\n",
    "    missing_by_period = defaultdict(int)\n",
    "\n",
    "    all_csvs = []\n",
    "    total_records = 0\n",
    "    total_missing = 0\n",
    "\n",
    "    print(\"\\nReading all CSV files to calculate statistics...\")\n",
    "    for csv_file in all_csv_files:\n",
    "        period = csv_file.parent.name\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            n_records = len(df)\n",
    "            n_missing = df['@Value'].isna().sum() + (df['@Value'] == \"\").sum() if '@Value' in df.columns else 0\n",
    "            all_csvs.append(df)\n",
    "            files_by_period[period] += 1\n",
    "            records_by_period[period] += n_records\n",
    "            missing_by_period[period] += n_missing\n",
    "            total_records += n_records\n",
    "            total_missing += n_missing\n",
    "        except Exception as e:\n",
    "            print(f\"  warning: could not read {csv_file.name}: {e}\")\n",
    "\n",
    "    stats['files_by_period'] = dict(files_by_period)\n",
    "    stats['records_by_period'] = dict(records_by_period)\n",
    "    stats['missing_by_period'] = dict(missing_by_period)\n",
    "    stats['total_records'] = total_records\n",
    "    stats['total_missing'] = total_missing\n",
    "    stats['overall_completeness'] = ((total_records - total_missing) / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "    for period in files_by_period:\n",
    "        rec = records_by_period[period]\n",
    "        miss = missing_by_period[period]\n",
    "        miss_pct = (miss / rec * 100) if rec > 0 else 0\n",
    "        print(f\"  {period}: {files_by_period[period]} files, {rec:,} records, {miss:,} missing ({miss_pct:.2f}%)\")\n",
    "\n",
    "    # cross-reference metadata with collected data\n",
    "    print(\"\\ncross-referencing collected data with metadata...\")\n",
    "\n",
    "    if all_csvs:\n",
    "        all_data = pd.concat(all_csvs, ignore_index=True)\n",
    "\n",
    "        # check if required columns exist in csv files\n",
    "        if 'SiteCode' in all_data.columns and 'SpeciesCode' in all_data.columns:\n",
    "            # identify actual (SiteCode, SpeciesCode) pairs in collected data\n",
    "            collected_pairs = set(\n",
    "                zip(all_data['SiteCode'], all_data['SpeciesCode'])\n",
    "            )\n",
    "            stats['collected_pairs'] = len(collected_pairs)\n",
    "\n",
    "            # find missing pairs (in metadata but not in collected data)\n",
    "            missing_pairs = expected_pairs - collected_pairs\n",
    "            stats['missing_pairs'] = list(missing_pairs)\n",
    "            stats['missing_pairs_count'] = len(missing_pairs)\n",
    "\n",
    "            # find extra pairs (in collected data but not in metadata)\n",
    "            extra_pairs = collected_pairs - expected_pairs\n",
    "            stats['extra_pairs'] = list(extra_pairs)\n",
    "            stats['extra_pairs_count'] = len(extra_pairs)\n",
    "\n",
    "            print(f\"  expected pairs from metadata: {len(expected_pairs)}\")\n",
    "            print(f\"  actually collected pairs: {len(collected_pairs)}\")\n",
    "            print(f\"  missing pairs (in metadata but not collected): {len(missing_pairs)}\")\n",
    "            print(f\"  extra pairs (collected but not in metadata): {len(extra_pairs)}\")\n",
    "\n",
    "            # group by SiteCode and SpeciesCode, count missing values\n",
    "            missing_breakdown = {}\n",
    "            for (site, species), group in all_data.groupby(['SiteCode', 'SpeciesCode']):\n",
    "                total_rows = len(group)\n",
    "                if '@Value' in group.columns:\n",
    "                    missing_rows = group['@Value'].isna().sum() + (group['@Value'] == \"\").sum()\n",
    "                else:\n",
    "                    missing_rows = 0\n",
    "                missing_breakdown[(site, species)] = (int(missing_rows), int(total_rows))\n",
    "            stats['missing_by_station_pollutant'] = missing_breakdown\n",
    "        else:\n",
    "            print(\"  warning: SiteCode or SpeciesCode columns not found\")\n",
    "            stats['missing_by_station_pollutant'] = {}\n",
    "            stats['collected_pairs'] = 0\n",
    "            stats['missing_pairs'] = []\n",
    "            stats['missing_pairs_count'] = 0\n",
    "            stats['extra_pairs'] = []\n",
    "            stats['extra_pairs_count'] = 0\n",
    "    else:\n",
    "        stats['missing_by_station_pollutant'] = {}\n",
    "        stats['collected_pairs'] = 0\n",
    "        stats['missing_pairs'] = list(expected_pairs)\n",
    "        stats['missing_pairs_count'] = len(expected_pairs)\n",
    "        stats['extra_pairs'] = []\n",
    "        stats['extra_pairs_count'] = 0\n",
    "\n",
    "    # distribution of nan by pollutant over time\n",
    "    if stats['missing_by_station_pollutant']:\n",
    "        pollutant_missing_summary = {}\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            if species not in pollutant_missing_summary:\n",
    "                pollutant_missing_summary[species] = {'total_missing': 0, 'total_records': 0}\n",
    "            pollutant_missing_summary[species]['total_missing'] += missing\n",
    "            pollutant_missing_summary[species]['total_records'] += total\n",
    "        for species in pollutant_missing_summary:\n",
    "            total_missing = pollutant_missing_summary[species]['total_missing']\n",
    "            total_records = pollutant_missing_summary[species]['total_records']\n",
    "            percentage = (total_missing / total_records * 100) if total_records > 0 else 0\n",
    "            pollutant_missing_summary[species]['percentage_missing'] = percentage\n",
    "        stats['missing_by_pollutant_type'] = pollutant_missing_summary\n",
    "    else:\n",
    "        stats['missing_by_pollutant_type'] = {}\n",
    "\n",
    "    # log file created during data cleaning process\n",
    "    if Path(nan_log_path).exists():\n",
    "        nan_log = pd.read_csv(nan_log_path)\n",
    "        replacements_by_year = nan_log.groupby('year_folder')['invalid_flags_replaced'].sum().to_dict()\n",
    "        stats['nan_replacements_by_year'] = replacements_by_year\n",
    "        stats['total_nan_replacements'] = nan_log['invalid_flags_replaced'].sum()\n",
    "        stats['mean_invalid_percentage'] = nan_log['percentage_invalid'].mean()\n",
    "        stats['max_invalid_percentage'] = nan_log['percentage_invalid'].max()\n",
    "    else:\n",
    "        stats['nan_replacements_by_year'] = {}\n",
    "        stats['total_nan_replacements'] = 0\n",
    "        stats['mean_invalid_percentage'] = 0\n",
    "        stats['max_invalid_percentage'] = 0\n",
    "\n",
    "    # calculate temporal coverage based on the files collected\n",
    "    stats['temporal_coverage'] = {\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2025-11-19',\n",
    "        'total_months': 35\n",
    "    }\n",
    "\n",
    "\n",
    "    def extract_year(period):\n",
    "        # period is usually like '2023_apr' or '2024_jan'\n",
    "        return str(period)[:4] if len(str(period)) >= 4 and str(period)[:4].isdigit() else 'unknown'\n",
    "\n",
    "    files_by_year = defaultdict(int)\n",
    "    records_by_year = defaultdict(int)\n",
    "    missing_by_year = defaultdict(int)\n",
    "    for period in files_by_period:\n",
    "        year = extract_year(period)\n",
    "        files_by_year[year] += files_by_period[period]\n",
    "        records_by_year[year] += records_by_period[period]\n",
    "        missing_by_year[year] += missing_by_period[period]\n",
    "    stats['files_by_year'] = dict(files_by_year)\n",
    "    stats['records_by_year'] = dict(records_by_year)\n",
    "    stats['missing_by_year'] = dict(missing_by_year)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e2a9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(stats):\n",
    "    \"\"\"\n",
    "    Print dataset statistics for LAQN using new column structure.\n",
    "\n",
    "    Parameters:\n",
    "        stats : dict\n",
    "            returned by get_laqn_dataset_statistics().\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"LAQN dataset statistics: initial assessment\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    print(\"\\nScale and scope:\")\n",
    "    print(f\"Total files collected: {stats['total_files']:,}\")\n",
    "    print(f\"Total measurement records: {stats['total_records']:,}\")\n",
    "    print(f\"Total missing values (@Value): {stats['total_missing']:,}\")\n",
    "    print(f\"Overall completeness: {stats['overall_completeness']:.2f}%\")\n",
    "    print(f\"Unique monitoring sites (SiteCode): {stats['unique_stations']}\")\n",
    "    print(f\"Total site-species combinations: {stats['total_combinations']}\")\n",
    "    print(f\"Unique pollutant types (SpeciesCode): {stats['unique_pollutants']}\")\n",
    "    print(f\"Unique geographic locations: {stats['unique_locations']}\")\n",
    "\n",
    "    # data collection coverage\n",
    "    print(\"\\nData collection coverage:\")\n",
    "    print(f\"Expected SiteCode/SpeciesCode pairs (from metadata): {stats.get('expected_pairs', 0)}\")\n",
    "    print(f\"Actually collected pairs: {stats.get('collected_pairs', 0)}\")\n",
    "    print(f\"Missing pairs (not collected): {stats.get('missing_pairs_count', 0)}\")\n",
    "    print(f\"Extra pairs (not in metadata): {stats.get('extra_pairs_count', 0)}\")\n",
    "\n",
    "    if stats.get('missing_pairs_count', 0) > 0:\n",
    "        print(f\"\\nWarning: {stats['missing_pairs_count']} SiteCode/SpeciesCode pairs from metadata were not found in collected data.\")\n",
    "        print(\"First 10 missing pairs:\")\n",
    "        for i, (site, species) in enumerate(stats['missing_pairs'][:10], 1):\n",
    "            print(f\"  {i}. {site} - {species}\")\n",
    "\n",
    "    if stats.get('extra_pairs_count', 0) > 0:\n",
    "        print(f\"\\nNote: {stats['extra_pairs_count']} SiteCode/SpeciesCode pairs in collected data are not in metadata.\")\n",
    "\n",
    "    print(\"\\nFiles by year:\")\n",
    "    for year, count in stats['files_by_year'].items():\n",
    "        print(f\"  {year}: {count:,} files\")\n",
    "\n",
    "    print(\"\\nRecords by year:\")\n",
    "    for year, count in stats['records_by_year'].items():\n",
    "        missing = stats['missing_by_year'].get(year, 0)\n",
    "        missing_pct = (missing / count * 100) if count > 0 else 0\n",
    "        print(f\"  {year}: {count:,} records, {missing:,} missing ({missing_pct:.2f}%)\")\n",
    "\n",
    "    # adding nan value summary below\n",
    "    print(\"\\nNaN replacement summary:\")\n",
    "    print(f\"Total invalid flags replaced: {stats['total_nan_replacements']:,}\")\n",
    "    print(f\"Mean invalid percentage per file: {stats['mean_invalid_percentage']:.2f}%\")\n",
    "    print(f\"Max invalid percentage: {stats['max_invalid_percentage']:.2f}%\")\n",
    "\n",
    "    # count of replacements by year\n",
    "    if stats['nan_replacements_by_year']:\n",
    "        print(\"\\nReplacements by year:\")\n",
    "        for year_folder, count in stats['nan_replacements_by_year'].items():\n",
    "            print(f\"  {year_folder}: {count:,} flags replaced\")\n",
    "\n",
    "    print(\"\\nTemporal coverage:\")\n",
    "    print(f\"Start date: {stats['temporal_coverage']['start_date']}\")\n",
    "    print(f\"End date: {stats['temporal_coverage']['end_date']}\")\n",
    "    print(f\"Total months: {stats['temporal_coverage']['total_months']}\")\n",
    "\n",
    "    print(\"\\nPollutant (SpeciesCode) distribution:\")\n",
    "    print(\"Site/species combinations by type:\")\n",
    "    for species, count in sorted(stats['pollutant_distribution'].items(),\n",
    "                                 key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_combinations']) * 100\n",
    "        print(f\"  {species}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # missing value distribution by pollutant type\n",
    "    print(\"\\nMissing value distribution by pollutant type (SpeciesCode):\")\n",
    "    if stats.get('missing_by_pollutant_type'):\n",
    "        # sort by percentage missing (highest first)\n",
    "        sorted_species = sorted(\n",
    "            stats['missing_by_pollutant_type'].items(),\n",
    "            key=lambda x: x[1]['percentage_missing'],\n",
    "            reverse=True\n",
    "        )\n",
    "        print(f\"{'SpeciesCode':<20} {'total records':>15} {'missing':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for species, data in sorted_species:\n",
    "            print(f\"{species:<20} {data['total_records']:>15,} {data['total_missing']:>12,} {data['percentage_missing']:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value distribution available.\")\n",
    "\n",
    "    # print missing values by site/species breakdown with row_number column\n",
    "    print(\"\\nMissing values by site/species (SiteCode/SpeciesCode):\")\n",
    "    if stats.get('missing_by_station_pollutant'):\n",
    "        # prepare a sorted list by missing percentage descending\n",
    "        breakdown = []\n",
    "        for (site, species), (missing, total) in stats['missing_by_station_pollutant'].items():\n",
    "            percent = (missing / total * 100) if total > 0 else 0\n",
    "            breakdown.append((site, species, missing, total, percent))\n",
    "        # sort by percentage descending and take top 20\n",
    "        breakdown.sort(key=lambda x: x[4], reverse=True)\n",
    "        breakdown = breakdown[:20]\n",
    "        print(f\"{'SiteCode':<20} {'SpeciesCode':<20} {'missing':>10} {'total_row':>12} {'% missing':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for site, species, missing, total, percent in breakdown:\n",
    "            print(f\"{site:<20} {species:<20} {missing:>10,} {total:>12,} {percent:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"  No missing value breakdown available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "602d9919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading metadata...\n",
      "  expected SiteCode/SpeciesCode pairs from metadata: 170\n",
      "\n",
      "scanning optimased directory for collected data...\n",
      "\n",
      "Total CSV files found: 4932\n",
      "\n",
      "Reading all CSV files to calculate statistics...\n",
      "  2023_mar: 141 files, 101,520 records, 14,883 missing (14.66%)\n",
      "  2025_feb: 141 files, 91,368 records, 12,034 missing (13.17%)\n",
      "  2024_feb: 141 files, 94,752 records, 9,582 missing (10.11%)\n",
      "  2025_aug: 141 files, 101,520 records, 16,123 missing (15.88%)\n",
      "  2024_aug: 141 files, 101,520 records, 19,425 missing (19.13%)\n",
      "  2025_mar: 141 files, 101,520 records, 15,384 missing (15.15%)\n",
      "  2023_feb: 141 files, 91,368 records, 12,838 missing (14.05%)\n",
      "  2024_mar: 141 files, 101,520 records, 11,279 missing (11.11%)\n",
      "  2023_aug: 141 files, 101,520 records, 11,360 missing (11.19%)\n",
      "  2024_jul: 141 files, 101,520 records, 12,934 missing (12.74%)\n",
      "  2025_jul: 141 files, 101,520 records, 16,506 missing (16.26%)\n",
      "  2024_oct: 141 files, 101,520 records, 11,079 missing (10.91%)\n",
      "  2023_sep: 141 files, 98,136 records, 11,727 missing (11.95%)\n",
      "  2025_oct: 141 files, 101,520 records, 12,342 missing (12.16%)\n",
      "  2023_jan: 141 files, 101,520 records, 17,911 missing (17.64%)\n",
      "  2023_jul: 141 files, 101,520 records, 11,160 missing (10.99%)\n",
      "  2024_jan: 141 files, 101,520 records, 10,375 missing (10.22%)\n",
      "  2025_sep: 141 files, 98,136 records, 18,344 missing (18.69%)\n",
      "  2024_sep: 141 files, 98,136 records, 15,469 missing (15.76%)\n",
      "  2023_oct: 141 files, 101,520 records, 13,964 missing (13.75%)\n",
      "  2025_jan: 141 files, 101,520 records, 11,198 missing (11.03%)\n",
      "  2024_dec: 141 files, 101,520 records, 9,421 missing (9.28%)\n",
      "  2024_apr: 141 files, 98,136 records, 11,208 missing (11.42%)\n",
      "  2024_nov: 141 files, 98,136 records, 9,063 missing (9.24%)\n",
      "  2023_may: 141 files, 101,520 records, 12,641 missing (12.45%)\n",
      "  2025_nov: 141 files, 60,912 records, 8,613 missing (14.14%)\n",
      "  2025_apr: 141 files, 98,136 records, 10,606 missing (10.81%)\n",
      "  2024_may: 141 files, 101,520 records, 12,839 missing (12.65%)\n",
      "  2025_may: 141 files, 101,520 records, 10,843 missing (10.68%)\n",
      "  2023_nov: 138 files, 96,048 records, 10,851 missing (11.30%)\n",
      "  2023_apr: 141 files, 98,136 records, 10,992 missing (11.20%)\n",
      "  2023_dec: 141 files, 101,520 records, 14,882 missing (14.66%)\n",
      "  2025_jun: 141 files, 98,136 records, 14,018 missing (14.28%)\n",
      "  2024_jun: 141 files, 98,136 records, 9,661 missing (9.84%)\n",
      "  2023_jun: 141 files, 98,136 records, 12,103 missing (12.33%)\n",
      "\n",
      "cross-referencing collected data with metadata...\n",
      "  expected pairs from metadata: 170\n",
      "  actually collected pairs: 141\n",
      "  missing pairs (in metadata but not collected): 53\n",
      "  extra pairs (collected but not in metadata): 24\n",
      "\n",
      "========================================\n",
      "LAQN dataset statistics: initial assessment\n",
      "========================================\n",
      "\n",
      "Scale and scope:\n",
      "Total files collected: 4,932\n",
      "Total measurement records: 3,446,208\n",
      "Total missing values (@Value): 443,658\n",
      "Overall completeness: 87.13%\n",
      "Unique monitoring sites (SiteCode): 78\n",
      "Total site-species combinations: 173\n",
      "Unique pollutant types (SpeciesCode): 6\n",
      "Unique geographic locations: 76\n",
      "\n",
      "Data collection coverage:\n",
      "Expected SiteCode/SpeciesCode pairs (from metadata): 170\n",
      "Actually collected pairs: 141\n",
      "Missing pairs (not collected): 53\n",
      "Extra pairs (not in metadata): 24\n",
      "\n",
      "Warning: 53 SiteCode/SpeciesCode pairs from metadata were not found in collected data.\n",
      "First 10 missing pairs:\n",
      "  1. BL0 - PM25\n",
      "  2. TH4 - PM25\n",
      "  3. BT6 - PM25\n",
      "  4. MEB - PM25\n",
      "  5. GN6 - PM25\n",
      "  6. GR8 - PM25\n",
      "  7. GN3 - PM25\n",
      "  8. TL6 - PM25\n",
      "  9. GT1 - PM25\n",
      "  10. CE3 - PM25\n",
      "\n",
      "Note: 24 SiteCode/SpeciesCode pairs in collected data are not in metadata.\n",
      "\n",
      "Files by year:\n",
      "  2023: 1,689 files\n",
      "  2025: 1,551 files\n",
      "  2024: 1,692 files\n",
      "\n",
      "Records by year:\n",
      "  2023: 1,192,464 records, 155,312 missing (13.02%)\n",
      "  2025: 1,055,808 records, 146,011 missing (13.83%)\n",
      "  2024: 1,197,936 records, 142,335 missing (11.88%)\n",
      "\n",
      "NaN replacement summary:\n",
      "Total invalid flags replaced: 0\n",
      "Mean invalid percentage per file: 0.00%\n",
      "Max invalid percentage: 0.00%\n",
      "\n",
      "Temporal coverage:\n",
      "Start date: 2023-01-01\n",
      "End date: 2025-11-19\n",
      "Total months: 35\n",
      "\n",
      "Pollutant (SpeciesCode) distribution:\n",
      "Site/species combinations by type:\n",
      "  NO2: 60 (34.7%)\n",
      "  PM25: 53 (30.6%)\n",
      "  PM10: 43 (24.9%)\n",
      "  O3: 11 (6.4%)\n",
      "  SO2: 4 (2.3%)\n",
      "  CO: 2 (1.2%)\n",
      "\n",
      "Missing value distribution by pollutant type (SpeciesCode):\n",
      "SpeciesCode            total records      missing    % missing\n",
      "------------------------------------------------------------\n",
      "O3                           268,320       47,056       17.54%\n",
      "PM2.5                        586,944      100,755       17.17%\n",
      "SO2                           97,824       15,803       16.15%\n",
      "PM10                       1,026,456      126,749       12.35%\n",
      "NO2                        1,417,752      148,803       10.50%\n",
      "CO                            48,912        4,492        9.18%\n",
      "\n",
      "Missing values by site/species (SiteCode/SpeciesCode):\n",
      "SiteCode             SpeciesCode             missing    total_row    % missing\n",
      "------------------------------------------------------------\n",
      "WM6                  PM10                     15,357       24,456       62.79%\n",
      "CE3                  NO2                      11,394       24,456       46.59%\n",
      "TL4                  NO2                       9,869       24,456       40.35%\n",
      "RI2                  O3                        9,732       24,456       39.79%\n",
      "WA7                  NO2                       9,236       24,456       37.77%\n",
      "BG1                  SO2                       8,373       24,456       34.24%\n",
      "TH4                  PM2.5                     8,042       24,456       32.88%\n",
      "CD1                  PM10                      7,711       24,456       31.53%\n",
      "WAA                  NO2                       7,697       24,456       31.47%\n",
      "CE3                  PM10                      7,652       24,456       31.29%\n",
      "CE3                  PM2.5                     7,652       24,456       31.29%\n",
      "TH4                  PM10                      7,457       24,456       30.49%\n",
      "CD1                  PM2.5                     7,339       24,456       30.01%\n",
      "GN6                  PM2.5                     7,240       24,456       29.60%\n",
      "CR8                  PM2.5                     7,207       24,456       29.47%\n",
      "GN0                  PM2.5                     7,099       24,456       29.03%\n",
      "HG4                  O3                        7,092       24,456       29.00%\n",
      "CD1                  NO2                       6,984       24,456       28.56%\n",
      "BT5                  PM2.5                     6,783       24,456       27.74%\n",
      "MY1                  O3                        6,634       24,456       27.13%\n",
      "\n",
      "Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/laqn_stats.csv\n",
      "Species (pollutant) distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/pollutant_distribution.csv\n",
      "Missing value distribution by species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_pollutant.csv\n",
      "Missing values by site/species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_station_pollutant.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "stats = get_laqn_dataset_statistics(base_dir, metadata_path, nan_log_path)\n",
    "print_dataset_statistics(stats)\n",
    "\n",
    "# Save statistics for later use as csv\n",
    "# Prepare flat data structure for csv\n",
    "stats_rows = []\n",
    "stats_rows.append([\"metric\", \"value\"])\n",
    "stats_rows.append([\"total_files\", stats['total_files']])\n",
    "stats_rows.append([\"total_records\", stats['total_records']])\n",
    "stats_rows.append([\"total_missing\", stats['total_missing']])\n",
    "stats_rows.append([\"overall_completeness_pct\", f\"{stats['overall_completeness']:.2f}\"])\n",
    "stats_rows.append([\"unique_sites\", stats['unique_stations']])\n",
    "stats_rows.append([\"total_site_species_combinations\", stats['total_combinations']])\n",
    "stats_rows.append([\"unique_species\", stats['unique_pollutants']])\n",
    "stats_rows.append([\"unique_locations\", stats['unique_locations']])\n",
    "stats_rows.append([\"expected_site_species_pairs\", stats.get('expected_pairs', 0)])\n",
    "stats_rows.append([\"collected_site_species_pairs\", stats.get('collected_pairs', 0)])\n",
    "stats_rows.append([\"missing_site_species_pairs_count\", stats.get('missing_pairs_count', 0)])\n",
    "stats_rows.append([\"extra_site_species_pairs_count\", stats.get('extra_pairs_count', 0)])\n",
    "stats_rows.append([\"total_nan_replacements\", stats['total_nan_replacements']])\n",
    "stats_rows.append([\"mean_invalid_pct\", f\"{stats['mean_invalid_percentage']:.2f}\"])\n",
    "stats_rows.append([\"max_invalid_pct\", f\"{stats['max_invalid_percentage']:.2f}\"])\n",
    "\n",
    "# Add year-specific metrics\n",
    "for year in ['2023', '2024', '2025']:\n",
    "    stats_rows.append([f\"files_{year}\", stats['files_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"records_{year}\", stats['records_by_year'].get(year, 0)])\n",
    "    stats_rows.append([f\"missing_{year}\", stats['missing_by_year'].get(year, 0)])\n",
    "    year_key = f'{year}measurements'\n",
    "    stats_rows.append([f\"replacements_{year}\", stats['nan_replacements_by_year'].get(year_key, 0)])\n",
    "\n",
    "# Save to csv stats report\n",
    "pd.DataFrame(stats_rows[1:], columns=stats_rows[0]).to_csv(stats_output_path, index=False)\n",
    "print(f\"\\nStatistics saved to: {stats_output_path}\")\n",
    "\n",
    "# Save species (pollutant) distribution to csv\n",
    "total_combinations = stats['total_combinations']\n",
    "species_distribution_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'count': v,\n",
    "            'percentage': round((v / total_combinations) * 100, 2) if total_combinations > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['pollutant_distribution'].items()\n",
    "    ]\n",
    ")\n",
    "species_distribution_df.to_csv(pollutant_distrubution_path, index=False)\n",
    "print(f\"Species (pollutant) distribution saved to: {pollutant_distrubution_path}\")\n",
    "\n",
    "# Save missing value distribution by species to csv\n",
    "if stats.get('missing_by_pollutant_type'):\n",
    "    missing_by_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SpeciesCode': k,\n",
    "            'total_records': v['total_records'],\n",
    "            'total_missing': v['total_missing'],\n",
    "            'percentage_missing': v['percentage_missing']\n",
    "        }\n",
    "        for k, v in stats['missing_by_pollutant_type'].items()\n",
    "    ])\n",
    "    missing_by_species_df.to_csv(nan_val_pollutant_split_path, index=False)\n",
    "    print(f\"Missing value distribution by species saved to: {nan_val_pollutant_split_path}\")\n",
    "\n",
    "# Save missing values by site/species to csv\n",
    "if stats.get('missing_by_station_pollutant'):\n",
    "    missing_by_site_species_df = pd.DataFrame([\n",
    "        {\n",
    "            'SiteCode': k[0],\n",
    "            'SpeciesCode': k[1],\n",
    "            'missing': v[0],\n",
    "            'total_row': v[1],\n",
    "            'percentage_missing': (v[0] / v[1] * 100) if v[1] > 0 else 0\n",
    "        }\n",
    "        for k, v in stats['missing_by_station_pollutant'].items()\n",
    "    ])\n",
    "    missing_by_site_species_df.to_csv(nan_val_stationPollutant_path, index=False)\n",
    "    print(f\"Missing values by site/species saved to: {nan_val_stationPollutant_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bd249",
   "metadata": {},
   "source": [
    "    loading metadata...\n",
    "    expected SiteCode/SpeciesCode pairs from metadata: 170\n",
    "\n",
    "    scanning optimased directory for collected data...\n",
    "\n",
    "    Total CSV files found: 4932\n",
    "\n",
    "    Reading all CSV files to calculate statistics...\n",
    "    2023_mar: 141 files, 101,520 records, 14,883 missing (14.66%)\n",
    "    2025_feb: 141 files, 91,368 records, 12,034 missing (13.17%)\n",
    "    2024_feb: 141 files, 94,752 records, 9,582 missing (10.11%)\n",
    "    2025_aug: 141 files, 101,520 records, 16,123 missing (15.88%)\n",
    "    2024_aug: 141 files, 101,520 records, 19,425 missing (19.13%)\n",
    "    2025_mar: 141 files, 101,520 records, 15,384 missing (15.15%)\n",
    "    2023_feb: 141 files, 91,368 records, 12,838 missing (14.05%)\n",
    "    2024_mar: 141 files, 101,520 records, 11,279 missing (11.11%)\n",
    "    2023_aug: 141 files, 101,520 records, 11,360 missing (11.19%)\n",
    "    2024_jul: 141 files, 101,520 records, 12,934 missing (12.74%)\n",
    "    2025_jul: 141 files, 101,520 records, 16,506 missing (16.26%)\n",
    "    2024_oct: 141 files, 101,520 records, 11,079 missing (10.91%)\n",
    "    2023_sep: 141 files, 98,136 records, 11,727 missing (11.95%)\n",
    "    2025_oct: 141 files, 101,520 records, 12,342 missing (12.16%)\n",
    "    2023_jan: 141 files, 101,520 records, 17,911 missing (17.64%)\n",
    "    2023_jul: 141 files, 101,520 records, 11,160 missing (10.99%)\n",
    "    2024_jan: 141 files, 101,520 records, 10,375 missing (10.22%)\n",
    "    2025_sep: 141 files, 98,136 records, 18,344 missing (18.69%)\n",
    "    2024_sep: 141 files, 98,136 records, 15,469 missing (15.76%)\n",
    "    2023_oct: 141 files, 101,520 records, 13,964 missing (13.75%)\n",
    "    2025_jan: 141 files, 101,520 records, 11,198 missing (11.03%)\n",
    "    2024_dec: 141 files, 101,520 records, 9,421 missing (9.28%)\n",
    "    2024_apr: 141 files, 98,136 records, 11,208 missing (11.42%)\n",
    "    2024_nov: 141 files, 98,136 records, 9,063 missing (9.24%)\n",
    "    2023_may: 141 files, 101,520 records, 12,641 missing (12.45%)\n",
    "    2025_nov: 141 files, 60,912 records, 8,613 missing (14.14%)\n",
    "    2025_apr: 141 files, 98,136 records, 10,606 missing (10.81%)\n",
    "    2024_may: 141 files, 101,520 records, 12,839 missing (12.65%)\n",
    "    2025_may: 141 files, 101,520 records, 10,843 missing (10.68%)\n",
    "    2023_nov: 138 files, 96,048 records, 10,851 missing (11.30%)\n",
    "    2023_apr: 141 files, 98,136 records, 10,992 missing (11.20%)\n",
    "    2023_dec: 141 files, 101,520 records, 14,882 missing (14.66%)\n",
    "    2025_jun: 141 files, 98,136 records, 14,018 missing (14.28%)\n",
    "    2024_jun: 141 files, 98,136 records, 9,661 missing (9.84%)\n",
    "    2023_jun: 141 files, 98,136 records, 12,103 missing (12.33%)\n",
    "\n",
    "    cross-referencing collected data with metadata...\n",
    "    expected pairs from metadata: 170\n",
    "    actually collected pairs: 141\n",
    "    missing pairs (in metadata but not collected): 53\n",
    "    extra pairs (collected but not in metadata): 24\n",
    "\n",
    "    ========================================\n",
    "    LAQN dataset statistics: initial assessment\n",
    "    ========================================\n",
    "\n",
    "    Scale and scope:\n",
    "    Total files collected: 4,932\n",
    "    Total measurement records: 3,446,208\n",
    "    Total missing values (@Value): 443,658\n",
    "    Overall completeness: 87.13%\n",
    "    Unique monitoring sites (SiteCode): 78\n",
    "    Total site-species combinations: 173\n",
    "    Unique pollutant types (SpeciesCode): 6\n",
    "    Unique geographic locations: 76\n",
    "\n",
    "    Data collection coverage:\n",
    "    Expected SiteCode/SpeciesCode pairs (from metadata): 170\n",
    "    Actually collected pairs: 141\n",
    "    Missing pairs (not collected): 53\n",
    "    Extra pairs (not in metadata): 24\n",
    "\n",
    "    Warning: 53 SiteCode/SpeciesCode pairs from metadata were not found in collected data.\n",
    "    First 10 missing pairs:\n",
    "    1. BL0 - PM25\n",
    "    2. TH4 - PM25\n",
    "    3. BT6 - PM25\n",
    "    4. MEB - PM25\n",
    "    5. GN6 - PM25\n",
    "    6. GR8 - PM25\n",
    "    7. GN3 - PM25\n",
    "    8. TL6 - PM25\n",
    "    9. GT1 - PM25\n",
    "    10. CE3 - PM25\n",
    "\n",
    "    Note: 24 SiteCode/SpeciesCode pairs in collected data are not in metadata.\n",
    "\n",
    "    Files by year:\n",
    "    2023: 1,689 files\n",
    "    2025: 1,551 files\n",
    "    2024: 1,692 files\n",
    "\n",
    "    Records by year:\n",
    "    2023: 1,192,464 records, 155,312 missing (13.02%)\n",
    "    2025: 1,055,808 records, 146,011 missing (13.83%)\n",
    "    2024: 1,197,936 records, 142,335 missing (11.88%)\n",
    "\n",
    "    NaN replacement summary:\n",
    "    Total invalid flags replaced: 0\n",
    "    Mean invalid percentage per file: 0.00%\n",
    "    Max invalid percentage: 0.00%\n",
    "\n",
    "    Temporal coverage:\n",
    "    Start date: 2023-01-01\n",
    "    End date: 2025-11-19\n",
    "    Total months: 35\n",
    "\n",
    "    Pollutant (SpeciesCode) distribution:\n",
    "    Site/species combinations by type:\n",
    "    NO2: 60 (34.7%)\n",
    "    PM25: 53 (30.6%)\n",
    "    PM10: 43 (24.9%)\n",
    "    O3: 11 (6.4%)\n",
    "    SO2: 4 (2.3%)\n",
    "    CO: 2 (1.2%)\n",
    "\n",
    "    Missing value distribution by pollutant type (SpeciesCode):\n",
    "    SpeciesCode            total records      missing    % missing\n",
    "    ------------------------------------------------------------\n",
    "    O3                           268,320       47,056       17.54%\n",
    "    PM2.5                        586,944      100,755       17.17%\n",
    "    SO2                           97,824       15,803       16.15%\n",
    "    PM10                       1,026,456      126,749       12.35%\n",
    "    NO2                        1,417,752      148,803       10.50%\n",
    "    CO                            48,912        4,492        9.18%\n",
    "\n",
    "    Missing values by site/species (SiteCode/SpeciesCode):\n",
    "    SiteCode             SpeciesCode             missing    total_row    % missing\n",
    "    ------------------------------------------------------------\n",
    "    WM6                  PM10                     15,357       24,456       62.79%\n",
    "    CE3                  NO2                      11,394       24,456       46.59%\n",
    "    TL4                  NO2                       9,869       24,456       40.35%\n",
    "    RI2                  O3                        9,732       24,456       39.79%\n",
    "    WA7                  NO2                       9,236       24,456       37.77%\n",
    "    BG1                  SO2                       8,373       24,456       34.24%\n",
    "    TH4                  PM2.5                     8,042       24,456       32.88%\n",
    "    CD1                  PM10                      7,711       24,456       31.53%\n",
    "    WAA                  NO2                       7,697       24,456       31.47%\n",
    "    CE3                  PM10                      7,652       24,456       31.29%\n",
    "    CE3                  PM2.5                     7,652       24,456       31.29%\n",
    "    TH4                  PM10                      7,457       24,456       30.49%\n",
    "    CD1                  PM2.5                     7,339       24,456       30.01%\n",
    "    GN6                  PM2.5                     7,240       24,456       29.60%\n",
    "    CR8                  PM2.5                     7,207       24,456       29.47%\n",
    "    GN0                  PM2.5                     7,099       24,456       29.03%\n",
    "    HG4                  O3                        7,092       24,456       29.00%\n",
    "    CD1                  NO2                       6,984       24,456       28.56%\n",
    "    BT5                  PM2.5                     6,783       24,456       27.74%\n",
    "    MY1                  O3                        6,634       24,456       27.13%\n",
    "\n",
    "    Statistics saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/laqn_stats.csv\n",
    "    Species (pollutant) distribution saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/pollutant_distribution.csv\n",
    "    Missing value distribution by species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_pollutant.csv\n",
    "    Missing values by site/species saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/nan_values_by_station_pollutant.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19af0ad",
   "metadata": {},
   "source": [
    "## 3) Data Quality validations:\n",
    "\n",
    "\n",
    "A critical gap from the laqn report by applying formal statistical tests to validate data quality patterns. While descriptive statistics show 0% (before I notice the flags of the dataset) issue rate, I need statistical evidence that this pattern is real and not due to chance.\n",
    "\n",
    "\n",
    "#### Purpuse:\n",
    " Checking data qualities if it is in the limits of eea, and make sence for general logic.\n",
    "- Outlier detection in pollutant measurements.\n",
    "- Data validity ranges based on WHO/EEA standards.\n",
    "- Measurement consistency across time periods.\n",
    "- Quality flags and suspicious patterns.\n",
    "\n",
    "### methodology\n",
    " applies environmental data quality assessment standards:\n",
    "1. Load aggregated measurement data from all csv files.\n",
    "2. Calculate statistical distributions for each pollutant type.\n",
    "3. Identify outliers using IQR method and domain knowledge.\n",
    "4. Check values against established valid ranges.\n",
    "5. Flag suspicious patterns constant values, extreme spikes.\n",
    "6. Calculate quality scores for each station-pollutant combination.\n",
    "\n",
    "#### air quality measurement standards\n",
    "\n",
    "- Uk air quality objectives, limits and policy.\n",
    "- https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://uk-air.defra.gov.uk/assets/documents/Air_Quality_Objectives_Update_20230403.pdf\n",
    "\n",
    "- DEFRA. (2023). *Air Pollution in the UK 2022*.\n",
    "  - Source: https://uk-air.defra.gov.uk/library/annualreport/\n",
    "  - Air Quality Objectives and limit values\n",
    "  - Compliance assessment methodology\n",
    "\n",
    "- UK Air Information Resource. (2024). *Air Pollution: UK Limits*.\n",
    "  - Source: https://uk-air.defra.gov.uk/air-pollution/uk-limits\n",
    "  - Current UK air quality objectives\n",
    "  - Legal limit values and target dates\n",
    "  - Measurement unit specifications (µg/m³)\n",
    "\n",
    "  -  for the rest of the pollutants\n",
    "\n",
    "\n",
    "- uk voc policy:\n",
    "  - https://assets.publishing.service.gov.uk/media/5d7a2912ed915d522e4164a5/VO__statement_Final_12092019_CS__1_.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_negative_values(base_dir, dry_run=True):\n",
    "    summary = []\n",
    "    all_csvs = list(Path(base_dir).rglob(\"*.csv\"))\n",
    "    for csv_file in all_csvs:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if '@Value' in df.columns and 'SpeciesCode' in df.columns and 'SiteCode' in df.columns:\n",
    "                df_valid = df[pd.to_numeric(df['@Value'], errors='coerce').notnull()].copy()\n",
    "                df_valid['@Value'] = df_valid['@Value'].astype(float)\n",
    "                negatives = df_valid[df_valid['@Value'] < 0]\n",
    "                if not negatives.empty:\n",
    "                    grouped = negatives.groupby(['SpeciesCode', 'SiteCode']).size().reset_index(name='neg_count')\n",
    "                    for _, row in grouped.iterrows():\n",
    "                        total = df_valid[(df_valid['SpeciesCode'] == row['SpeciesCode']) & (df_valid['SiteCode'] == row['SiteCode'])].shape[0]\n",
    "                        percent = (row['neg_count'] / total * 100) if total > 0 else 0\n",
    "                        summary.append({\n",
    "                            'SpeciesCode': row['SpeciesCode'],\n",
    "                            'SiteCode': row['SiteCode'],\n",
    "                            'NegativeCount': row['neg_count'],\n",
    "                            'TotalCount': total,\n",
    "                            'PercentNegative': round(percent, 2),\n",
    "                            'File': str(csv_file)\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process {csv_file}: {e}\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    if not summary_df.empty:\n",
    "        print(\"Negative value summary (by pollutant and site):\")\n",
    "        display(summary_df)\n",
    "    else:\n",
    "        print(\"No negative values found in the scanned files.\")\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecd274c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative values found in the scanned files.\n"
     ]
    }
   ],
   "source": [
    "#  Example usage:\n",
    "neg_summary = find_negative_values(base_dir)\n",
    "neg_summary.to_csv(\"laqn_negative_value_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfdc10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negatives_with_nan(base_dir):\n",
    "    base_dir = Path(base_dir)\n",
    "    for csv_file in base_dir.rglob('*.csv'):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if '@Value' in df.columns:\n",
    "            df['@Value'] = pd.to_numeric(df['@Value'], errors='coerce')\n",
    "            df.loc[df['@Value'] < 0, '@Value'] = np.nan\n",
    "            df.to_csv(csv_file, index=False)\n",
    "        else:\n",
    "            print(f\"Skipped (no @Value column): {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e803925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "replace_negatives_with_nan(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89d21ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_metrics(base_dir, csv_output_path):\n",
    "    \"\"\"\n",
    "    validates laqn measurements against uk air quality objectives with averaging periods.\n",
    "    \n",
    "    - path: data/laqn/optimased/YYYY_month/SiteCode_SpeciesCode_YYYY-MM-DD_YYYY-MM-DD.csv\n",
    "    - columns: @MeasurementDateGMT, @Value, SpeciesCode, SiteCode, SpeciesName, SiteName, SiteType, Latitude, Longitude\n",
    "    \n",
    "    parameters:\n",
    "        base_dir : path to laqn optimased directory (e.g., data/laqn/optimased/)\n",
    "        uk_limits_path : path to uk_pollutant_limits.csv from parsed pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    if not Path(csv_output_path).exists():\n",
    "        print(f\"error: uk limits file not found at {csv_output_path}\")\n",
    "        return {}\n",
    "    \n",
    "    # load uk legal limits from parsed pdf\n",
    "    uk_limits = pd.read_csv(csv_output_path, encoding=\"utf-8\")\n",
    "    uk_limits.columns = [col.strip().replace(' ', '_') for col in uk_limits.columns]\n",
    "    uk_limits_dict = {}\n",
    "    \n",
    "    for _, row in uk_limits.iterrows():\n",
    "        poll_std = row['pollutant_std']\n",
    "        limit_val = row['limit']\n",
    "        conc_type = str(row['concentration_measured_as']).lower().strip()\n",
    "        unit = row['unit']\n",
    "        \n",
    "        if pd.notna(poll_std) and pd.notna(limit_val):\n",
    "            if poll_std not in uk_limits_dict:\n",
    "                uk_limits_dict[poll_std] = []\n",
    "            \n",
    "            # averaging period detection\n",
    "            avg_period = 'unknown'\n",
    "            if 'annual' in conc_type and 'running' in conc_type:\n",
    "                avg_period = 'running_annual'\n",
    "            elif 'running annual' in conc_type:\n",
    "                avg_period = 'running_annual'\n",
    "            elif 'annual' in conc_type:\n",
    "                avg_period = 'annual'\n",
    "            elif '24 hour' in conc_type or '24-hour' in conc_type:\n",
    "                avg_period = '24hour'\n",
    "            elif '8 hour' in conc_type or '8-hour' in conc_type:\n",
    "                avg_period = '8hour'\n",
    "            elif '1 hour' in conc_type or '1-hour' in conc_type or 'hour mean' in conc_type:\n",
    "                avg_period = '1hour'\n",
    "            elif 'maximum daily' in conc_type:\n",
    "                avg_period = 'daily_max'\n",
    "            \n",
    "            uk_limits_dict[poll_std].append({\n",
    "                'limit': float(limit_val),\n",
    "                'type': conc_type,\n",
    "                'unit': unit,\n",
    "                'avg_period': avg_period\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nuk limits loaded for {len(uk_limits_dict)} pollutants:\")\n",
    "    for poll, limits in uk_limits_dict.items():\n",
    "        period_info = ', '.join([f\"{lim['avg_period']}: {lim['limit']}\" for lim in limits])\n",
    "        print(f\"  {poll}: {period_info}\")\n",
    "    \n",
    "    # load all laqn measurement data with timestamp\n",
    "    print(\"\\nloading laqn measurement data...\")\n",
    "    all_data = []\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    # iterate through year-month folders (e.g., 2023_jan, 2024_feb, etc.)\n",
    "    for year_month_dir in base_path.glob('*'):\n",
    "        if year_month_dir.is_dir():\n",
    "            print(f\"processing {year_month_dir.name}...\")\n",
    "            # iterate through csv files (SiteCode_SpeciesCode_YYYY-MM-DD_YYYY-MM-DD.csv)\n",
    "            for csv_file in year_month_dir.glob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    # check for required columns\n",
    "                    required_cols = {'@MeasurementDateGMT', '@Value', 'SpeciesCode'}\n",
    "                    if not required_cols.issubset(df.columns):\n",
    "                        print(f\"skipped {csv_file}: missing required columns. found: {list(df.columns)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if not df.empty:\n",
    "                        all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"error reading {csv_file}: {e}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"error: no measurement data found\")\n",
    "        return {}\n",
    "    \n",
    "    df_all = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"loaded {len(df_all):,} total records\")\n",
    "    \n",
    "    # filter valid values and parse timestamp\n",
    "    df_all['@Value'] = pd.to_numeric(df_all['@Value'], errors='coerce')\n",
    "    df_valid = df_all[df_all['@Value'].notna()].copy()\n",
    "    \n",
    "    # parse timestamp - laqn uses ISO format with timezone\n",
    "    df_valid['@MeasurementDateGMT'] = pd.to_datetime(df_valid['@MeasurementDateGMT'], errors='coerce')\n",
    "    df_valid = df_valid[df_valid['@MeasurementDateGMT'].notna()]\n",
    "    \n",
    "    print(f\"analysing {len(df_valid):,} valid measurements with timestamps\")\n",
    "    \n",
    "    # calculate quality metrics for each pollutant\n",
    "    print(\"\\nprocessing quality metrics by pollutant...\")\n",
    "    quality_results = {}\n",
    "    \n",
    "    for pollutant in df_valid['SpeciesCode'].unique():\n",
    "        if pd.isna(pollutant):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nprocessing {pollutant}...\")\n",
    "        \n",
    "        poll_data = df_valid[df_valid['SpeciesCode'] == pollutant].copy()\n",
    "        \n",
    "        if len(poll_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # basic statistics on raw hourly data\n",
    "        q_metrics = {\n",
    "            'pollutant': pollutant,\n",
    "            'count': int(len(poll_data)),\n",
    "            'mean_hourly': float(poll_data['@Value'].mean()),\n",
    "            'median_hourly': float(poll_data['@Value'].median()),\n",
    "            'std_hourly': float(poll_data['@Value'].std()),\n",
    "            'min': float(poll_data['@Value'].min()),\n",
    "            'max': float(poll_data['@Value'].max()),\n",
    "            'p95': float(poll_data['@Value'].quantile(0.95)),\n",
    "            'p99': float(poll_data['@Value'].quantile(0.99))\n",
    "        }\n",
    "        \n",
    "        # check for suspicious values\n",
    "        negative_count = (poll_data['@Value'] < 0).sum()\n",
    "        zero_count = (poll_data['@Value'] == 0).sum()\n",
    "        \n",
    "        q_metrics['negative_values'] = int(negative_count)\n",
    "        q_metrics['negative_pct'] = float((negative_count / len(poll_data) * 100))\n",
    "        q_metrics['zero_values'] = int(zero_count)\n",
    "        q_metrics['zero_pct'] = float((zero_count / len(poll_data) * 100))\n",
    "        \n",
    "        # laqn uses standard codes (NO2, PM10, PM25, SO2, CO, O3) - direct match to uk limits\n",
    "        # but PM2.5 might appear as PM25 in laqn\n",
    "        poll_std_code = pollutant\n",
    "        if pollutant == 'PM25':\n",
    "            poll_std_code = 'PM25'  # uk limits use PM25\n",
    "        \n",
    "        # check against uk limits with proper averaging\n",
    "        if poll_std_code in uk_limits_dict:\n",
    "            uk_poll_limits = uk_limits_dict[poll_std_code]\n",
    "            \n",
    "            for limit_info in uk_poll_limits:\n",
    "                avg_period = limit_info['avg_period']\n",
    "                limit_value = limit_info['limit']\n",
    "                \n",
    "                if avg_period == 'annual':\n",
    "                    poll_data['year'] = poll_data['@MeasurementDateGMT'].dt.year\n",
    "                    annual_means = poll_data.groupby('year')['@Value'].mean()\n",
    "                    \n",
    "                    q_metrics['uk_annual_limit'] = limit_value\n",
    "                    q_metrics['mean_annual'] = float(annual_means.mean())\n",
    "                    q_metrics['exceeds_uk_annual'] = q_metrics['mean_annual'] > limit_value\n",
    "                    \n",
    "                    print(f\"  annual mean: {q_metrics['mean_annual']:.2f} vs limit {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '24hour':\n",
    "                    poll_data['date'] = poll_data['@MeasurementDateGMT'].dt.date\n",
    "                    daily_means = poll_data.groupby('date')['@Value'].mean()\n",
    "                    \n",
    "                    exceedances = (daily_means > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_24hour_limit'] = limit_value\n",
    "                    q_metrics['daily_exceedances'] = int(exceedances)\n",
    "                    q_metrics['daily_exceedances_pct'] = float((exceedances / len(daily_means) * 100))\n",
    "                    \n",
    "                    print(f\"  24-hour: {exceedances} days exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '8hour':\n",
    "                    poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "                    poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "                    \n",
    "                    exceedances = (poll_data_sorted['rolling_8h'] > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_8hour_limit'] = limit_value\n",
    "                    q_metrics['8hour_exceedances'] = int(exceedances)\n",
    "                    q_metrics['8hour_exceedances_pct'] = float((exceedances / len(poll_data_sorted) * 100))\n",
    "                    \n",
    "                    print(f\"  8-hour: {exceedances} periods exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == '1hour':\n",
    "                    exceedances = (poll_data['@Value'] > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_1hour_limit'] = limit_value\n",
    "                    q_metrics['hourly_exceedances'] = int(exceedances)\n",
    "                    q_metrics['hourly_exceedances_pct'] = float((exceedances / len(poll_data) * 100))\n",
    "                    \n",
    "                    print(f\"  1-hour: {exceedances} hours exceed {limit_value}\")\n",
    "                \n",
    "                elif avg_period == 'running_annual':\n",
    "                    poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "                    poll_data_sorted['rolling_annual'] = poll_data_sorted['@Value'].rolling(window=24*365, min_periods=24*300).mean()\n",
    "                    \n",
    "                    q_metrics['uk_running_annual_limit'] = limit_value\n",
    "                    q_metrics['mean_running_annual'] = float(poll_data_sorted['rolling_annual'].mean())\n",
    "                    q_metrics['exceeds_running_annual'] = q_metrics['mean_running_annual'] > limit_value\n",
    "                    \n",
    "                    print(f\"  running annual: {q_metrics['mean_running_annual']:.2f} vs limit {limit_value}\")\n",
    "                \n",
    "                elif avg_period == 'daily_max':\n",
    "                    poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "                    poll_data_sorted['date'] = poll_data_sorted['@MeasurementDateGMT'].dt.date\n",
    "                    poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "                    \n",
    "                    daily_max = poll_data_sorted.groupby('date')['rolling_8h'].max()\n",
    "                    exceedances = (daily_max > limit_value).sum()\n",
    "                    \n",
    "                    q_metrics['uk_daily_max_limit'] = limit_value\n",
    "                    q_metrics['daily_max_exceedances'] = int(exceedances)\n",
    "                    \n",
    "                    print(f\"  daily max 8h: {exceedances} days exceed {limit_value}\")\n",
    "            \n",
    "            # overall assessment: use most restrictive limit for out of range check\n",
    "            all_limits = [lim['limit'] for lim in uk_poll_limits]\n",
    "            max_limit = max(all_limits)\n",
    "            \n",
    "            # define extreme threshold as 10x highest uk limit\n",
    "            extreme_threshold = max_limit * 10\n",
    "            out_of_range = (poll_data['@Value'] > extreme_threshold).sum()\n",
    "            \n",
    "            q_metrics['extreme_threshold'] = extreme_threshold\n",
    "            q_metrics['out_of_range'] = int(out_of_range)\n",
    "            q_metrics['out_of_range_pct'] = float((out_of_range / len(poll_data) * 100))\n",
    "        \n",
    "        else:\n",
    "            print(f\"  no uk limits defined for {poll_std_code}\")\n",
    "            q_metrics['uk_annual_limit'] = None\n",
    "            q_metrics['exceeds_uk_annual'] = False\n",
    "            q_metrics['out_of_range'] = 0\n",
    "            q_metrics['out_of_range_pct'] = 0.0\n",
    "        \n",
    "        # always calculate annual mean\n",
    "        poll_data['year'] = poll_data['@MeasurementDateGMT'].dt.year\n",
    "        annual_means = poll_data.groupby('year')['@Value'].mean()\n",
    "        q_metrics['mean_annual'] = float(annual_means.mean())\n",
    "        \n",
    "        # if annual limit exists, compare\n",
    "        if 'uk_annual_limit' in q_metrics and q_metrics['uk_annual_limit'] is not None:\n",
    "            q_metrics['exceeds_uk_annual'] = q_metrics['mean_annual'] > q_metrics['uk_annual_limit']\n",
    "        else:\n",
    "            q_metrics['exceeds_uk_annual'] = None\n",
    "        \n",
    "        # ozone (O3): count days where max running 8-hour mean > 100 µg/m³\n",
    "        if poll_std_code == 'O3':\n",
    "            poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "            poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "            poll_data_sorted['date'] = poll_data_sorted['@MeasurementDateGMT'].dt.date\n",
    "            daily_max_8h = poll_data_sorted.groupby('date')['rolling_8h'].max()\n",
    "            o3_exceedance_days = (daily_max_8h > 100).sum()\n",
    "            q_metrics['o3_exceedance_days'] = int(o3_exceedance_days)\n",
    "        \n",
    "        # CO: maximum daily running 8-hour mean\n",
    "        if poll_std_code == 'CO':\n",
    "            poll_data_sorted = poll_data.sort_values('@MeasurementDateGMT')\n",
    "            poll_data_sorted['date'] = poll_data_sorted['@MeasurementDateGMT'].dt.date\n",
    "            poll_data_sorted['rolling_8h'] = poll_data_sorted['@Value'].rolling(window=8, min_periods=6).mean()\n",
    "            daily_max_8h = poll_data_sorted.groupby('date')['rolling_8h'].max()\n",
    "            co_max_daily_8h_mean = daily_max_8h.max()\n",
    "            q_metrics['co_max_daily_8h_mean'] = float(co_max_daily_8h_mean)\n",
    "        \n",
    "        quality_results[pollutant] = q_metrics\n",
    "    \n",
    "    return quality_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d40c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quality_metrics(quality_results):\n",
    "    \"\"\"\n",
    "    print comprehensive quality metrics report with uk compliance for laqn data.\n",
    "    \n",
    "    parameters:\n",
    "        quality_results : dict\n",
    "            dictionary returned by calculate_quality_metrics_laqn\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"laqn quality metrics report\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for poll, metrics in quality_results.items():\n",
    "        print(f\"\\n{poll}:\")\n",
    "        print(f\"  total measurements: {metrics['count']:,}\")\n",
    "        print(f\"  hourly mean: {metrics['mean_hourly']:.2f}\")\n",
    "        \n",
    "        if 'mean_annual' in metrics:\n",
    "            print(f\"  annual mean: {metrics['mean_annual']:.2f}\", end=\"\")\n",
    "            if 'uk_annual_limit' in metrics and metrics['uk_annual_limit'] is not None:\n",
    "                print(f\" (limit: {metrics['uk_annual_limit']})\")\n",
    "                status = \"exceeds\" if metrics['exceeds_uk_annual'] else \"compliant\"\n",
    "                print(f\"    status: {status}\")\n",
    "            else:\n",
    "                print(\" (no uk annual limit)\")\n",
    "        \n",
    "        if 'o3_exceedance_days' in metrics:\n",
    "            print(f\"  O3 8-hour mean exceedance days: {metrics['o3_exceedance_days']}\")\n",
    "        if 'co_max_daily_8h_mean' in metrics:\n",
    "            print(f\"  CO max daily 8-hour mean: {metrics['co_max_daily_8h_mean']:.2f}\")\n",
    "        \n",
    "        if 'daily_exceedances' in metrics:\n",
    "            print(f\"  24-hour exceedances: {metrics['daily_exceedances']} days\")\n",
    "        if 'hourly_exceedances' in metrics:\n",
    "            print(f\"  1-hour exceedances: {metrics['hourly_exceedances']} hours\")\n",
    "        if metrics['negative_values'] > 0:\n",
    "            print(f\"  warning: {metrics['negative_values']} negative values\")\n",
    "        if metrics['out_of_range'] > 0:\n",
    "            print(f\"  warning: {metrics['out_of_range']} extreme values\")\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c9eb27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "uk limits loaded for 11 pollutants:\n",
      "  PM10: 24hour: 50.0, annual: 40.0\n",
      "  PM2.5: annual: 20.0\n",
      "  NO2: annual: 40.0\n",
      "  O3: 8hour: 100.0\n",
      "  SO2: 24hour: 125.0\n",
      "  PAH: annual: 0.25\n",
      "  Benzene: running_annual: 16.25\n",
      "  1,3-butadiene: running_annual: 2.25\n",
      "  CO: daily_max: 10.0\n",
      "  LEAD: annual: 0.5\n",
      "  NOx: annual: 30.0\n",
      "\n",
      "loading laqn measurement data...\n",
      "processing 2023_mar...\n",
      "processing 2025_feb...\n",
      "processing 2024_feb...\n",
      "processing 2025_aug...\n",
      "processing 2024_aug...\n",
      "processing 2025_mar...\n",
      "processing 2023_feb...\n",
      "processing 2024_mar...\n",
      "processing 2023_aug...\n",
      "processing 2024_jul...\n",
      "processing 2025_jul...\n",
      "processing 2024_oct...\n",
      "processing 2023_sep...\n",
      "processing 2025_oct...\n",
      "processing 2023_jan...\n",
      "processing 2023_jul...\n",
      "processing 2024_jan...\n",
      "processing 2025_sep...\n",
      "processing 2024_sep...\n",
      "processing 2023_oct...\n",
      "processing 2025_jan...\n",
      "processing 2024_dec...\n",
      "processing 2024_apr...\n",
      "processing 2024_nov...\n",
      "processing 2023_may...\n",
      "processing 2025_nov...\n",
      "processing 2025_apr...\n",
      "processing 2024_may...\n",
      "processing 2025_may...\n",
      "processing 2023_nov...\n",
      "processing 2023_apr...\n",
      "processing 2023_dec...\n",
      "processing report...\n",
      "processing 2025_jun...\n",
      "processing 2024_jun...\n",
      "processing 2023_jun...\n",
      "loaded 3,446,208 total records\n",
      "analysing 2,981,417 valid measurements with timestamps\n",
      "\n",
      "processing quality metrics by pollutant...\n",
      "\n",
      "processing NO2...\n",
      "  annual mean: 23.30 vs limit 40.0\n",
      "\n",
      "processing PM2.5...\n",
      "  annual mean: 9.29 vs limit 20.0\n",
      "\n",
      "processing PM10...\n",
      "  24-hour: 3 days exceed 50.0\n",
      "  annual mean: 17.32 vs limit 40.0\n",
      "\n",
      "processing SO2...\n",
      "  24-hour: 0 days exceed 125.0\n",
      "\n",
      "processing O3...\n",
      "  8-hour: 2741 periods exceed 100.0\n",
      "\n",
      "processing CO...\n",
      "  daily max 8h: 0 days exceed 10.0\n",
      "\n",
      "========================================\n",
      "laqn quality metrics report\n",
      "========================================\n",
      "\n",
      "NO2:\n",
      "  total measurements: 1,267,018\n",
      "  hourly mean: 23.34\n",
      "  annual mean: 23.30 (limit: 40.0)\n",
      "    status: compliant\n",
      "\n",
      "PM2.5:\n",
      "  total measurements: 482,559\n",
      "  hourly mean: 9.25\n",
      "  annual mean: 9.29 (limit: 20.0)\n",
      "    status: compliant\n",
      "  warning: 11 extreme values\n",
      "\n",
      "PM10:\n",
      "  total measurements: 896,146\n",
      "  hourly mean: 17.23\n",
      "  annual mean: 17.32 (limit: 40.0)\n",
      "    status: compliant\n",
      "  24-hour exceedances: 3 days\n",
      "  warning: 3 extreme values\n",
      "\n",
      "SO2:\n",
      "  total measurements: 71,217\n",
      "  hourly mean: 1.47\n",
      "  annual mean: 1.52 (no uk annual limit)\n",
      "  24-hour exceedances: 0 days\n",
      "\n",
      "O3:\n",
      "  total measurements: 220,912\n",
      "  hourly mean: 47.71\n",
      "  annual mean: 47.80 (no uk annual limit)\n",
      "  O3 8-hour mean exceedance days: 70\n",
      "\n",
      "CO:\n",
      "  total measurements: 43,565\n",
      "  hourly mean: 0.20\n",
      "  annual mean: 0.20 (no uk annual limit)\n",
      "  CO max daily 8-hour mean: 2.14\n",
      "========================================\n",
      "\n",
      "saving laqn quality metrics report...\n",
      "saved to: {'NO2': {'pollutant': 'NO2', 'count': 1267018, 'mean_hourly': 23.33811129755063, 'median_hourly': 19.0, 'std_hourly': 17.169706453593633, 'min': 0.0, 'max': 376.3, 'p95': 57.1, 'p99': 78.3, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 632, 'zero_pct': 0.049880901455227944, 'uk_annual_limit': 40.0, 'mean_annual': 23.296558581709963, 'exceeds_uk_annual': False, 'extreme_threshold': 400.0, 'out_of_range': 0, 'out_of_range_pct': 0.0}, 'PM2.5': {'pollutant': 'PM2.5', 'count': 482559, 'mean_hourly': 9.249439550396948, 'median_hourly': 7.0, 'std_hourly': 7.869002875250606, 'min': 0.0, 'max': 909.0, 'p95': 23.9, 'p99': 38.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2090, 'zero_pct': 0.4331076614465796, 'uk_annual_limit': 20.0, 'mean_annual': 9.287265567932431, 'exceeds_uk_annual': False, 'extreme_threshold': 200.0, 'out_of_range': 11, 'out_of_range_pct': 0.0022795140076135767}, 'PM10': {'pollutant': 'PM10', 'count': 896146, 'mean_hourly': 17.23079754861373, 'median_hourly': 14.4, 'std_hourly': 12.470894582830518, 'min': 0.0, 'max': 759.0, 'p95': 39.0, 'p99': 61.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 835, 'zero_pct': 0.09317678146194928, 'uk_24hour_limit': 50.0, 'daily_exceedances': 3, 'daily_exceedances_pct': 0.2944062806673209, 'uk_annual_limit': 40.0, 'mean_annual': 17.3172696741254, 'exceeds_uk_annual': False, 'extreme_threshold': 500.0, 'out_of_range': 3, 'out_of_range_pct': 0.0003347668795040094}, 'SO2': {'pollutant': 'SO2', 'count': 71217, 'mean_hourly': 1.46688431133016, 'median_hourly': 1.1, 'std_hourly': 3.2138526496443744, 'min': 0.0, 'max': 271.4, 'p95': 4.2, 'p99': 7.2, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2348, 'zero_pct': 3.296965612143168, 'uk_24hour_limit': 125.0, 'daily_exceedances': 0, 'daily_exceedances_pct': 0.0, 'extreme_threshold': 1250.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 1.5215136717118256, 'exceeds_uk_annual': None}, 'O3': {'pollutant': 'O3', 'count': 220912, 'mean_hourly': 47.70532836604621, 'median_hourly': 48.1, 'std_hourly': 23.953550068547706, 'min': 0.0, 'max': 198.6, 'p95': 85.6, 'p99': 108.68899999999849, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 260, 'zero_pct': 0.11769392337220251, 'uk_8hour_limit': 100.0, '8hour_exceedances': 2741, '8hour_exceedances_pct': 1.2407655537046427, 'extreme_threshold': 1000.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 47.798197519325676, 'exceeds_uk_annual': None, 'o3_exceedance_days': 70}, 'CO': {'pollutant': 'CO', 'count': 43565, 'mean_hourly': 0.20249512223114888, 'median_hourly': 0.2, 'std_hourly': 0.18651998992979038, 'min': 0.0, 'max': 4.9, 'p95': 0.5, 'p99': 0.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 6773, 'zero_pct': 15.546883966486858, 'uk_daily_max_limit': 10.0, 'daily_max_exceedances': 0, 'extreme_threshold': 100.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 0.2006708313657066, 'exceeds_uk_annual': None, 'co_max_daily_8h_mean': 2.1374999999999997}}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# calculate quality metrics\n",
    "quality_results = calculate_quality_metrics(base_dir, csv_output_path)\n",
    "\n",
    "print_quality_metrics(quality_results)\n",
    "\n",
    "if quality_results:\n",
    "    # save comprehensive report\n",
    "    print(\"\\nsaving laqn quality metrics report...\")\n",
    "    \n",
    "    quality_rows = []\n",
    "    for poll, metrics in quality_results.items():\n",
    "        row = {\n",
    "            'pollutant': metrics['pollutant'],\n",
    "            'total_measurements': metrics['count'],\n",
    "            'mean_hourly': f\"{metrics['mean_hourly']:.2f}\",\n",
    "            'min': f\"{metrics['min']:.2f}\",\n",
    "            'max': f\"{metrics['max']:.2f}\",\n",
    "            'p95': f\"{metrics['p95']:.2f}\",\n",
    "            'negative_values': metrics['negative_values'],\n",
    "            'zero_values': metrics['zero_values'],\n",
    "            'out_of_range': metrics['out_of_range']\n",
    "        }\n",
    "        \n",
    "        # add uk limit compliance fields\n",
    "        if 'uk_annual_limit' in metrics and metrics['uk_annual_limit']:\n",
    "            row['uk_annual_limit'] = metrics['uk_annual_limit']\n",
    "            row['mean_annual'] = f\"{metrics['mean_annual']:.2f}\" if 'mean_annual' in metrics else 'n/a'\n",
    "            row['exceeds_annual'] = 'yes' if metrics.get('exceeds_uk_annual', False) else 'no'\n",
    "        \n",
    "        if 'daily_exceedances' in metrics:\n",
    "            row['uk_24hour_limit'] = metrics['uk_24hour_limit']\n",
    "            row['daily_exceedances'] = metrics['daily_exceedances']\n",
    "        \n",
    "        if 'hourly_exceedances' in metrics:\n",
    "            row['uk_1hour_limit'] = metrics['uk_1hour_limit']\n",
    "            row['hourly_exceedances'] = metrics['hourly_exceedances']\n",
    "        \n",
    "        if 'o3_exceedance_days' in metrics:\n",
    "            row['o3_exceedance_days'] = metrics['o3_exceedance_days']\n",
    "        \n",
    "        if 'co_max_daily_8h_mean' in metrics:\n",
    "            row['co_max_daily_8h_mean'] = f\"{metrics['co_max_daily_8h_mean']:.2f}\"\n",
    "        \n",
    "        quality_rows.append(row)\n",
    "    \n",
    "    #ssave to quality metrics csv\n",
    "    pd.DataFrame(quality_rows).to_csv(\"quality_metrics.csv\", index=False)\n",
    "    print(f\"saved to: {quality_results}\")\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"laqn quality metrics calculation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f20fa",
   "metadata": {},
   "source": [
    "    uk limits loaded for 11 pollutants:\n",
    "    PM10: 24hour: 50.0, annual: 40.0\n",
    "    PM2.5: annual: 20.0\n",
    "    NO2: annual: 40.0\n",
    "    O3: 8hour: 100.0\n",
    "    SO2: 24hour: 125.0\n",
    "    PAH: annual: 0.25\n",
    "    Benzene: running_annual: 16.25\n",
    "    1,3-butadiene: running_annual: 2.25\n",
    "    CO: daily_max: 10.0\n",
    "    LEAD: annual: 0.5\n",
    "    NOx: annual: 30.0\n",
    "\n",
    "    loading laqn measurement data...\n",
    "    processing 2023_mar...\n",
    "    processing 2025_feb...\n",
    "    processing 2024_feb...\n",
    "    processing 2025_aug...\n",
    "    processing 2024_aug...\n",
    "    processing 2025_mar...\n",
    "    processing 2023_feb...\n",
    "    processing 2024_mar...\n",
    "    processing 2023_aug...\n",
    "    processing 2024_jul...\n",
    "    processing 2025_jul...\n",
    "    processing 2024_oct...\n",
    "    processing 2023_sep...\n",
    "    processing 2025_oct...\n",
    "    processing 2023_jan...\n",
    "    processing 2023_jul...\n",
    "    processing 2024_jan...\n",
    "    processing 2025_sep...\n",
    "    processing 2024_sep...\n",
    "    processing 2023_oct...\n",
    "    processing 2025_jan...\n",
    "    processing 2024_dec...\n",
    "    processing 2024_apr...\n",
    "    processing 2024_nov...\n",
    "    processing 2023_may...\n",
    "    processing 2025_nov...\n",
    "    processing 2025_apr...\n",
    "    processing 2024_may...\n",
    "    processing 2025_may...\n",
    "    processing 2023_nov...\n",
    "    processing 2023_apr...\n",
    "    processing 2023_dec...\n",
    "    processing report...\n",
    "    processing 2025_jun...\n",
    "    processing 2024_jun...\n",
    "    processing 2023_jun...\n",
    "    loaded 3,446,208 total records\n",
    "    analysing 2,981,417 valid measurements with timestamps\n",
    "\n",
    "    processing quality metrics by pollutant...\n",
    "\n",
    "    processing NO2...\n",
    "    annual mean: 23.30 vs limit 40.0\n",
    "\n",
    "    processing PM2.5...\n",
    "    annual mean: 9.29 vs limit 20.0\n",
    "\n",
    "    processing PM10...\n",
    "    24-hour: 3 days exceed 50.0\n",
    "    annual mean: 17.32 vs limit 40.0\n",
    "\n",
    "    processing SO2...\n",
    "    24-hour: 0 days exceed 125.0\n",
    "\n",
    "    processing O3...\n",
    "    8-hour: 2741 periods exceed 100.0\n",
    "\n",
    "    processing CO...\n",
    "    daily max 8h: 0 days exceed 10.0\n",
    "\n",
    "    ========================================\n",
    "    laqn quality metrics report\n",
    "    ========================================\n",
    "\n",
    "    NO2:\n",
    "    total measurements: 1,267,018\n",
    "    hourly mean: 23.34\n",
    "    annual mean: 23.30 (limit: 40.0)\n",
    "        status: compliant\n",
    "\n",
    "    PM2.5:\n",
    "    total measurements: 482,559\n",
    "    hourly mean: 9.25\n",
    "    annual mean: 9.29 (limit: 20.0)\n",
    "        status: compliant\n",
    "    warning: 11 extreme values\n",
    "\n",
    "    PM10:\n",
    "    total measurements: 896,146\n",
    "    hourly mean: 17.23\n",
    "    annual mean: 17.32 (limit: 40.0)\n",
    "        status: compliant\n",
    "    24-hour exceedances: 3 days\n",
    "    warning: 3 extreme values\n",
    "\n",
    "    SO2:\n",
    "    total measurements: 71,217\n",
    "    hourly mean: 1.47\n",
    "    annual mean: 1.52 (no uk annual limit)\n",
    "    24-hour exceedances: 0 days\n",
    "\n",
    "    O3:\n",
    "    total measurements: 220,912\n",
    "    hourly mean: 47.71\n",
    "    annual mean: 47.80 (no uk annual limit)\n",
    "    O3 8-hour mean exceedance days: 70\n",
    "\n",
    "    CO:\n",
    "    total measurements: 43,565\n",
    "    hourly mean: 0.20\n",
    "    annual mean: 0.20 (no uk annual limit)\n",
    "    CO max daily 8-hour mean: 2.14\n",
    "    ========================================\n",
    "\n",
    "    saving laqn quality metrics report...\n",
    "    saved to: {'NO2': {'pollutant': 'NO2', 'count': 1267018, 'mean_hourly': 23.33811129755063, 'median_hourly': 19.0, 'std_hourly': 17.169706453593633, 'min': 0.0, 'max': 376.3, 'p95': 57.1, 'p99': 78.3, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 632, 'zero_pct': 0.049880901455227944, 'uk_annual_limit': 40.0, 'mean_annual': 23.296558581709963, 'exceeds_uk_annual': False, 'extreme_threshold': 400.0, 'out_of_range': 0, 'out_of_range_pct': 0.0}, 'PM2.5': {'pollutant': 'PM2.5', 'count': 482559, 'mean_hourly': 9.249439550396948, 'median_hourly': 7.0, 'std_hourly': 7.869002875250606, 'min': 0.0, 'max': 909.0, 'p95': 23.9, 'p99': 38.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2090, 'zero_pct': 0.4331076614465796, 'uk_annual_limit': 20.0, 'mean_annual': 9.287265567932431, 'exceeds_uk_annual': False, 'extreme_threshold': 200.0, 'out_of_range': 11, 'out_of_range_pct': 0.0022795140076135767}, 'PM10': {'pollutant': 'PM10', 'count': 896146, 'mean_hourly': 17.23079754861373, 'median_hourly': 14.4, 'std_hourly': 12.470894582830518, 'min': 0.0, 'max': 759.0, 'p95': 39.0, 'p99': 61.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 835, 'zero_pct': 0.09317678146194928, 'uk_24hour_limit': 50.0, 'daily_exceedances': 3, 'daily_exceedances_pct': 0.2944062806673209, 'uk_annual_limit': 40.0, 'mean_annual': 17.3172696741254, 'exceeds_uk_annual': False, 'extreme_threshold': 500.0, 'out_of_range': 3, 'out_of_range_pct': 0.0003347668795040094}, 'SO2': {'pollutant': 'SO2', 'count': 71217, 'mean_hourly': 1.46688431133016, 'median_hourly': 1.1, 'std_hourly': 3.2138526496443744, 'min': 0.0, 'max': 271.4, 'p95': 4.2, 'p99': 7.2, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 2348, 'zero_pct': 3.296965612143168, 'uk_24hour_limit': 125.0, 'daily_exceedances': 0, 'daily_exceedances_pct': 0.0, 'extreme_threshold': 1250.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 1.5215136717118256, 'exceeds_uk_annual': None}, 'O3': {'pollutant': 'O3', 'count': 220912, 'mean_hourly': 47.70532836604621, 'median_hourly': 48.1, 'std_hourly': 23.953550068547706, 'min': 0.0, 'max': 198.6, 'p95': 85.6, 'p99': 108.68899999999849, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 260, 'zero_pct': 0.11769392337220251, 'uk_8hour_limit': 100.0, '8hour_exceedances': 2741, '8hour_exceedances_pct': 1.2407655537046427, 'extreme_threshold': 1000.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 47.798197519325676, 'exceeds_uk_annual': None, 'o3_exceedance_days': 70}, 'CO': {'pollutant': 'CO', 'count': 43565, 'mean_hourly': 0.20249512223114888, 'median_hourly': 0.2, 'std_hourly': 0.18651998992979038, 'min': 0.0, 'max': 4.9, 'p95': 0.5, 'p99': 0.8, 'negative_values': 0, 'negative_pct': 0.0, 'zero_values': 6773, 'zero_pct': 15.546883966486858, 'uk_daily_max_limit': 10.0, 'daily_max_exceedances': 0, 'extreme_threshold': 100.0, 'out_of_range': 0, 'out_of_range_pct': 0.0, 'mean_annual': 0.2006708313657066, 'exceeds_uk_annual': None, 'co_max_daily_8h_mean': 2.1374999999999997}}\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed540e1",
   "metadata": {},
   "source": [
    "## 5) Chi-Square Test for LAQN Data Quality\n",
    "\n",
    "Uses statistical tests to mathematically prove that LAQN data collection process was consistent and reliable across time. \n",
    "It serves as a quality control check that ensures we didn't accidentally collect more data in some months than others, which could bias our analysis.\n",
    "\n",
    "#### Why Chi-Square Test?\n",
    " - The chi-square test answers one simple question: Are my monthly file counts similar enough to trust, or are some months suspiciously different?\n",
    " - Air pollution varies by season\n",
    " - Policy decisions need unbiased evidence\n",
    " - Academic reviewers will question imbalanced datasets\n",
    "\n",
    "### What Chi-Square Test Does\n",
    "\n",
    "The chi-square test answers one simple question: Are my monthly file counts similar enough to trust, or are some months suspiciously different?\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. What we observe: Count how many data files we have for each month.\n",
    "2. What we expect: If data collection was perfect, each month should have roughly the same count.\n",
    "3. The test: Measures how far observed counts are from the expected counts.\n",
    "4. The result: Gives us a p-value that tells us if the differences are just random variation or a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39e36dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_tests(base_dir):\n",
    "    \"\"\"\n",
    "    run statistical tests to prove laqn data collection was consistent.\n",
    "    \n",
    "    - chi-square test: checks if we have similar amounts of data for each year\n",
    "    - if p-value < 0.05: data isn't evenly spread - potential problem\n",
    "    - if p-value > 0.05: data is evenly spread - good\n",
    "    \n",
    "    parameters:\n",
    "        base_dir : path to laqn optimised directory\n",
    "            e.g., data/laqn/optimised/\n",
    "            \n",
    "    returns:\n",
    "        dict with test results (chi2 statistic, p-value, counts)\n",
    "    \"\"\"\n",
    "    \n",
    "    # count files per year 2023:12 months, 2024:12 months, 2025:11 months up to nov\n",
    "    yearly_data = {'2023': 0, '2024': 0, '2025': 0}\n",
    "    year_months = {'2023': 12, '2024': 12, '2025': 11}\n",
    "    \n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    # iterate through all month folders and count files by year\n",
    "    for month_dir in base_path.glob('*'):\n",
    "        if not month_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # extract year from folder name (e.g., \"2023_jan\" =\"2023\")\n",
    "        folder_name = month_dir.name\n",
    "        try:\n",
    "            year = folder_name.split('_')[0]\n",
    "            if year in yearly_data:\n",
    "                # count csv files in this month\n",
    "                file_count = len(list(month_dir.glob('*.csv')))\n",
    "                yearly_data[year] += file_count\n",
    "        except Exception as e:\n",
    "            print(f\"warning: couldn't process {folder_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nChi-square test for laqn yearly distribution\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"\\nobserved file counts by year:\")\n",
    "    \n",
    "    # prepare for chi-square\n",
    "    year_counts = [yearly_data[y] for y in ['2023', '2024', '2025']]\n",
    "    total_files = sum(year_counts)\n",
    "    total_months = 35  # total months 35\n",
    "    \n",
    "    # Expected counts proportional to number of months\n",
    "    expected_counts = [\n",
    "        total_files * (year_months[year] / total_months)\n",
    "        for year in ['2023', '2024', '2025']\n",
    "    ]\n",
    "    \n",
    "    # display counts\n",
    "    for year, count, expected in zip(['2023', '2024', '2025'], \n",
    "                                      year_counts, expected_counts):\n",
    "        print(f\"  {year}: {count:5d} files (expected: {expected:7.1f})\")\n",
    "    \n",
    "    # run chi-square test\n",
    "    chi2, p_value = stats.chisquare(\n",
    "        f_obs=year_counts, \n",
    "        f_exp=expected_counts\n",
    "    )\n",
    "    \n",
    "    print()\n",
    "    print(f\"chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # interpret results\n",
    "    if p_value < 0.05:\n",
    "        print(f\"Resultreject null hypothesis (p < 0.05)\")\n",
    "        print(f\"Interpretation: years not evenly distributed\")\n",
    "        print(f\"Action: investigate why some years have different file counts\")\n",
    "    else:\n",
    "        print(f\"Result: accept null hypothesis (p >= 0.05)\")\n",
    "        print(f\"Interpretation: years evenly distributed\")\n",
    "        print(f\"Conclusion: data collection was consistent across years\")\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    return {\n",
    "        'test': 'chi-square year-wise laqn',\n",
    "        'chi2_statistic': chi2,\n",
    "        'p_value': p_value,\n",
    "        'year_counts': year_counts,\n",
    "        'expected_counts': expected_counts,\n",
    "        'total_files': total_files,\n",
    "        'total_months': total_months\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91b62073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chi-square test for laqn data...\n",
      "\n",
      "Chi-square test for laqn yearly distribution\n",
      "========================================\n",
      "\n",
      "observed file counts by year:\n",
      "  2023:  1689 files (expected:  1691.0)\n",
      "  2024:  1692 files (expected:  1691.0)\n",
      "  2025:  1551 files (expected:  1550.1)\n",
      "\n",
      "chi-square statistic: 0.0035\n",
      "p-value: 0.9983\n",
      "\n",
      "Result: accept null hypothesis (p >= 0.05)\n",
      "Interpretation: years evenly distributed\n",
      "Conclusion: data collection was consistent across years\n",
      "========================================\n",
      "\n",
      "Statistical test results saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/chi_square_tests.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# run chi-square test for laqn\n",
    "print(\"Starting chi-square test for laqn data...\")\n",
    "\n",
    "test_results_laqn = chi_square_tests(base_dir)\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame([{\n",
    "    'test_name': test_results_laqn['test'],\n",
    "    'statistic': f\"{test_results_laqn['chi2_statistic']:.4f}\",\n",
    "    'p_value': f\"{test_results_laqn['p_value']:.4f}\",\n",
    "    'interpretation': ('evenly distributed' \n",
    "                      if test_results_laqn['p_value'] >= 0.05 \n",
    "                      else 'unevenly distributed'),\n",
    "    'total_files': test_results_laqn['total_files'],\n",
    "    'total_months': test_results_laqn['total_months'],\n",
    "    '2023_count': test_results_laqn['year_counts'][0],\n",
    "    '2024_count': test_results_laqn['year_counts'][1],\n",
    "    '2025_count': test_results_laqn['year_counts'][2],\n",
    "    '2023_expected': f\"{test_results_laqn['expected_counts'][0]:.1f}\",\n",
    "    '2024_expected': f\"{test_results_laqn['expected_counts'][1]:.1f}\",\n",
    "    '2025_expected': f\"{test_results_laqn['expected_counts'][2]:.1f}\"\n",
    "}]).to_csv(chi_square_laqn_output, index=False)\n",
    "\n",
    "print(f\"\\nStatistical test results saved to: {chi_square_laqn_output}\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2fbd7",
   "metadata": {},
   "source": [
    "    Starting chi-square test for laqn data...\n",
    "\n",
    "    Chi-square test for laqn yearly distribution\n",
    "    ========================================\n",
    "\n",
    "    observed file counts by year:\n",
    "    2023:  1689 files (expected:  1691.0)\n",
    "    2024:  1692 files (expected:  1691.0)\n",
    "    2025:  1551 files (expected:  1550.1)\n",
    "\n",
    "    chi-square statistic: 0.0035\n",
    "    p-value: 0.9983\n",
    "\n",
    "    Result: accept null hypothesis (p >= 0.05)\n",
    "    Interpretation: years evenly distributed\n",
    "    Conclusion: data collection was consistent across years\n",
    "    ========================================\n",
    "\n",
    "    Statistical test results saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/chi_square_tests.csv\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a80da",
   "metadata": {},
   "source": [
    "### Year-over-Year Station Coverage Analysis\n",
    "\n",
    "After confirming data distribution consistency with the chi-square test, we need to understand **what changed** between years. This analysis identifies:\n",
    "\n",
    "- Which station-pollutant combinations stopped reporting\n",
    "- Whether equipment was decommissioned or failed\n",
    "- If monitoring network coverage changed over time\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Even if file counts are statistically balanced, the **specific stations** might have changed:\n",
    "- Equipment upgrades/replacements\n",
    "- Station relocations\n",
    "- Policy changes in monitoring priorities\n",
    "- Budget constraints affecting network coverage\n",
    "\n",
    "### What We Track\n",
    "\n",
    "For each year (2023, 2024, 2025):\n",
    "1. Count unique station-pollutant combinations\n",
    "2. Identify combinations present in 2023 but missing in later years\n",
    "3. Quantify network coverage changes\n",
    "\n",
    "### Expected Findings\n",
    "\n",
    "Based on LAQN data quality report:\n",
    "- Some stations may have been decommissioned\n",
    "- Equipment failures affecting specific pollutants\n",
    "- Normal operational gaps vs. permanent losses\n",
    "\n",
    "This analysis helps distinguish between:\n",
    "- **Temporary gaps**: Equipment maintenance, calibration\n",
    "- **Permanent losses**: Station closure, equipment decommissioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "247e7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_year_difference(base_dir):\n",
    "    \"\"\"\n",
    "    find which station-pollutant combinations are missing in later years.\n",
    "    \n",
    "    analyses laqn network coverage changes by comparing active combinations\n",
    "    across 2023, 2024, and 2025.\n",
    "    \n",
    "    parameters:\n",
    "        base_dir \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    # unique station-pollutant combinations per year\n",
    "    year_files = {'2023': set(), '2024': set(), '2025': set()}\n",
    "    \n",
    "    # iterate through all month folders\n",
    "    for month_dir in base_path.glob('*'):\n",
    "        if not month_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # extract year from folder name (e.g., \"2023_jan\" -> \"2023\")\n",
    "        folder_name = month_dir.name\n",
    "        try:\n",
    "            year = folder_name.split('_')[0]\n",
    "            if year not in year_files:\n",
    "                continue\n",
    "            \n",
    "            # process all csv files in this month\n",
    "            for csv_file in month_dir.glob('*.csv'):\n",
    "                # file format: SiteCode_SpeciesCode_YYYY-MM-DD_YYYY-MM-DD.csv\n",
    "                # example: BG1_NO2_2023-01-01_2023-01-31.csv\n",
    "                filename = csv_file.stem\n",
    "                parts = filename.split('_')\n",
    "                \n",
    "                if len(parts) >= 2:\n",
    "                    site_code = parts[0]\n",
    "                    species_code = parts[1]\n",
    "                    combination = (site_code, species_code)\n",
    "                    year_files[year].add(combination)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"warning: couldn't process {folder_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # find what's in 2023 but missing in 2024/2025\n",
    "    lost_2024 = year_files['2023'] - year_files['2024']\n",
    "    lost_2025 = year_files['2023'] - year_files['2025']\n",
    "    \n",
    "    # also check what's new in later years\n",
    "    new_2024 = year_files['2024'] - year_files['2023']\n",
    "    new_2025 = year_files['2025'] - year_files['2023']\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"LAQN station/pollutant coverage analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nyearly totals:\")\n",
    "    print(f\"2023: {len(year_files['2023'])} unique station-pollutant combinations.\")\n",
    "    print(f\"2024: {len(year_files['2024'])} unique station-pollutant combinations.\")\n",
    "    print(f\"2025: {len(year_files['2025'])} unique station-pollutant combinations.\")\n",
    "    \n",
    "    print(\"\\nChanges over time:\")\n",
    "    print(f\"Lost in 2024 (vs 2023): {len(lost_2024)} combinations\")\n",
    "    print(f\"Lost in 2025 (vs 2023): {len(lost_2025)} combinations\")\n",
    "    print(f\"New in 2024 (vs 2023): {len(new_2024)} combinations\")\n",
    "    print(f\"New in 2025 (vs 2023): {len(new_2025)} combinations\")\n",
    "    \n",
    "    # show examples of lost combinations\n",
    "    if lost_2024:\n",
    "        print(\"\\nExamples of combinations lost in 2024:\")\n",
    "        for site, species in sorted(lost_2024)[:10]:\n",
    "            print(f\"    {site} - {species}\")\n",
    "        if len(lost_2024) > 10:\n",
    "            print(f\"    ... and {len(lost_2024) - 10} more\")\n",
    "    \n",
    "    if lost_2025:\n",
    "        print(\"\\nExamples of combinations lost in 2025:\")\n",
    "        for site, species in sorted(lost_2025)[:10]:\n",
    "            print(f\"    {site} - {species}\")\n",
    "        if len(lost_2025) > 10:\n",
    "            print(f\"    ... and {len(lost_2025) - 10} more\")\n",
    "    \n",
    "    # show examples of new combinations\n",
    "    if new_2024:\n",
    "        print(\"\\nExamples of new combinations in 2024:\")\n",
    "        for site, species in sorted(new_2024)[:10]:\n",
    "            print(f\"    {site} - {species}\")\n",
    "        if len(new_2024) > 10:\n",
    "            print(f\"    ... and {len(new_2024) - 10} more\")\n",
    "    \n",
    "    if new_2025:\n",
    "        print(\"\\nExamples of new combinations in 2025:\")\n",
    "        for site, species in sorted(new_2025)[:10]:\n",
    "            print(f\"    {site} - {species}\")\n",
    "        if len(new_2025) > 10:\n",
    "            print(f\"    ... and {len(new_2025) - 10} more\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        '2023_count': len(year_files['2023']),\n",
    "        '2024_count': len(year_files['2024']),\n",
    "        '2025_count': len(year_files['2025']),\n",
    "        '2023_combinations': year_files['2023'],\n",
    "        '2024_combinations': year_files['2024'],\n",
    "        '2025_combinations': year_files['2025'],\n",
    "        'lost_2024': lost_2024,\n",
    "        'lost_2025': lost_2025,\n",
    "        'new_2024': new_2024,\n",
    "        'new_2025': new_2025\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26451043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing station-pollutant coverage changes across years...\n",
      "\n",
      "========================================\n",
      "LAQN station/pollutant coverage analysis\n",
      "========================================\n",
      "\n",
      "yearly totals:\n",
      "2023: 141 unique station-pollutant combinations.\n",
      "2024: 141 unique station-pollutant combinations.\n",
      "2025: 141 unique station-pollutant combinations.\n",
      "\n",
      "Changes over time:\n",
      "Lost in 2024 (vs 2023): 0 combinations\n",
      "Lost in 2025 (vs 2023): 0 combinations\n",
      "New in 2024 (vs 2023): 0 combinations\n",
      "New in 2025 (vs 2023): 0 combinations\n",
      "============================================================\n",
      "\n",
      "year coverage summary saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/year_coverage_analysis.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# run year difference analysis for laqn\n",
    "print(\"Analysing station-pollutant coverage changes across years...\")\n",
    "\n",
    "analysis_laqn = analyse_year_difference(base_dir)\n",
    "\n",
    "\n",
    "# create summary dataframe\n",
    "summary_data = {\n",
    "    'year': ['2023', '2024', '2025'],\n",
    "    'total_combinations': [\n",
    "        analysis_laqn['2023_count'],\n",
    "        analysis_laqn['2024_count'],\n",
    "        analysis_laqn['2025_count']\n",
    "    ],\n",
    "    'lost_vs_2023': [0, len(analysis_laqn['lost_2024']), len(analysis_laqn['lost_2025'])],\n",
    "    'new_vs_2023': [0, len(analysis_laqn['new_2024']), len(analysis_laqn['new_2025'])]\n",
    "}\n",
    "\n",
    "pd.DataFrame(summary_data).to_csv(year_diff_output, index=False)\n",
    "print(f\"\\nyear coverage summary saved to: {year_diff_output}\")\n",
    "\n",
    "# optionally save detailed lists of lost/new combinations\n",
    "if analysis_laqn['lost_2024'] or analysis_laqn['lost_2025']:\n",
    "    lost_details_output = Path.home() / \"Desktop\" / \"data science projects\" / \"air-pollution-levels\" / \"data\" / \"laqn\" / \"report\" / \"lost_combinations_detail.csv\"\n",
    "    \n",
    "    lost_records = []\n",
    "    \n",
    "    for site, species in analysis_laqn['lost_2024']:\n",
    "        lost_records.append({\n",
    "            'site_code': site,\n",
    "            'species_code': species,\n",
    "            'year_lost': '2024',\n",
    "            'present_in_2023': 'yes',\n",
    "            'present_in_2024': 'no',\n",
    "            'present_in_2025': 'yes' if (site, species) in analysis_laqn['2025_combinations'] else 'no'\n",
    "        })\n",
    "    \n",
    "    for site, species in analysis_laqn['lost_2025']:\n",
    "        if (site, species) not in analysis_laqn['lost_2024']:  # avoid duplicates\n",
    "            lost_records.append({\n",
    "                'site_code': site,\n",
    "                'species_code': species,\n",
    "                'year_lost': '2025',\n",
    "                'present_in_2023': 'yes',\n",
    "                'present_in_2024': 'yes' if (site, species) in analysis_laqn['2024_combinations'] else 'no',\n",
    "                'present_in_2025': 'no'\n",
    "            })\n",
    "    \n",
    "    if lost_records:\n",
    "        pd.DataFrame(lost_records).to_csv(lost_details_output, index=False)\n",
    "        print(f\"Detailed lost combinations saved to: {lost_details_output}\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e5038",
   "metadata": {},
   "source": [
    "## 6) Seasonal Trend Analysis\n",
    "\n",
    "Analyses the cleaned LAQN air quality data to identify:\n",
    "- Temporal trends (yearly, monthly, daily patterns).\n",
    "- Pollutant-specific characteristics.\n",
    "- Geographic hotspots.\n",
    "- Seasonal variations.\n",
    "- Exceedances of UK legal limits.\n",
    "\n",
    "### 1. Overall Pollutant Trends\n",
    " \n",
    "- Yearly average comparison by pollutant.\n",
    "- Monthly trends across all years.\n",
    "- Statistical summary tables.\n",
    "- Visualisations: bar charts & line plots.\n",
    "\n",
    "### 2. Pollutant-Specific Analysis\n",
    "\n",
    "- Time series plot (daily averages).\n",
    "- Distribution histogram (mean, median).\n",
    "- Hourly pattern (rush hour peaks).\n",
    "- Monthly boxplots (seasonal variation).\n",
    "\n",
    "### 3. Seasonal Patterns\n",
    "\n",
    "- Winter/Spring/Summer/Autumn definitions.\n",
    "- Seasonal averages per pollutant.\n",
    "- Comparison visualisation.\n",
    "- Geographic distribution (can add later).\n",
    " \n",
    "### 4. UK Legal Limit Exceedances\n",
    "\n",
    "- Compares actual measurements vs UK limits.\n",
    "- Calculates exceedance percentages.\n",
    "- Identifies which pollutants/stations exceed limits most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06cdcdd",
   "metadata": {},
   "source": [
    "##### Data Analysis and Results\n",
    "##### Pollution patterns, trends, and insights from LAQN dataset (2023-2025)\n",
    "\n",
    "This section analyses the cleaned LAQN air quality data to identify:\n",
    "- Temporal trends (yearly, monthly, daily patterns).\n",
    "- Pollutant-specific characteristics.\n",
    "- Geographic hotspots.\n",
    "- Seasonal variations.\n",
    "- Exceedances of UK legal limits.\n",
    "\n",
    "Structure:\n",
    "1. Overall pollution trends (2023-2025).\n",
    "2. Pollutant-specific analysis.\n",
    "3. Temporal patterns (seasonal, weekly, daily).\n",
    "4. Geographic distribution.\n",
    "5. Limit exceedances analysis.\n",
    "6. Key findings summary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091731e",
   "metadata": {},
   "source": [
    "#### 1) Load pollution data.\n",
    "\n",
    "Sample data for analysis. Begin with only analysing 100 files, that's why the run function is set up very bottom at 100.\n",
    "Once quick exploration is done it will be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c92fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pollution_data(base_dir, sample_size=None, pollutants=None, stations=None):\n",
    "    \"\"\"\n",
    "    Load a sample of data for analysis.\n",
    "\n",
    "    For full dataset analysis, set sample_size=None. For quick exploration, use sample_size=100.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir : path to laqn optimised directory\n",
    "        sample_size : int, optional - number of files to load\n",
    "        pollutants : list, optional - filter by species codes (e.g., ['NO2', 'PM25'])\n",
    "        stations : list, optional - filter by site codes (e.g., ['BG1', 'MY1'])\n",
    "\n",
    "    Returns:\n",
    "        Combined data with cols: timestamp, value, station_name, pollutant_name, year, month, day.\n",
    "    \"\"\"\n",
    "    base_dir = Path(base_dir)\n",
    "    all_data = []\n",
    "    file_count = 0\n",
    "\n",
    "    # Iterate through month folders (2023_jan, 2024_feb, etc.)\n",
    "    for month_dir in sorted(base_dir.glob('*')):\n",
    "        if not month_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Get all CSV files in this month\n",
    "        all_files = list(month_dir.glob('*.csv'))\n",
    "        \n",
    "        # Filter by pollutant if specified (files named: SiteCode_SpeciesCode_dates.csv)\n",
    "        if pollutants:\n",
    "            all_files = [f for f in all_files if any(f'_{p}_' in f.stem for p in pollutants)]\n",
    "        \n",
    "        # Filter by station if specified\n",
    "        if stations:\n",
    "            all_files = [f for f in all_files if any(f.stem.startswith(f'{s}_') for s in stations)]\n",
    "        \n",
    "        # Apply sample size limit\n",
    "        if sample_size and file_count >= sample_size:\n",
    "            break\n",
    "        \n",
    "        files_to_load = all_files\n",
    "        if sample_size:\n",
    "            remaining = sample_size - file_count\n",
    "            files_to_load = all_files[:remaining]\n",
    "        \n",
    "        # Load each file\n",
    "        for filepath in files_to_load:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Check required columns exist (LAQN format)\n",
    "                if '@MeasurementDateGMT' not in df.columns or '@Value' not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Standardise column names\n",
    "                df = df.rename(columns={\n",
    "                    '@MeasurementDateGMT': 'timestamp',\n",
    "                    '@Value': 'value',\n",
    "                    'SiteName': 'station_name',\n",
    "                    'SpeciesCode': 'pollutant_name'\n",
    "                })\n",
    "                \n",
    "                # Add metadata if missing\n",
    "                if 'station_name' not in df.columns and 'SiteName' in df.columns:\n",
    "                    df['station_name'] = df['SiteName']\n",
    "                elif 'station_name' not in df.columns:\n",
    "                    # Extract from filename: SiteCode_SpeciesCode_dates.csv\n",
    "                    parts = filepath.stem.split('_')\n",
    "                    df['station_name'] = parts[0] if len(parts) > 0 else 'unknown'\n",
    "                \n",
    "                if 'pollutant_name' not in df.columns:\n",
    "                    parts = filepath.stem.split('_')\n",
    "                    df['pollutant_name'] = parts[1] if len(parts) > 1 else 'unknown'\n",
    "                \n",
    "                all_data.append(df)\n",
    "                file_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: couldn't load {filepath.name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"\\n  Error: no data loaded.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all data\n",
    "    data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Parse timestamp and extract time components\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data['year'] = data['timestamp'].dt.year\n",
    "    data['month'] = data['timestamp'].dt.month\n",
    "    data['day'] = data['timestamp'].dt.day\n",
    "    data['hour'] = data['timestamp'].dt.hour\n",
    "    data['dayofweek'] = data['timestamp'].dt.dayofweek\n",
    "    data['week'] = data['timestamp'].dt.isocalendar().week\n",
    "    \n",
    "    # Add month name for plotting\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    data['month_name'] = data['month'].apply(lambda x: month_names[x-1])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n  Loaded {len(data):,} measurements from {file_count} files.\")\n",
    "    print(f\"  Date range: {data['timestamp'].min()} to {data['timestamp'].max()}\")\n",
    "    print(f\"  Stations: {data['station_name'].nunique()}\")\n",
    "    print(f\"  Pollutants: {data['pollutant_name'].nunique()}\")\n",
    "    print(\"\\n  Pollutant breakdown:\")\n",
    "    for pollutant, count in data['pollutant_name'].value_counts().items():\n",
    "        print(f\"    {pollutant}: {count:,} measurements\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6183ebd",
   "metadata": {},
   "source": [
    "#### 2) Function for analysing overall trends.\n",
    "\n",
    "CSV will be saved as yearly_averages.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4dc2731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_overall_trends(data, report_dir):\n",
    "    \"\"\"\n",
    "    Overall pollution trends across study period.\n",
    "    \n",
    "    Creates:\n",
    "    - Yearly average comparison.\n",
    "    - Monthly trends.\n",
    "    - Overall statistics table.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Overall pollutant trends\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"  Error: no data loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate yearly averages per pollutant\n",
    "    yearly_avg = data.groupby(['year', 'pollutant_name'])['value'].agg([\n",
    "        'mean', 'median', 'std', 'count'\n",
    "    ]).round(2)\n",
    "    \n",
    "    print(\"\\nYearly averages by pollutant:\")\n",
    "    print(yearly_avg)\n",
    "    \n",
    "    # Save to CSV yearly_averages.csv under report/detailed_analysis\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    yearly_avg.to_csv(report_dir / 'yearly_averages.csv')\n",
    "    \n",
    "    # Create visualisation with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Yearly averages bar chart\n",
    "    yearly_pivot = data.groupby(['year', 'pollutant_name'])['value'].mean().unstack()\n",
    "    yearly_pivot.plot(kind='bar', ax=axes[0], width=0.8, alpha=0.8)\n",
    "    axes[0].set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Average concentration', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Yearly average pollution levels by pollutant', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "    axes[0].legend(title='Pollutant', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "    \n",
    "    # Plot 2: Monthly trend across all years\n",
    "    monthly_avg = data.groupby(['month_name', 'pollutant_name'])['value'].mean().unstack()\n",
    "    month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    monthly_avg = monthly_avg.reindex(month_order)\n",
    "    monthly_avg.plot(ax=axes[1], marker='o', linewidth=2)\n",
    "    axes[1].set_xlabel('Month', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Average concentration', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Monthly average pollution levels (all years combined)', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "    axes[1].legend(title='Pollutant', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(report_dir / 'overall_trends.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n  Visualisation saved: overall_trends.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return yearly_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b039a5",
   "metadata": {},
   "source": [
    "#### 3) Analyse specific pollutant.\n",
    "\n",
    "Here I will do full analysis on the pollutants I fetched:\n",
    "1. All pollutants at optimised/year_month/*.\n",
    "2. The pollutants UK have limits - uk_pollutant_limits.csv.\n",
    "3. And also pollutants common between DEFRA/LAQN: NO2, PM10, PM2.5, CO, SO2, O3.\n",
    "\n",
    "The PNG visualisation will be saved as: (pollutant_name)_analysis.png.\n",
    "Report/detailed_analysis path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "faa74447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_pollutant_specific(data, pollutant, report_dir):\n",
    "    \"\"\"\n",
    "    Deep dive into a specific pollutant.\n",
    "    \n",
    "    Parameters:\n",
    "        pollutant : str - pollutant name (e.g., 'NO2', 'PM25').\n",
    "            \n",
    "    Creates:\n",
    "        - Time series plot.\n",
    "        - Distribution histogram.\n",
    "        - Hourly pattern.\n",
    "        - Statistics summary.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"Pollutant specific analysis: {pollutant}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"  Error: no data loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter for this pollutant\n",
    "    poll_data = data[data['pollutant_name'] == pollutant].copy()\n",
    "    \n",
    "    if poll_data.empty:\n",
    "        print(f\"  Error: no data found for {pollutant}.\")\n",
    "        return None\n",
    "    \n",
    "    # Remove NaN values before analysis\n",
    "    poll_data_clean = poll_data[poll_data['value'].notna()].copy()\n",
    "    \n",
    "    if poll_data_clean.empty:\n",
    "        print(f\"  Warning: all values are NaN for {pollutant}, skipping analysis.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n  Total measurements: {len(poll_data):,}\")\n",
    "    print(f\"  Valid (non-NaN) measurements: {len(poll_data_clean):,}\")\n",
    "    print(f\"  Date range: {poll_data_clean['timestamp'].min()} to {poll_data_clean['timestamp'].max()}\")\n",
    "    print(f\"  Stations: {poll_data_clean['station_name'].nunique()}\")\n",
    "    \n",
    "    # Statistics\n",
    "    stats = poll_data_clean['value'].describe()\n",
    "    print(f\"\\n  Concentration statistics for {pollutant}:\")\n",
    "    print(f\"    Mean: {stats['mean']:.2f}\")\n",
    "    print(f\"    Median: {stats['50%']:.2f}\")\n",
    "    print(f\"    Std dev: {stats['std']:.2f}\")\n",
    "    print(f\"    Min: {stats['min']:.2f}\")\n",
    "    print(f\"    Max: {stats['max']:.2f}\")\n",
    "    print(f\"    25th percentile: {stats['25%']:.2f}\")\n",
    "    print(f\"    75th percentile: {stats['75%']:.2f}\")\n",
    "    \n",
    "    # Create 4 panel visualisation\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Panel 1: Time series (daily average)\n",
    "    daily_avg = poll_data_clean.groupby(poll_data_clean['timestamp'].dt.date)['value'].mean()\n",
    "    axes[0, 0].plot(daily_avg.index, daily_avg.values, linewidth=1, alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Date', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel(f'{pollutant} concentration', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_title(f'Daily average {pollutant} levels (2023-2025)', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Panel 2: Distribution histogram (only non-NaN values)\n",
    "    axes[0, 1].hist(poll_data_clean['value'], bins=50, color='steelblue', \n",
    "                   alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].axvline(poll_data_clean['value'].mean(), color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Mean: {poll_data_clean[\"value\"].mean():.1f}')\n",
    "    axes[0, 1].axvline(poll_data_clean['value'].median(), color='orange', linestyle='--', \n",
    "                      linewidth=2, label=f'Median: {poll_data_clean[\"value\"].median():.1f}')\n",
    "    axes[0, 1].set_xlabel(f'{pollutant} concentration', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_title(f'{pollutant} distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Panel 3: Hourly pattern\n",
    "    hourly_avg = poll_data_clean.groupby('hour')['value'].mean()\n",
    "    axes[1, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', \n",
    "                   linewidth=2, color='forestgreen')\n",
    "    axes[1, 0].set_xlabel('Hour of day', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel(f'Average {pollutant}', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_title(f'Average {pollutant} by hour of day', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xticks(range(0, 24, 2))\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Panel 4: Monthly boxplot\n",
    "    month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    poll_data_clean['month_name'] = pd.Categorical(poll_data_clean['month_name'], \n",
    "                                                   categories=month_order, ordered=True)\n",
    "    poll_data_clean.boxplot(column='value', by='month_name', ax=axes[1, 1])\n",
    "    axes[1, 1].set_xlabel('Month', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel(f'{pollutant} concentration', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_title(f'Monthly {pollutant} distribution', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].get_figure().suptitle('')  # Remove default title\n",
    "    plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    safe_name = pollutant.replace('.', '_').replace('/', '_').replace(' ', '_')\n",
    "    plt.savefig(report_dir / f'{safe_name}_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n  Visualisation saved: {safe_name}_analysis.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return poll_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccaaa83",
   "metadata": {},
   "source": [
    "#### 4) Analyse seasonal patterns created for checking seasonal patterns.\n",
    "\n",
    "Function creates season definition.\n",
    "Calculates seasonal averages.\n",
    "And visualises it.\n",
    "Report/detailed_analysis folder and image name: seasonal_patterns.png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8bc72387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_seasonal_patterns(data, report_dir):\n",
    "    \"\"\"\n",
    "    Seasonal variation analysis.\n",
    "    \n",
    "    Creates:\n",
    "    - Season definitions (Winter, Spring, Summer, Autumn).\n",
    "    - Seasonal averages.\n",
    "    - Visualisation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Seasonal pattern analysis\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"  Error: no data loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Define seasons\n",
    "    def assign_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        else:\n",
    "            return 'Autumn'\n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    data_copy['season'] = data_copy['month'].apply(assign_season)\n",
    "    \n",
    "    # Calculate seasonal averages\n",
    "    seasonal_avg = data_copy.groupby(['season', 'pollutant_name'])['value'].mean().unstack()\n",
    "    season_order = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "    seasonal_avg = seasonal_avg.reindex(season_order)\n",
    "    \n",
    "    print(\"\\nSeasonal averages:\")\n",
    "    print(seasonal_avg.round(2))\n",
    "    \n",
    "    # Save\n",
    "    seasonal_avg.to_csv(report_dir / 'seasonal_averages.csv')\n",
    "    \n",
    "    # Visualise\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    seasonal_avg.plot(kind='bar', ax=ax, width=0.8, alpha=0.8)\n",
    "    ax.set_xlabel('Season', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Average concentration', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Seasonal pollution patterns by pollutant', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.legend(title='Pollutant', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(report_dir / 'seasonal_patterns.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n  Visualisation saved: seasonal_patterns.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return seasonal_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8700ca",
   "metadata": {},
   "source": [
    "#### 5) Analyse limit exceedances function checks the uk_pollutant_limits.csv.\n",
    "\n",
    "1. Requires pollutant_limits.csv with columns: pollutant_std, limit, concentration measured as, objective.\n",
    "2. Report/detailed_analysis path creates file called: limit_exceedances.csv.\n",
    "   - Column structure of the file: pollutant, objective, averaging_period, limit_value, total_measurements, exceedances, exceedance_pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2cd2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_limit_exceedances(data, report_dir, limits_path):\n",
    "    \"\"\"\n",
    "    Analysis of UK legal limit exceedances.\n",
    "    \n",
    "    Matches pollutant names in data with standard codes in UK limits CSV.\n",
    "    \n",
    "    Creates:\n",
    "        - Exceedance counts per pollutant.\n",
    "        - Percentage of measurements exceeding limits.\n",
    "        - Comparison against UK legal objectives.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"UK legal limit exceedance analysis\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"  Error: no data loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Load limits\n",
    "    try:\n",
    "        limits_df = pd.read_csv(limits_path)\n",
    "        print(f\"\\n  Loaded {len(limits_df)} UK limit standards from: {limits_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: couldn't load limits file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # LAQN uses standard codes directly (NO2, PM10, PM25, etc.)\n",
    "    # Map PM2.5 variations\n",
    "    pollutant_mapping = {\n",
    "        'PM2.5': 'PM25',\n",
    "        'PM25': 'PM25',\n",
    "        'NO2': 'NO2',\n",
    "        'NO': 'NO',\n",
    "        'NOx': 'NOx',\n",
    "        'SO2': 'SO2',\n",
    "        'O3': 'O3',\n",
    "        'PM10': 'PM10',\n",
    "        'CO': 'CO'\n",
    "    }\n",
    "    \n",
    "    # Prepare results storage\n",
    "    exceedance_results = []\n",
    "    pollutants_checked = 0\n",
    "    pollutants_with_limits = 0\n",
    "    \n",
    "    # For each pollutant in data\n",
    "    for pollutant in data['pollutant_name'].unique():\n",
    "        pollutants_checked += 1\n",
    "        \n",
    "        # Map to standard code (most LAQN codes are already standard)\n",
    "        std_code = pollutant_mapping.get(pollutant, pollutant)\n",
    "        \n",
    "        # Get limits for this pollutant\n",
    "        limits = limits_df[limits_df['pollutant_std'] == std_code]\n",
    "        \n",
    "        if limits.empty:\n",
    "            continue  # Skip if no limits defined\n",
    "        \n",
    "        pollutants_with_limits += 1\n",
    "        \n",
    "        # Get clean data (remove NaN)\n",
    "        poll_data = data[data['pollutant_name'] == pollutant].copy()\n",
    "        poll_data = poll_data[poll_data['value'].notna()]\n",
    "        \n",
    "        if poll_data.empty:\n",
    "            print(f\"\\n  {pollutant}: No valid data to check.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  Checking {pollutant} ({std_code}):\")\n",
    "        \n",
    "        # Check each limit type for this pollutant\n",
    "        for _, limit_row in limits.iterrows():\n",
    "            limit_value = limit_row['limit']\n",
    "            averaging = limit_row['concentration measured as']\n",
    "            objective = limit_row['objective']\n",
    "            unit = limit_row['unit']\n",
    "            \n",
    "            # Handle unit conversion if needed\n",
    "            data_values = poll_data['value'].copy()\n",
    "            if unit == 'mg/m3':\n",
    "                # Convert µg/m3 to mg/m3 if needed (divide by 1000)\n",
    "                # Assuming data is in µg/m3, convert to mg/m3\n",
    "                data_values = data_values / 1000.0\n",
    "                print(f\"    Note: converted values from µg/m³ to mg/m³ for comparison.\")\n",
    "            \n",
    "            # Calculate exceedances\n",
    "            exceedances = (data_values > limit_value).sum()\n",
    "            total = len(data_values)\n",
    "            pct = (exceedances / total * 100) if total > 0 else 0\n",
    "            \n",
    "            exceedance_results.append({\n",
    "                'pollutant': pollutant,\n",
    "                'pollutant_code': std_code,\n",
    "                'objective': objective,\n",
    "                'averaging_period': averaging,\n",
    "                'limit_value': limit_value,\n",
    "                'unit': unit,\n",
    "                'total_measurements': total,\n",
    "                'exceedances': exceedances,\n",
    "                'exceedance_pct': round(pct, 2)\n",
    "            })\n",
    "            \n",
    "            # Report results\n",
    "            if exceedances > 0:\n",
    "                print(f\"    {averaging}: {exceedances:,} / {total:,} ({pct:.1f}%) EXCEED {limit_value} {unit}\")\n",
    "            else:\n",
    "                print(f\"    {averaging}: All measurements within limit ({limit_value} {unit}).\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n  Summary:\")\n",
    "    print(f\"    Total pollutants in dataset: {pollutants_checked}\")\n",
    "    print(f\"    Pollutants with UK limits: {pollutants_with_limits}\")\n",
    "    \n",
    "    if exceedance_results:\n",
    "        exceedance_df = pd.DataFrame(exceedance_results)\n",
    "        exceedance_df.to_csv(report_dir / 'limit_exceedances.csv', index=False)\n",
    "        print(f\"    Results saved: limit_exceedances.csv\")\n",
    "        \n",
    "        # Print overall exceedance summary\n",
    "        total_exceedances = exceedance_df['exceedances'].sum()\n",
    "        total_checks = exceedance_df['total_measurements'].sum()\n",
    "        print(f\"\\n  Overall: {total_exceedances:,} exceedances out of {total_checks:,} measurements checked.\")\n",
    "        \n",
    "        return exceedance_df\n",
    "    else:\n",
    "        print(\"\\n  No matching limits found for any pollutants in dataset.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade17144",
   "metadata": {},
   "source": [
    "#### 6) All analyses.\n",
    "\n",
    "Finally function for run_full_analysis for all the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae88c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_analysis(base_dir, csv_output_path=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run complete analysis pipeline.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir : str or Path\n",
    "            Path to optimised directory.\n",
    "        csv_output_path : str or Path, optional\n",
    "            Path to pollutant_limits.csv.\n",
    "        sample_size : int, optional\n",
    "            Number of files to load (None = all).\n",
    "            \n",
    "    Returns:\n",
    "        data : DataFrame with loaded pollution data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Starting full pollution analysis\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Load data\n",
    "    data = load_pollution_data(base_dir, sample_size=sample_size)\n",
    "    \n",
    "    if data.empty:\n",
    "        print(\"\\n  Error: no data loaded, cannot continue.\")\n",
    "        return None\n",
    "    \n",
    "    # Run all analyses\n",
    "    analyse_overall_trends(data, report_dir)\n",
    "    \n",
    "    # Analyze each pollutant found in data\n",
    "    for pollutant in data['pollutant_name'].unique():\n",
    "        analyse_pollutant_specific(data, pollutant, report_dir)\n",
    "    \n",
    "    analyse_seasonal_patterns(data, report_dir)\n",
    "    \n",
    "    if csv_output_path and Path(csv_output_path).exists():\n",
    "        analyse_limit_exceedances(data, report_dir, Path(csv_output_path))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Analysis complete\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"\\nAll results saved to: {report_dir}\")\n",
    "    print(\"\\nGenerated files:\")\n",
    "    for f in sorted(report_dir.iterdir()):\n",
    "        print(f\"  - {f.name}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c2f8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting full pollution analysis\n",
      "========================================\n",
      "\n",
      "  Loaded 3,446,208 measurements from 4932 files.\n",
      "  Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
      "  Stations: 64\n",
      "  Pollutants: 6\n",
      "\n",
      "  Pollutant breakdown:\n",
      "    NO2: 1,417,752 measurements\n",
      "    PM10: 1,026,456 measurements\n",
      "    PM2.5: 586,944 measurements\n",
      "    O3: 268,320 measurements\n",
      "    SO2: 97,824 measurements\n",
      "    CO: 48,912 measurements\n",
      "\n",
      "========================================\n",
      "Overall pollutant trends\n",
      "========================================\n",
      "\n",
      "Yearly averages by pollutant:\n",
      "                      mean  median    std   count\n",
      "year pollutant_name                              \n",
      "2023 CO               0.24     0.2   0.19   16242\n",
      "     NO2             24.92    20.0  18.67  436830\n",
      "     O3              46.28    46.8  24.13   77787\n",
      "     PM10            16.69    14.0  12.40  317643\n",
      "     PM2.5            8.81     7.0   7.08  154094\n",
      "     SO2              1.21     0.8   1.23   27604\n",
      "2024 CO               0.16     0.1   0.17   14592\n",
      "     NO2             22.78    19.0  15.97  444363\n",
      "     O3              47.38    47.9  22.97   76265\n",
      "     PM10            16.32    14.0  11.61  312108\n",
      "     PM2.5            8.42     6.9   7.33  174697\n",
      "     SO2              1.17     0.8   1.74   23860\n",
      "2025 CO               0.20     0.2   0.18   12731\n",
      "     NO2             22.19    17.9  16.59  385825\n",
      "     O3              49.73    49.7  24.70   66860\n",
      "     PM10            18.94    15.9  13.33  266395\n",
      "     PM2.5           10.63     8.0   8.96  153768\n",
      "     SO2              2.18     1.4   5.55   19753\n",
      "\n",
      "  Visualisation saved: overall_trends.png\n",
      "\n",
      "========================================\n",
      "Pollutant specific analysis: PM10\n",
      "========================================\n",
      "\n",
      "  Total measurements: 1,026,456\n",
      "  Valid (non-NaN) measurements: 896,146\n",
      "  Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
      "  Stations: 42\n",
      "\n",
      "  Concentration statistics for PM10:\n",
      "    Mean: 17.23\n",
      "    Median: 14.40\n",
      "    Std dev: 12.47\n",
      "    Min: 0.00\n",
      "    Max: 759.00\n",
      "    25th percentile: 9.50\n",
      "    75th percentile: 21.40\n",
      "\n",
      "  Visualisation saved: PM10_analysis.png\n",
      "\n",
      "========================================\n",
      "Pollutant specific analysis: NO2\n",
      "========================================\n",
      "\n",
      "  Total measurements: 1,417,752\n",
      "  Valid (non-NaN) measurements: 1,267,018\n",
      "  Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
      "  Stations: 58\n",
      "\n",
      "  Concentration statistics for NO2:\n",
      "    Mean: 23.34\n",
      "    Median: 19.00\n",
      "    Std dev: 17.17\n",
      "    Min: 0.00\n",
      "    Max: 376.30\n",
      "    25th percentile: 10.50\n",
      "    75th percentile: 31.90\n",
      "\n",
      "  Visualisation saved: NO2_analysis.png\n",
      "\n",
      "========================================\n",
      "Pollutant specific analysis: PM2.5\n",
      "========================================\n",
      "\n",
      "  Total measurements: 586,944\n",
      "  Valid (non-NaN) measurements: 482,559\n",
      "  Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
      "  Stations: 24\n",
      "\n",
      "  Concentration statistics for PM2.5:\n",
      "    Mean: 9.25\n",
      "    Median: 7.00\n",
      "    Std dev: 7.87\n",
      "    Min: 0.00\n",
      "    Max: 909.00\n",
      "    25th percentile: 4.80\n",
      "    75th percentile: 11.00\n",
      "\n",
      "  Visualisation saved: PM2_5_analysis.png\n",
      "\n",
      "========================================\n",
      "Pollutant specific analysis: SO2\n",
      "========================================\n",
      "\n",
      "  Total measurements: 97,824\n",
      "  Valid (non-NaN) measurements: 71,217\n",
      "  Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
      "  Stations: 4\n",
      "\n",
      "  Concentration statistics for SO2:\n",
      "    Mean: 1.47\n",
      "    Median: 1.10\n",
      "    Std dev: 3.21\n",
      "    Min: 0.00\n",
      "    Max: 271.40\n",
      "    25th percentile: 0.50\n",
      "    75th percentile: 1.80\n",
      "\n",
      "  Visualisation saved: SO2_analysis.png\n",
      "\n",
      "========================================\n",
      "Pollutant specific analysis: CO\n",
      "========================================\n",
      "\n",
      "  Total measurements: 48,912\n",
      "  Valid (non-NaN) measurements: 43,565\n",
      "  Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
      "  Stations: 2\n",
      "\n",
      "  Concentration statistics for CO:\n",
      "    Mean: 0.20\n",
      "    Median: 0.20\n",
      "    Std dev: 0.19\n",
      "    Min: 0.00\n",
      "    Max: 4.90\n",
      "    25th percentile: 0.10\n",
      "    75th percentile: 0.30\n",
      "\n",
      "  Visualisation saved: CO_analysis.png\n",
      "\n",
      "========================================\n",
      "Pollutant specific analysis: O3\n",
      "========================================\n",
      "\n",
      "  Total measurements: 268,320\n",
      "  Valid (non-NaN) measurements: 220,912\n",
      "  Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
      "  Stations: 11\n",
      "\n",
      "  Concentration statistics for O3:\n",
      "    Mean: 47.71\n",
      "    Median: 48.10\n",
      "    Std dev: 23.95\n",
      "    Min: 0.00\n",
      "    Max: 198.60\n",
      "    25th percentile: 31.10\n",
      "    75th percentile: 63.50\n",
      "\n",
      "  Visualisation saved: O3_analysis.png\n",
      "\n",
      "========================================\n",
      "Seasonal pattern analysis\n",
      "========================================\n",
      "\n",
      "Seasonal averages:\n",
      "pollutant_name    CO    NO2     O3   PM10  PM2.5   SO2\n",
      "season                                                \n",
      "Winter          0.28  28.14  39.20  17.10  10.09  1.63\n",
      "Spring          0.16  23.01  57.33  18.74  10.58  1.52\n",
      "Summer          0.15  18.25  53.14  16.13   7.46  1.41\n",
      "Autumn          0.22  24.69  39.80  16.91   8.90  1.34\n",
      "\n",
      "  Visualisation saved: seasonal_patterns.png\n",
      "\n",
      "========================================\n",
      "UK legal limit exceedance analysis\n",
      "========================================\n",
      "\n",
      "  Loaded 12 UK limit standards from: uk_pollutant_limits.csv\n",
      "\n",
      "  Checking PM10 (PM10):\n",
      "    24 hour mean: 18,787 / 896,146 (2.1%) EXCEED 50.0 µg/m3\n",
      "    annual mean: 40,685 / 896,146 (4.5%) EXCEED 40.0 µg/m3\n",
      "\n",
      "  Checking NO2 (NO2):\n",
      "    annual mean: 194,871 / 1,267,018 (15.4%) EXCEED 40.0 µg/m3\n",
      "\n",
      "  Checking SO2 (SO2):\n",
      "    24 hour mean: 14 / 71,217 (0.0%) EXCEED 125.0 µg/m3\n",
      "\n",
      "  Checking CO (CO):\n",
      "    Note: converted values from µg/m³ to mg/m³ for comparison.\n",
      "    maximum daily: All measurements within limit (10.0 mg/m3).\n",
      "\n",
      "  Checking O3 (O3):\n",
      "    8 hour mean: 3,677 / 220,912 (1.7%) EXCEED 100.0 µg/m3\n",
      "\n",
      "  Summary:\n",
      "    Total pollutants in dataset: 6\n",
      "    Pollutants with UK limits: 5\n",
      "    Results saved: limit_exceedances.csv\n",
      "\n",
      "  Overall: 258,034 exceedances out of 3,395,004 measurements checked.\n",
      "\n",
      "========================================\n",
      "Analysis complete\n",
      "========================================\n",
      "\n",
      "All results saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/detailed_analysis\n",
      "\n",
      "Generated files:\n",
      "  - CO_analysis.png\n",
      "  - NO2_analysis.png\n",
      "  - O3_analysis.png\n",
      "  - PM10_analysis.png\n",
      "  - PM2_5_analysis.png\n",
      "  - SO2_analysis.png\n",
      "  - limit_exceedances.csv\n",
      "  - overall_trends.png\n",
      "  - seasonal_averages.csv\n",
      "  - seasonal_patterns.png\n",
      "  - yearly_averages.csv\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# # Run full analysis on LAQN data\n",
    "# # Start with sample of 100 files for quick exploration\n",
    "# data = run_full_analysis(\n",
    "#     base_dir=base_dir,\n",
    "#     csv_output_path=csv_output_path,\n",
    "#     sample_size=100\n",
    "# )\n",
    "\n",
    "# Once exploration is complete, run on full dataset:\n",
    "data = run_full_analysis(\n",
    "    base_dir=base_dir,\n",
    "    csv_output_path=csv_output_path,\n",
    "    sample_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b5fd9",
   "metadata": {},
   "source": [
    "    ========================================\n",
    "    Starting full pollution analysis\n",
    "    ========================================\n",
    "\n",
    "    Loaded 3,446,208 measurements from 4932 files.\n",
    "    Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
    "    Stations: 64\n",
    "    Pollutants: 6\n",
    "\n",
    "    Pollutant breakdown:\n",
    "        NO2: 1,417,752 measurements\n",
    "        PM10: 1,026,456 measurements\n",
    "        PM2.5: 586,944 measurements\n",
    "        O3: 268,320 measurements\n",
    "        SO2: 97,824 measurements\n",
    "        CO: 48,912 measurements\n",
    "\n",
    "    ========================================\n",
    "    Overall pollutant trends\n",
    "    ========================================\n",
    "\n",
    "    Yearly averages by pollutant:\n",
    "                        mean  median    std   count\n",
    "    year pollutant_name                              \n",
    "    2023 CO               0.24     0.2   0.19   16242\n",
    "        NO2             24.92    20.0  18.67  436830\n",
    "        O3              46.28    46.8  24.13   77787\n",
    "        PM10            16.69    14.0  12.40  317643\n",
    "        PM2.5            8.81     7.0   7.08  154094\n",
    "        SO2              1.21     0.8   1.23   27604\n",
    "    2024 CO               0.16     0.1   0.17   14592\n",
    "        NO2             22.78    19.0  15.97  444363\n",
    "        O3              47.38    47.9  22.97   76265\n",
    "        PM10            16.32    14.0  11.61  312108\n",
    "        PM2.5            8.42     6.9   7.33  174697\n",
    "        SO2              1.17     0.8   1.74   23860\n",
    "    2025 CO               0.20     0.2   0.18   12731\n",
    "        NO2             22.19    17.9  16.59  385825\n",
    "        O3              49.73    49.7  24.70   66860\n",
    "        PM10            18.94    15.9  13.33  266395\n",
    "        PM2.5           10.63     8.0   8.96  153768\n",
    "        SO2              2.18     1.4   5.55   19753\n",
    "\n",
    "    Visualisation saved: overall_trends.png\n",
    "\n",
    "    ========================================\n",
    "    Pollutant specific analysis: PM10\n",
    "    ========================================\n",
    "\n",
    "    Total measurements: 1,026,456\n",
    "    Valid (non-NaN) measurements: 896,146\n",
    "    Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
    "    Stations: 42\n",
    "\n",
    "    Concentration statistics for PM10:\n",
    "        Mean: 17.23\n",
    "        Median: 14.40\n",
    "        Std dev: 12.47\n",
    "        Min: 0.00\n",
    "        Max: 759.00\n",
    "        25th percentile: 9.50\n",
    "        75th percentile: 21.40\n",
    "\n",
    "    Visualisation saved: PM10_analysis.png\n",
    "\n",
    "    ========================================\n",
    "    Pollutant specific analysis: NO2\n",
    "    ========================================\n",
    "\n",
    "    Total measurements: 1,417,752\n",
    "    Valid (non-NaN) measurements: 1,267,018\n",
    "    Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
    "    Stations: 58\n",
    "\n",
    "    Concentration statistics for NO2:\n",
    "        Mean: 23.34\n",
    "        Median: 19.00\n",
    "        Std dev: 17.17\n",
    "        Min: 0.00\n",
    "        Max: 376.30\n",
    "        25th percentile: 10.50\n",
    "        75th percentile: 31.90\n",
    "\n",
    "    Visualisation saved: NO2_analysis.png\n",
    "\n",
    "    ========================================\n",
    "    Pollutant specific analysis: PM2.5\n",
    "    ========================================\n",
    "\n",
    "    Total measurements: 586,944\n",
    "    Valid (non-NaN) measurements: 482,559\n",
    "    Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
    "    Stations: 24\n",
    "\n",
    "    Concentration statistics for PM2.5:\n",
    "        Mean: 9.25\n",
    "        Median: 7.00\n",
    "        Std dev: 7.87\n",
    "        Min: 0.00\n",
    "        Max: 909.00\n",
    "        25th percentile: 4.80\n",
    "        75th percentile: 11.00\n",
    "\n",
    "    Visualisation saved: PM2_5_analysis.png\n",
    "\n",
    "    ========================================\n",
    "    Pollutant specific analysis: SO2\n",
    "    ========================================\n",
    "\n",
    "    Total measurements: 97,824\n",
    "    Valid (non-NaN) measurements: 71,217\n",
    "    Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
    "    Stations: 4\n",
    "\n",
    "    Concentration statistics for SO2:\n",
    "        Mean: 1.47\n",
    "        Median: 1.10\n",
    "        Std dev: 3.21\n",
    "        Min: 0.00\n",
    "        Max: 271.40\n",
    "        25th percentile: 0.50\n",
    "        75th percentile: 1.80\n",
    "\n",
    "    Visualisation saved: SO2_analysis.png\n",
    "\n",
    "    ========================================\n",
    "    Pollutant specific analysis: CO\n",
    "    ========================================\n",
    "\n",
    "    Total measurements: 48,912\n",
    "    Valid (non-NaN) measurements: 43,565\n",
    "    Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
    "    Stations: 2\n",
    "\n",
    "    Concentration statistics for CO:\n",
    "        Mean: 0.20\n",
    "        Median: 0.20\n",
    "        Std dev: 0.19\n",
    "        Min: 0.00\n",
    "        Max: 4.90\n",
    "        25th percentile: 0.10\n",
    "        75th percentile: 0.30\n",
    "\n",
    "    Visualisation saved: CO_analysis.png\n",
    "\n",
    "    ========================================\n",
    "    Pollutant specific analysis: O3\n",
    "    ========================================\n",
    "\n",
    "    Total measurements: 268,320\n",
    "    Valid (non-NaN) measurements: 220,912\n",
    "    Date range: 2023-01-01 00:00:00 to 2025-11-18 23:00:00\n",
    "    Stations: 11\n",
    "\n",
    "    Concentration statistics for O3:\n",
    "        Mean: 47.71\n",
    "        Median: 48.10\n",
    "        Std dev: 23.95\n",
    "        Min: 0.00\n",
    "        Max: 198.60\n",
    "        25th percentile: 31.10\n",
    "        75th percentile: 63.50\n",
    "\n",
    "    Visualisation saved: O3_analysis.png\n",
    "\n",
    "    ========================================\n",
    "    Seasonal pattern analysis\n",
    "    ========================================\n",
    "\n",
    "    Seasonal averages:\n",
    "    pollutant_name    CO    NO2     O3   PM10  PM2.5   SO2\n",
    "    season                                                \n",
    "    Winter          0.28  28.14  39.20  17.10  10.09  1.63\n",
    "    Spring          0.16  23.01  57.33  18.74  10.58  1.52\n",
    "    Summer          0.15  18.25  53.14  16.13   7.46  1.41\n",
    "    Autumn          0.22  24.69  39.80  16.91   8.90  1.34\n",
    "\n",
    "    Visualisation saved: seasonal_patterns.png\n",
    "\n",
    "    ========================================\n",
    "    UK legal limit exceedance analysis\n",
    "    ========================================\n",
    "\n",
    "    Loaded 12 UK limit standards from: uk_pollutant_limits.csv\n",
    "\n",
    "    Checking PM10 (PM10):\n",
    "        24 hour mean: 18,787 / 896,146 (2.1%) EXCEED 50.0 µg/m3\n",
    "        annual mean: 40,685 / 896,146 (4.5%) EXCEED 40.0 µg/m3\n",
    "\n",
    "    Checking NO2 (NO2):\n",
    "        annual mean: 194,871 / 1,267,018 (15.4%) EXCEED 40.0 µg/m3\n",
    "\n",
    "    Checking SO2 (SO2):\n",
    "        24 hour mean: 14 / 71,217 (0.0%) EXCEED 125.0 µg/m3\n",
    "\n",
    "    Checking CO (CO):\n",
    "        Note: converted values from µg/m³ to mg/m³ for comparison.\n",
    "        maximum daily: All measurements within limit (10.0 mg/m3).\n",
    "\n",
    "    Checking O3 (O3):\n",
    "        8 hour mean: 3,677 / 220,912 (1.7%) EXCEED 100.0 µg/m3\n",
    "\n",
    "    Summary:\n",
    "        Total pollutants in dataset: 6\n",
    "        Pollutants with UK limits: 5\n",
    "        Results saved: limit_exceedances.csv\n",
    "\n",
    "    Overall: 258,034 exceedances out of 3,395,004 measurements checked.\n",
    "\n",
    "    ========================================\n",
    "    Analysis complete\n",
    "    ========================================\n",
    "\n",
    "    All results saved to: /Users/burdzhuchaglayan/Desktop/data science projects/air-pollution-levels/data/laqn/report/detailed_analysis\n",
    "\n",
    "    Generated files:\n",
    "    - CO_analysis.png\n",
    "    - NO2_analysis.png\n",
    "    - O3_analysis.png\n",
    "    - PM10_analysis.png\n",
    "    - PM2_5_analysis.png\n",
    "    - SO2_analysis.png\n",
    "    - limit_exceedances.csv\n",
    "    - overall_trends.png\n",
    "    - seasonal_averages.csv\n",
    "    - seasonal_patterns.png\n",
    "    - yearly_averages.csv\n",
    "    ========================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
